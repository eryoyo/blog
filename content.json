{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/blog/2022/04/28/hello-world/"},{"title":"使用hexo-icarus快速创建自己的博客网站","text":"使用hexo+icarus快速搭建属于自己的博客网站 准备环境 安装nodejs✅ 安装git✅ 安装hexo✅ 12# 此为全局安装，可能需要sudo权限npm install -g hexo-cli 创建git仓库直接在github主页创建一个新的仓库，此处假设仓库名称为blog_tensorrt 使用hexo建初始博客首先初始化一个博客项目，此处blog可以换成自己想要起的名称。该操作之后在当前目录下会出现一个叫做blog的新的文件夹 1hexo init blog 进入blog文件夹下 1cd blog 可以看到当前的文件夹下有一个themes的文件夹，此时看到里面没有文件，下载icarus主题代码到其中 1git clone git@github.com:ppoffice/hexo-theme-icarus.git /themes/icarus 之后修改_config.yml文件，将theme修改为icarus 1theme: icarus 之后在命令行进行构建 1hexo g 输入生成命令可能会报错，提示有没有安装的包，安装确实的包 1yarn add bulma-stylus@0.8.0 hexo-component-inferno@^1.1.0 hexo-pagination@^2.0.0 hexo-renderer-inferno@^0.1.3 inferno@^7.3.3 inferno-create-element@^7.3.3 接着生成 12# 该命令多执行几次，知道没有新的文件生成hexo g 查看网页初始效果 1hexo s 打开网页http://localhost:4000 自定义博客设计此时博客目录下有文件_config.icarus.yml，修改该文件即可，每一项在icarus官网https://ppoffice.github.io/hexo-theme-icarus/Configuration/icarus%E7%94%A8%E6%88%B7%E6%8C%87%E5%8D%97-%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/#more均有详细的说明，在此不做赘述。 部署网站首先修改_config.yml文件 123456789101112# Sitetitle: eryoyo的博客subtitle: 坚持✊description: tensorrt笔记整理keywords: author: eryoyolanguage: zh-CNtimezone: Asia/Shanghai# URL## Set your site url here. For example, if you use GitHub Page, set url as 'https://username.github.io/project'url: https://eryoyo.github.io/blog_tensorrt 之后进行本地查看 123hexo cleanhexo ghexo s 网站可以在http://localhost:4000/blog_tensorrt里面查看到 之后接着修改_config.yml文件 1234deploy: type: git repo: git@github.com:eryoyo/blog_tensorrt.git branch: master 安装部署需要的包 1npm install hexo-deployer-git --save 之后部署 1hexo deploy 在仓库里面setting里面修改github pages的none为master分支，点击save，等待一会之后就可以在访问自己刚刚部署到的网站了","link":"/blog/2022/04/28/%E4%BD%BF%E7%94%A8hexo-icarus%E5%BF%AB%E9%80%9F%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/"},{"title":"博客链接整理","text":"存放自己博客相关的链接 发博客平台 GitHub: https://eryoyo.github.io/blog/ 知乎: https://www.zhihu.com/creator/manage/creation/all 博客园： https://i.cnblogs.com/posts CSDN: https://mp.csdn.net/ bilibili: https://member.bilibili.com/platform/home eryoyo: https://eryoyo.xyz/admin/login 微信公众号： https://mp.weixin.qq.com/cgi-bin/home 创建博客使用 github博客评论区管理： https://disqus.com/ https://disqus.com/home/ github博客markdown书写前front-matter管理和文档: https://hexo.io/zh-cn/docs/front-matter hexo主题市场: https://hexo.io/themes/ icarus主题配置: https://ppoffice.github.io/hexo-theme-icarus/Configuration/icarus%E7%94%A8%E6%88%B7%E6%8C%87%E5%8D%97-%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/#%E5%AF%BC%E8%88%AA%E6%A0%8F icarus快速上手: https://ppoffice.github.io/hexo-theme-icarus/uncategorized/icarus%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/#install-source icarus源码: https://github.com/ppoffice/hexo-theme-icarus 腾讯云控制台: 域名：https://console.cloud.tencent.com/domain 备案：https://console.cloud.tencent.com/beian/manage SSL证书：https://console.cloud.tencent.com/certoverview 服务器：https://console.cloud.tencent.com/lighthouse/instance/index eryoyo服务器模板源码: https://github.com/ZHENFENG13/My-Blog eryoyo网站配图： https://en.wikipedia.org/wiki/Limerick","link":"/blog/2022/04/28/%E5%8D%9A%E5%AE%A2%E9%93%BE%E6%8E%A5%E6%95%B4%E7%90%86/"},{"title":"linux系统中makefile书写规则","text":"本文自https://blog.csdn.net/haoel/article/details/2886整理而来，便于自己快速回顾makefile的书写。 基础规则makefile使得工程完成自动化编译，告诉make命令如何编译和链接程序。编译过程是指生成.o中间代码文件，链接是指将中间代码文件合起来成为可执行文件。编译关注的是语法正确和声明正确，链接会关注函数实现，也就是在所有的中间代码文件当中寻找函数实现，如果没有找到就会报错，需要指定中间代码文件位置。 下面是makefile的基础书写规则，也就是利用command来从prerequisites生成target。target表示目标文件，prerequisites表示生成target需要的文件，command表示任意的shell命令，以Tab键开头。 12target:prerequisites command 遵循着一个规律，也就是当target不存在的时候会直接生成，假如target存在，就会查看prerequisites是否比target新，假如是就重新生成target，也就是重新执行command。 常用的点 /表示换行","link":"/blog/2022/04/29/linux%E7%B3%BB%E7%BB%9F%E4%B8%ADmakefile%E4%B9%A6%E5%86%99%E8%A7%84%E5%88%99/"},{"title":"刷题基础1","text":"主要总结使用C++刷题时会使用的一些基础知识 cstdio头文件：程序中处理输入输出 scanf printf %d %lld f% lf% c% s% 变量类型：int(9), long long(18), float(128), double(1024), char(127), bool(0,1) 0~9(48~57),A~Z(65~90),a~z(97~122) ^相同为0，不同为1 scanf(“%d:%d:%d”, &amp;hh, &amp;mm, &amp;ss) 除了%c以外，scanf对其他格式符例如%d、s%的输入是以空白符（空格、Tab）为结束判断标志的 %md:保持m位右对齐输出，不足m位用空格补齐 %0md:用0补齐 %.mf:使浮点数保留m位小数输出 getchar:输入单个字符，可识别换行符，putchar:输出单个字符 typedef long long ll:给long long起一个别名ll math函数：fabs(取浮点数绝对值),floor,ceil(向下、上取整)，pow(a, b):返回a的b次方，sqrt:算术平方根，log:以自然对数为底的对数，没有对任意底数求对数的函数，使用换底公式：以a为底b的对数=以自然对数为底b的对数除以以自然对数为底a的对数，sin,cos,tan,asin,acos,atan,pi=acos(-1.0)，round四舍五入 给数组赋初值0：int a[10] = {0} 当数组较大时应该定义在主函数的外面10^6 memset(数组名， 值， sizeof(数组名)):为数组里面的每一个元素赋相同的初值, string.h头文件，值=0赋全0，值=-1赋全1 字符串数组赋初值可以用字符串”hello world” scanf,printf输入输出：c%识别空格和换行，s%以空格和换行作为分隔符 getchar(),putchar(char)输入输出：识别换行符 gets(str),puts(str)输入输出：以换行符作为分隔符 字符数组的末尾有\\0,表示空字符，占用一个字符位 gets和printf会在末尾自动添加，此外需要自己添加\\0 string.h strlen(字符数组):返回\\0字符前的字符个数，不包含\\0 strcmp(字符数组a，字符数组b)：字典序排序, a == b:返回0, a &lt; b:返回负数, a &gt; b:返回正数 strcpy(字符数组a，字符数组b):将b复制给a,包括\\0 strcat(字符数组a，字符数组b):将b拼接到a后面 sscanf(str, “%d”, &amp;n):将str以整数的格式输出到n sprintf(str, “%d”, n):将n以整数的格式写到字符数组中 数组作为函数参数的时候，第一维不需要填写长度，第二维需要, 不允许返回数组 指针变量支持加减法操作和自增自减操作，操作单位为其基类型，例如int为其基类型, 储存的是int类型变量的地址，那么＋1之后指向当前变量的下一个int 数组名称可作为数组的首地址来使用 指针的引用：int* &amp;p指针本质也是一个无符号整数，所以可以使用引用 cin和cout:iostream char str[100]; cin.getline(str, 100); string str; getline(cin, str); cout设置小数精度：头文件iomanip, cout&lt;&lt;setiosflags(ios::fixed)&lt;&lt;setprecision(2)&lt;&lt;123.4567&lt;&lt;endl输出123.46 浮点数的比较： const double eps = 1e-8 #define Equal(a, b) ((fabs((a) - (b))) &lt; (eps)) 圆周率π：const double pi = acos(-1.0) 0.00输出为-0.00只能先将结果存到字符串中然后与-0.00比较配合eps修正为0.00 sqrt,asin,acos配合eps使得变量在定义域内 scanf有返回值，代表成功读入的参数个数，如果读取失败则返回EOF while(scanf(“%d”, &amp;n) != EOF) while(gets(str) != NULL) 用一个结构体给另一个结构体赋值的时候，结构体里面假如有字符串数组，赋值的过程中不是用的地址，而是地址上面的值","link":"/blog/2022/04/29/%E5%88%B7%E9%A2%98%E5%9F%BA%E7%A1%801/"},{"title":"刷题基础2","text":"总结使用C++刷题时STL的使用 vector 初始化： 123vector&lt;typename&gt; name;// vector数组:vector&lt;typename&gt; arrayname[size]; 访问： 12345678910// 通过下标vec[i];// 通过迭代器vector&lt;typename&gt;::iterator it = name.begin(); *it; it++;// 在常用的STL容器中只有vector和string允许使用it+i的用法*(vec.begin() + i); 操作： 1234567vec.push_back(x)vec.pop_back()vec.size()vec.clear() // 清空vector中所有元素vec.insert(it, x) // 在it所指位置处插入一个元素，当前位置原先的元素及其之后的元素后移vec.erase(it) // 删除it所指位置的元素vec.erase(it1, it2) //删除[it1,it2)区间内的所有元素 set 初始化： 123set&lt;typename&gt; name;// set数组set&lt;typename&gt; arrayname[size]; 访问： 1234// 通过迭代器set&lt;typename&gt;::iterator it = name.begin();*it; it++; 操作： 1234567name.insert(x) // 复杂度O(logn)name.find(x) // 返回集合中值为x的迭代器，时间复杂度为O(logn)name.erase(it) // 删除it所指位置的元素，时间复杂度为O(1)name.erase(x) // 删除集合中值为x的元素，复杂度为0(logn)name.erase(it1, it2) // 删除[it1, it2)区间内的元素name.size() // 获取元素的个数，复杂度为O(1)name.clear() // 清空所有元素，复杂度为O(n) 注： set的作用是去重并升序排序 multiset允许元素重复 unordered_set:其只实现去重不实现排序，速度更快，原理为散列实现代替set的红黑树（自平衡二叉树）实现 string 初始化： 12345string str = &quot;abcd&quot;;cin&gt;&gt;str; cout&lt;&lt;str;// scanf不能输入stringprintf(&quot;%s&quot;, str.c_str()); 访问元素： 123456// 通过下标// 通过迭代器：string::iterator it = str.begin();it++;*it; 常用函数： 12345678910111213141516171819str1 += str2; // 将str2拼接到str1后面str1 = str1 + str2;// ==, !=, &lt;, &lt;=, &gt;, &gt;=:以字典序比较两个字符串str.length();str.size();str.insert(pos, string); // 从下标为pos的位置开始插入字符串，该位置及其之后的元素全部后移str.insert(it, it1, it2); // 从it所指位置开始插入it1到it2之间的元素str.erase(it); // 删除单个元素str.erase(it1, it2); // 删除一个区间内的所有元素str.erase(pos, length); // 删除下标为pos开始的length个元素str.clear(); // 清空元素str.substr(pos, len); // 取出len个元素string::npos; // find函数失配时的返回值，常量-1或4294967295str1.find(str2); // 当str2是str1的子串的时候，返回第一次出现的位置，否则返回string::npos,复杂度为O(mn)str.replace(pos, len, str2);str.replace(it1, it2, str2); // 将范围内的字符串替换为str2，修改了原字符串 map 初始化： 1map&lt;typename1, typename2&gt; mp; // key可以为STL类型，但是不能用数组类型 访问元素： 12345678// 元素按照key的大小升序排序// 通过下标访问// 通过迭代器：map&lt;typename1, typename2&gt;::iterator it;it-&gt;first; it-&gt;second; 常用函数： 123456mp.find(key); // 返回key对应的元素的迭代器mp.erase(it);mp.erase(key);mp.erase(it1, it2);mp.size();mp.clear(); queue 初始化： 1queue&lt;typename&gt; name; 访问元素： 12q.front(); // 最先进入的元素; q.back(); 常用函数： 123456q.front();q.back();q.pop();q.push(x);q.empty();q.size(); priority_queue 初始化： 1priority_queue&lt;typename&gt; name; // 大顶堆 访问元素： 1q.top(); 常用函数： 12345q.push(x); // O(logn)q.top(); // O(1)q.pop(); // O(logn)q.empty(); // O(1)q.size() 自定义优先级队列： 12345678910111213141516171819202122// 基本数据类型：// 小顶堆：priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; q;// 大顶堆(默认)：priority_queue&lt;int, vector&lt;int&gt;, less&lt;int&gt;&gt; q;// 结构体：// 重载&lt;符号：写在结构体里面friend bool operator &lt; (student a, student b){ return a.age &lt; b.age;//大顶堆 return a.age &gt; b.age;//小顶堆}priority_queue&lt;student&gt; q;// 重载：写在结构体外面struct cmp{ bool operator () (fruit f1, fruit f2){ return f1.price &lt; f2.price;//大顶堆 return f1.price &gt; f2.price;//小顶堆 }}priority_queue&lt;fruit, vector&lt;fruit&gt;, cmp&gt; q; stack 初始化： 1stack&lt;typename&gt; s; 访问元素： 1s.top(); 常用函数： 12345s.push(x);s.top();s.pop();s.empty();s.size(); pair 初始化： 123// utility头文件，包含了map头文件也就是包含了utility, map的元素为元组pair&lt;typename1, typename2&gt; name(e1, e2);p = make_pair(e1, e2); 访问元素： 12p.first;p.second; 常用函数： ==, !=, &lt;, &lt;=, &gt;, &gt;=,先比较first，再比较second algorithm12345678910111213max(a, b);min(a, b);abs(x);swap(a, b);reverse(it1, it2); // 将[it1, it2)范围内的元素翻转next_permutation(a, a + 5); // 返回下一个全排列fill(a, a + 5, 233); // 将数组指定范围内填充为指定数字sort(it1, it2, cmp); // it为指针或者迭代器bool cmp(int i1, int i2){ return i1 &gt; i2;//递减排列}lower_bound(it1, it2, val); // 返回第一个&gt;=val的元素, 否则返回可以插入该元素的指针或者迭代器, 找到则返回指针或迭代器upper_bound(it1, it2, val); // 返回第一个&gt;val的元素","link":"/blog/2022/04/29/%E5%88%B7%E9%A2%98%E5%9F%BA%E7%A1%802/"},{"title":"机器学习基本概念","text":"机器学习：寻找一个函数的能力 在语音识别中，这个函数将一段语音转化为文字；在图像分类中，这个函数得到图片的类别；在下围棋中，这个函数获取下一步的棋子应该下在什么地方 Regression：寻找输出连续值的函数 classification：寻找输出离散值的函数，alpha go也可以看作一个分类的任务 structure learning：让机器创造一些东西，包括结构化的图片或者文档 模型: $y=b+wx_1$ 特征$x_1$，权重$w$，偏置$b$，标签$\\hat{y}$，损失函数$L(b,w)$ ​ mean absolute errorMAE: $e=|y-\\hat{y}|$ ​ Mean square error(MSE): $e=(y-\\hat{y})^2$ Optimization: 找到最佳的$w$, $b$使得$w^*,b^*=argmin\\ L$ ​ Gradient descent: 首先随机初始化，之后计算关于$w,b$的梯度，更新$w,b$，$w^1\\leftarrow W^0-\\eta\\frac{\\partial L}{\\partial w}$","link":"/blog/2022/05/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"},{"title":"深度学习基础概念","text":"普通的线性模型不能满足更加复杂的需求 更加复杂的曲线可以用一系列曲线叠加的方式得到，这样的曲线抽象为$y=c\\frac{1}{1+e^{-(b+wx_1)}}$，即sigmoid函数$$r = b + Wx\\a = \\sigma(r)\\y = b + c^Ta\\y = b + c^T \\ \\sigma(b + Wx)$$除了模型变复杂之外，损失函数和优化的过程是一样的 另外一种激活函数ReLU：$c\\ max(0, b + wx_1)$ rectified linear unit，两个ReLU可以生成一个hard sigmoid 可以使模型变得更加复杂，也就是增加层数。但是不能将网络变得过于复杂，会出现过拟合现象 为什么不将网络直接变胖，而是需要变深呢？","link":"/blog/2022/05/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"},{"title":"Colab和Pytorch","text":"李宏毅课程中Colab, Pytorch总结 Colab 点击+ Code来添加code cell，点击+ Text来添加text cell，还有将cell上移以及下移的选项，可以将cell删除 在code cell中有两种命令可以输入，一种是python代码，直接输入即可；另一种是shell命令，在命令的前面需要加上! 命令的前面加上!，会先执行这个命令，之后就会杀掉这个进程，而在命令的前面加上%就会影响和这个jupyter相关的进程，称为magic command，可以查看https://ipython.readthedocs.io/en/stable/interactive/magics.html来知道更多的magic command 更换为使用GPU：点击Runtime-点击Change Runtime Type-选择GPU，更换runtime的操作会重启这个session 点击cell前面的运行按钮就会执行这个cell的代码，也可以点击runtime里面的选项来运行代码 查看抽到的GPU类型!nvidia-smi，有三种可能性P100&gt;T4&gt;k80 通过google drive下载文件，google drive分享的文件如下链接https://drive.google.com/open?id=xxxxxx，这个xxxxxx就是文件ID，使用下列命令来下载`!gdown –id ‘xxxxxx’ –output pikachu.png` 下载之后的文件可以通过点击文件夹图标查看，这些文件都是暂时储存的，在session结束之后就会被清理 可以点击上载图标来上传本地文件，可以下载文件 可以不用每次都上传以及下载文件，直接将google drive挂载到colab即可，首先导入包from google.colab import drive之后挂载drive.mount('/content/drive')","link":"/blog/2022/05/06/Colab%E5%92%8CPytorch/"},{"title":"Docker总结","text":"总结Docker的使用小知识 Docker使得程序的部署变得简单，Docker依赖的底层技术支持包括namespace实现进程之间的隔离，Control Groups来完成资源控制，资源包括核心数、内存和硬盘等等。依赖Union file sstem完成Container和image的分层。 镜像：分层的，是只读的，镜像可以理解为一个树状的结构，依赖关系体现在dockerfile文件中 容器：通过image来创建，在image上又添加了一层，这一层可以读写 仓库：dockerhub 容器的启动过程： 检查镜像是否存在，假如不存在下载 利用镜像创建一个容器 启动刚刚创建的容器 分配一个文件系统给容器，相当于在镜像层外挂载一个可读可写层 从宿主机的网桥接口中桥接一个给容器 从网桥中分一个IP地址给容器 执行用户指定的yinys","link":"/blog/2022/05/16/Docker%E6%80%BB%E7%BB%93/"},{"title":"卜算法学习笔记-lecture1-绪论","text":"算法的概念算法是指给出解决问题的操作步骤之后，无论是人还是机器都可以按照步骤机械性的执行得到问题的结果。我们在日常生活中回遇到各种的实际问题遇到之后的解决流程如下： 首先在一系列世纪问题中找到一个特定的topic，得到一个实际问题，在这个实际问题的基础之上我们可以抽象出数学问题，之后通过对该数学问题的观察，特别是对输入输出结构的观察，得到解决问题的算法。面对一个数学问题，我们首先问自己以下几个问题。 这个问题的最简单版本是否可以解决，假如不可以，我们可以降低问题的难度直到简单版本可以解决； 问题是否可以拆分为小问题，假如可以拆分，那么采用devide andconquer方法，如果有最优子结构，可以尝试动态规划，还有短视的策略贪心可以使用； 可行解的形式是什么，是否采用枚举办法，枚举时需要注意剪枝； 问题的解是否可以通过微小扰动变为另一种形式，这种可以采用逐步改进的办法解决问题，包括线性规划、局部搜索和半退火、网络流等； 基于旅行商问题介绍三种算法设计过程traveling salesman problem, TSP 输入：结点集合$V={v_1,…,v_n}$，以及结点之间的距离矩阵$D = (d_{i,j}) \\in R^{n\\times n}$,其中$d_{i,j}$表示节点$i$与节点$j$之间的距离； 输出：最短的环游路线，即从任意节点作为出发点，经过每个节点一次且仅一次，最终返回出发点的里程最短的环游；分而治之算法设计过程简介我们需要观察这个问题是否可以分解成简单实例，并且简单实例的解是否可以组合出复杂实例的解。 第一种尝试，我们尝试减少结点数，可以看到，我们可以很容易的将五个结点的实例变为四个结点的实例，但是简单实例不太容易组合成复杂实例; 第二种尝试，求解一个辅助问题，计算从起始结点$s$出发，经过中间结点集合$S$，最终达到目的节点$x$的最短路径，其长度计为$M(s,S, x)$; 可以看到在第二种尝试之下可以将这个问题分解为简单实例并可以合起来组成复杂实例。以5个结点的实例为例，包含结点$a,b,c,d,e$，以$a$为起始节点，那么可以从$b,c,e$返回起点，最短里程可以表示为$min{d_{b,a} + M(a,{c,d},b), d_{c,a} + M(a,{b,d},c), d_{e,a} + M(a,{c,b},e)}$这个算法可以表示为以下伪代码计算$M(s,S,x)$的伪代码算法的复杂度计算如下，我们需要枚举所有的结点子集$S$，所以总共有$2^n$个子集，路线起点确定，但是终点$x$有$n$种可能，所以总共要$O(2^nn)$才能计算出$M(s,S,x)$表格的值，计算出来之后还要经过$n$次比较的到最终的结果，Bellman-held-karp的复杂度为$O(2^n n^2)$. 逐步改进的算法设计流程基本过程是从问题的一个粗糙的，质量不太高的完整可行解开始，不断改进，直到获得满意的解为止，一般性框架为。求解过程中需要关注三个方面： 初始粗糙可行解的选择； 可行解的改进办法，也即扰动办法； 算法的终止条件，常见的终止条件包括当前的可行解无法进一步改进；迭代次数超过预先定义的值；当前可行解的质量超过预先定义的阈值； 智能枚举算法设计流程通过观察解的形式来枚举 枚举边的各种情况，最终的解的形式是边的集合，枚举各种边是否存在的情况，在这个过程中可以进行剪枝 枚举点的各种情况，也就是将环游表示成$X=x_1,x_2,…,x_{n-2},x_i \\in V （1\\leq i \\leq n - 2)$. 算法的复杂度 时间复杂度：算法执行过程中总的基本操作次数 空间复杂度：算法所使用的存储单元数目 因为仅凭复杂度在一个或几个特定实例上的时间和空间复杂度难以评估其性能，因此，常用的方式是考虑具有同等规模的所有实例 最坏情况时间复杂度：$worst-case\\ time\\ complexity$在所有实例上基本操作次数的最大值作为时间复杂度 最坏情况空间复杂度：$worst-case\\ space\\ complexity$在所有实例上使用存储单元数目的最大值作为空间复杂度 还有平均情况时间复杂度和平均情况空间复杂度的概念，但是需要知道问题实例的概率分布 算法根据时间复杂度分为指数时间算法和多项式时间算法，时间复杂度为$O(n^{\\log n})$的算法是指数时间算法 大O记号 大O记号:考虑两个函数 $f(n)$ 和 $g(n)$，其定义域是正整数，值域是正实数。如果存在一个正数 $c &gt; 0$ 以及 $N &gt; 0$，使得对任意的 $n &gt; N$，总有 $f(n) ≤ cg(n)$成立，则记为 $f(n) = O(g(n))$。 $\\Omega$记号：考虑两个函数 $f(n)$ 和 $g(n)$，其定义域是正整数，值域是正实数。如果$f(n) = O(g(n))$，则可以记为$g(n) = \\Omega(f(n))$ $\\Theta$记号：如果 $f(n) = O(g(n))$ 和 $g(n) = O(f(n))$ 同时成立，则可以记为 $f(n) = \\Theta(g(n))$。","link":"/blog/2022/08/31/%E5%8D%9C%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture1-%E7%BB%AA%E8%AE%BA/"},{"title":"模式识别与机器学习学习笔记-lecture1-绪论","text":"概述模式：存在于世间和空间中可观察的物体，如果我们可以区别他们是否相同或是否相似，都可以称之为模式。 模式的直观特性： 可观察性 可区分性 相似性 模式识别：对周围物体的认识、人的识别、声音的辨别、气味的分辨 数据聚类目标：用某种相似性度量的方法见原始数据组成有意义的和有用的各种数据集是一种非监督学习的方法，解决方案是数据驱动的 统计分类基于概率统计模型得到各类别的特征向量的分布，以取得分类的方法；特征向量分布的获得是基于一个类别已知的训练样本集；是一种监督分类的方法，分类器是概念驱动的； 结构模式识别该方法通过考虑识别对象的各部分之间的联系来达到识别分类的目的；识别采用结构匹配的形式，通过计算一个匹配程度值（matching score）来评估一个未知的对象或位未知对象某些部分与某种典型模式的关系如何；当制定出来一组可以描述对象部分之间关系的规则后，可以应用一种特殊的结构模式识别方法-句法模式识别，来检查一个模式基元的序列是否遵守某种规则，即句法规则或语法； 神经网络由一系列互相联系的、相同的单元（神经元）组成，相互间的联系可以在不同的神经元之间传递增强或抑制信号；增强或抑制是通过调整神经元相互间联系的权重系数来实现；神经网络可以实现监督和非监督学习条件下的分类； 监督学习监督学习是从有标记的训练数据来推断或建立一个模型 无监督学习无监督学习与监督学习的不同之处在于，没有任何训练样本，需要直接对数据进行建模，寻找数据的内在结构及规律，如类别和聚类； 半监督学习利用少量的标注样本和大量的未标注样本进行训练和分类 增强学习机器人选择一个动作用于环境，环境接受该动作后状态发生变化，同时产生一个强化信号反馈回来，机器人根据强化信号和环境当前状态再选择下一个动作； 集成学习ensemble learning指联合训练多个弱分类器并通过集成策略将弱分类器组合使用；常见的集成策略有：boosting, bagging, random subspace, stacking；常见的算法主要有：决策树、随机森林、adaboost, GBDT, DART等； 深度学习源于人工神经网络的研究，通过层次化模型结构可从低层原始特征中逐渐抽象出高层次的语义特征； 元学习meta learning学会学习，利用以往的知识经验来指导新任务的学习，具有学会学习的能力 多任务学习联合训练多个学习任务 多标记学习处理的数据集中的每个样本可同时存在多个类标 对抗学习系统构成模式识别系统 机器学习","link":"/blog/2022/08/31/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture1-%E7%BB%AA%E8%AE%BA/"},{"title":"模式识别与机器学习学习笔记-数学知识","text":"数学期望(均值)和方差随机变量X的数学期望(或称均值）记作$E(x)$，它描述了随机变量的取值中心，随机变量$(X-E(X))^2$的数学期望称为$X$的方差，记作$\\sigma^2$，而$\\sigma$称为$X$的均方差（标准差）。它描述了随机变量与均值的偏差的疏密程度。 若$X$是连续型随机变量，其分布密度为$p(x)$，则当积分绝对收敛的时候 $$m = E(X)=\\int_{-\\infty}^{\\infty}xp(x)dx \\\\sigma^2 = E{(X-m)^2} = \\int_{-\\infty}^{\\infty}(x-m)^2p(x)dx$$ 若$X$是离散型随机变量，其可能取值为$x_k,k=1,2,…,$且$P(X=x_k)=p_k$，则（当级数是绝对收敛时）$$m = E(X)=\\sum^{\\infty}{k=1}x_kp_k \\D(X) = \\sum^{\\infty}{k=1}(x_k-m)^2p_k$$协方差矩阵协方差矩阵说明随机向量$X$的各分量的分散情况，定义为：$$\\begin{aligned}c&amp;=E{(X-m)(X-m)^T} \\&amp;=E\\left{\\left[\\begin{matrix}(X_1-m_1) \\\\vdots \\(X_1-m_1)\\end{matrix}\\right]\\left[\\begin{matrix}(X_1-m_1)\\cdots(X_n-m_n)\\end{matrix}\\right]\\right}\\&amp;=\\left[\\begin{matrix}E[(X_1-m_1)(X_1-m_1)]\\cdots E[(X_1-m_1)(X_n-m_n)] \\\\vdots \\ddots \\vdots \\E[(X_n-m_n)(X_1-m_1)]\\cdots E[(X_n-m_n)(X_n-m_n)]\\end{matrix}\\right] \\&amp;=\\left(\\begin{matrix}\\lambda_{11} \\cdots \\lambda_{1n} \\\\vdots \\ddots \\vdots \\\\lambda_{n1} \\cdots \\lambda_{nn}\\end{matrix}\\right)\\end{aligned}$$其中，协方差矩阵的各分量为：$$\\lambda_{ij} = E[(X_i-m_i)(X_j-m_j)]$$若$i \\neq j$，则$\\lambda_{ij}$是$X$的第$i$个分量与第$j$个分量的协方差；若$i = j$，则$\\lambda_{ij}$是随机变量$X_i$的方差，即协方差矩阵的对角分量；一维正态密度函数一维随机变量$X$的正态密度函数表示为：$$p(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left[-\\frac{(x-m)^2}{2\\sigma^2}\\right]$$其中均值$m=E(X)=\\int_{-\\infty}^{\\infty}xp(x)dx$；方差$\\sigma^2=E{(X-m)^2}=\\int_{-\\infty}^{\\infty}(x-m)^2p(x)dx$，$\\sigma$为标准差。在$m$左右各为$k\\sigma$的范围内，概率为：$$\\begin{aligned}p{m-k\\sigma\\leq x\\leq m+k\\sigma}&amp;= \\int_{m-k\\sigma}^{m+k\\sigma}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left[-\\frac{1}{2}\\left(\\frac{x-m}{\\sigma}\\right)^2\\right]dx \\&amp;= \\frac{1}{\\sqrt{2\\pi}}\\int_{-k}^k exp\\left[-\\frac{y^2}{2}\\right]dy\\end{aligned}$$其中，$y=(x-m)/\\sigma$，此时$p$与$k$的关系：$$p{m-k\\sigma \\leq x \\leq m + k\\sigma}=\\begin{cases} 0.683 &amp; k = 1 \\0.954 &amp; k = 2 \\0.997 &amp; k = 3\\end{cases}$$因此，在区间$|x-m|\\leq 3\\sigma$内，差不多包含了全部由正态样本取样的子样本。正态密度函数可完全由均值和方差所决定，因此可以由下式表示：$p(x)\\sim N(m,\\sigma^2)$ 多维正态密度函数$n$维随机向量的正态密度函数表示为：$$p(x)=\\frac{1}{(\\sqrt{2\\pi})^{\\frac{n}{2}}|C|^\\frac{1}{2}}exp\\left{-\\frac{1}{2}(x-m)^TC^{-1}(x-m)\\right}$$其中$$x=\\left[\\begin{matrix}x_1 \\\\vdots \\x_n\\end{matrix}\\right],m=\\left[\\begin{matrix}m_1 \\\\vdots \\m_n\\end{matrix}\\right],C=\\left[\\begin{matrix}\\sigma_{11}^2 &amp; \\cdots &amp; \\sigma_{1n}^2 \\\\vdots &amp; \\ddots &amp; \\vdots \\\\sigma_{n1}^2 &amp; \\cdots &amp; \\sigma_{nn}^2\\end{matrix}\\right]$$$|C|$为协方差矩阵$C$的行列式。多维正态密度函数由其均值$m$和协方差矩阵$C$确定，因此可用下式表示：$$p(x) \\sim N(m, C)$$","link":"/blog/2022/09/01/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/"},{"title":"卜算法学习笔记-latex必备知识","text":"开始第一个project创建一个.tax文件或者在overleaf中创建一个project 123456\\documentclass{article}\\begin{document}First document. This is a simple example, with no extra parameters or packages included.\\end{document} 第一行表示文章的类别class为article，还有一些类别，如book、report等； 之后的内容是文章的正文，包裹在\\begin(document}和\\end{document}中； 正文里面的内容可以进行编辑，编辑之后的内容要看效果需要点击recompile，在一些简单的编辑器里面，可以在命令行输入pdflatex &lt;your document&gt;之后就可以看到编辑之后的效果； 绪论preamble\\begin{document}之前的部分都是绪论，定义了文章类型、文章使用的语言等；首先看一下一个常见的绪论： 12\\documentclass[12pt, letterpaper]{article}\\usepackage[utf8]{inputenc} \\documentclass[12pt, letterpaper]{article}定义了字体大小12pt，还有一些尺寸(9pt, 11pt, 12pt)，默认为10pt;定义了纸张尺寸letterpaper，还有尺寸a4paper 和 legalpaper如果还有疑问访问page size and margins \\usepackage[utf8]{inputenc}定义编码 添加作者、标题、日期需要在正文中添加\\maketitle 1234567891011121314\\documentclass[12pt, letterpaper, twoside]{article}\\usepackage[utf8]{inputenc}\\title{First document}\\author{Hubert Farnsworth \\thanks{funded by the Overleaf team}}\\date{February 2017}\\begin{document}\\maketitleWe have now added a title, author and date to our first \\LaTeX{} document!\\end{document} 添加注释在注释前添加% 黑体、斜体、下划线 黑体：\\textbf{…} 斜体：\\textit{…} 下划线：\\underline{…} \\emph{…}：在正常字体中斜体，在斜体中就是正常字体 加载图片123456789101112131415\\documentclass{article}% 在文章中加载图片需要添加的包\\usepackage{graphicx}% 指示图片所在的文件夹\\graphicspath{ {images/} }\\begin{document}The universe is immense and it seems to be homogeneous, in a large scale, everywhere we look at.% 真正将图片添加在文章中的地方\\includegraphics{universe}There's a picture of a galaxy above\\end{document} 图片题目、说明和引用12345678910111213\\begin{figure}[h] \\centering \\includegraphics[width=0.25\\textwidth]{mesh} % 图片名称 \\caption{a nice plot} % 给图片标签 \\label{fig:mesh1}\\end{figure}% \\ref引用As you can see in the figure \\ref{fig:mesh1}, the function grows near 0. Also, in the page \\pageref{fig:mesh1} is the same example. 创建列表无序列表1234\\begin{itemize} \\item The individual entries are indicated with a black dot, a so-called bullet. \\item The text in the entries may be of any length.\\end{itemize} 有序列表1234\\begin{enumerate} \\item This is the first entry in our list \\item The list numbers increase with each entry we add\\end{enumerate} 数学公式需要添加\\usepackage{amsmath} 内联公式1\\( ... \\), $ ... $ or \\begin{math} ... \\end{math} 行内公式1\\[ ... \\], \\begin{displaymath} ... \\end{displaymath} or \\begin{equation} ... \\end{equation} 文章格式摘要1234567\\begin{document}\\begin{abstract}This is a simple paragraph at the beginning of the document. A brief introduction about the main subject.\\end{abstract}\\end{document} 开始一个新段落按两次换行 换行\\\\, \\newline chapter、section次序：part和chapter只可以在report和book中 1234567-1 \\part{part}0 \\chapter{chapter}1 \\section{section}2 \\subsection{subsection}3 \\subsubsection{subsubsection}4 \\paragraph{paragraph}5 \\subparagraph{subparagraph} 表格1234567\\begin{center}\\begin{tabular}{ c c c } cell1 &amp; cell2 &amp; cell3 \\\\ cell4 &amp; cell5 &amp; cell6 \\\\ cell7 &amp; cell8 &amp; cell9 \\end{tabular}\\end{center} \\begin{tabular}{ c c c }里面的{c c c}表示有三列，c表示居中，还有r和l 以下是一个添加分割线的例子 1234567891011121314151617\\begin{center} \\begin{tabular}{||c c c c||} \\hline Col1 &amp; Col2 &amp; Col2 &amp; Col3 \\\\ [0.5ex] \\hline\\hline 1 &amp; 6 &amp; 87837 &amp; 787 \\\\ \\hline 2 &amp; 7 &amp; 78 &amp; 5415 \\\\ \\hline 3 &amp; 545 &amp; 778 &amp; 7507 \\\\ \\hline 4 &amp; 545 &amp; 18744 &amp; 7560 \\\\ \\hline 5 &amp; 88 &amp; 788 &amp; 6344 \\\\ [1ex] \\hline\\end{tabular}\\end{center} 添加表标题123456789101112131415161718Table \\ref{table:data} is an example of referenced \\LaTeX{} elements.\\begin{table}[h!]\\centering\\begin{tabular}{||c c c c||} \\hline Col1 &amp; Col2 &amp; Col2 &amp; Col3 \\\\ [0.5ex] \\hline\\hline 1 &amp; 6 &amp; 87837 &amp; 787 \\\\ 2 &amp; 7 &amp; 78 &amp; 5415 \\\\ 3 &amp; 545 &amp; 778 &amp; 7507 \\\\ 4 &amp; 545 &amp; 18744 &amp; 7560 \\\\ 5 &amp; 88 &amp; 788 &amp; 6344 \\\\ [1ex] \\hline\\end{tabular}\\caption{Table to test captions and labels}\\label{table:data}\\end{table} 目录 section chapter自动识别目录，但是section*需要添加\\addcontentsline{toc}{section}{Unnumbered Section}123456789101112131415161718192021222324252627282930313233343536\\documentclass{article}\\usepackage[utf8]{inputenc} \\title{Sections and Chapters}\\author{Gubert Farnsworth}\\date{ } \\begin{document} \\maketitle \\tableofcontents\\section{Introduction} This is the first section. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Etiam lobortisfacilisis sem. Nullam nec mi et neque pharetra sollicitudin. Praesent imperdietmi nec ante. Donec ullamcorper, felis non sodales... \\section*{Unnumbered Section}\\addcontentsline{toc}{section}{Unnumbered Section}Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Etiam lobortis facilisissem. Nullam nec mi et neque pharetra sollicitudin. Praesent imperdiet mi necante...\\section{Second Section} Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Etiam lobortis facilisissem. Nullam nec mi et neque pharetra sollicitudin. Praesent imperdiet mi necante... \\end{document}","link":"/blog/2022/09/03/%E5%8D%9C%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-latex%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"},{"title":"自然语言处理学习笔记-lecture1-绪论","text":"基本概念 语言是个体之间由于沟通需要而制定的指令。 自然语言:人类之间用于沟通交流的语言。 自然语言的特点 线性:自然语言呈现为一种线性的符号序列。 层次性:自然语言内部存在层次结构。 歧义性:同一个自然语言句子存在多种不同的理解。 演化性:自然语言随着时代不断演化。 典型任务中文分词 输入:一段不带空格的汉语文本。 输出:以空格隔开词语的汉语文本。 示例程序：示例程序 词性标注 输入:给定一个词语的序列。 输出:输出一个对应的词性的序列。 示例程序：示例程序 文本分类 输入:一段文本 输出:该文本的类别。 示例程序：示例程序 语言模型 输入:给定一个词语序列 输出:预测下一个词 示例程序：示例程序1 示例程序2 语法改错 输入:一段可能包含语法错误的文本。 输出:识别出文本中的语法错误并进行修改。 示例程序：示例程序 句法分析 输入:一个自然语言句子 输出:句子的句法结构(短语结构或依存结构) 示例程序：示例程序1 示例程序2 拼音输入法 输入:拼音符号的序列 输出:汉字序列 示例程序：示例程序 情感分析 输入:一段自然语言文本。 输出:情感的类别(如正面、中性、负面) 示例程序：添加链接描述 语义角色标注 输入:一个自然语言句子。 输出:标出句子的谓语及相关语义角色。 示例程序：示例程序 语义分析 输入:一个自然语言处理句子 输出:该句子的语义表示形式 示例程序：示例程序 指代消解 输入:一段自然语言文本 输出:该文本中代词所指向的名词 示例程序：示例程序 机器翻译 输入:一段源语言文本 输出:一段目标语言文本 示例程序：示例程序 文本摘要 输入:一段自然语言长文本。 输出:一段能概括长文本核心意思的短文本。 示例程序：示例程序 对联生成 输入:对联的上联 输出:对联的下联以及横批 示例程序：示例程序 诗词生成 输入:诗词的关键词 输出:五绝、七绝、律诗或者词 示例程序：示例程序 问答系统 输入:一个自然语言问题。 输出:该问题的答案。 示例程序：示例程序 对话系统 输入:一个自然语言句子 输出:另一个自然语言句子作为回复 示例程序：示例程序 图像标题生成 输入:一张图像 输出:一个自然语言句子，对该图像的内容进行描述。 示例程序：示例程序 发展历史 1943：Warren McCulloch与Walter Pitts提出神经网络。 1949：Warren Weaver提出利用计算机自动翻译人类语言。 1950：Alan Turing提出“图灵测试”检验机器是否具备智能。 1955：Noam Chomsky提出形式语言体系，用数学描述语言。 1957：Frank Rosenblatt提出了感知机，推动了神经网络的发展。 1964：Joseph Weizenbaum研制聊天机器人ELIZA。 1965：Edward Feigenbaum提出专家系统DENDRAL。 1966：Leonard Baum和Lloyd Welch提出隐马尔科夫模型。 1970：CYK算法被提出并广泛用于上下文无关语言的分析。 1974：Paul Werbos为神经网络提出后向传播算法。 1984：Douglas Lenat提出了常识知识库Cyc。 1989：IBM公司提出著名的统计机器翻译IBM模型。 1993：宾夕法尼亚大学推出宾州树库，对句法分析研究起到极大推动作用。 1995：Vladimir Vapnik提出了支持向量机。 1996：Adwait Ratnaparkhi将最大熵模型引入自然语言处理 2001：Tim Berners-Lee提出语义网。 2003：Yoshua Bengio将分布式表示用于语言模型。 2006：Geoffrey Hinton引领了深度学习的兴起。 2011：IBM公司研制的“沃森”系统在知识问答任务中获胜。 2012：Google公司推出了知识图谱并在搜索引擎中使用。 2013：Google公司推出word2vec模型。 2014：Yoshua Bengio将注意力机制引入自然语言处理。 2017：Google公司提出Transformer模型。 2018：Google公司提出BERT预训练语言模型。小结 理性主义方法和经验主义方法交相辉映，齐头并进 – 理性主义:形式文法、专家系统、知识图谱 – 经验主义:隐马可夫模型、最大熵模型、神经网络 当前挑战 – 模型:过于依赖人工设计。 – 数据:标注数据严重不足。 – 训练:训练成本过于高昂。 – 推断:难以保证可靠可信。 相关资源学术机构 Association for Computational Linguistics – 创建时间:1962年– 机构网站:https://www.aclweb.org/– 自然语言处理领域影响力最大的国际学术机构。 中国中文信息学会 – 创建时间:1981年– 期刊网站:http://www.cipsc.org.cn/– 自然语言处理领域影响力最大的国内学术机构。 学术期刊 Computational Linguistics – 创建时间:1974年– 期刊网站:https://www.mitpressjournals.org/loi/coli– 自然语言处理领域传统上最好的国际期刊。 Transactions of the Association for Computaional Linguistcs – 创建时间：2013年– 期刊网站:https://transacl.org/index.php/tacl– 自然语言处理领域广受好评的顶级国际期刊。 学术会议 ACL:影响力最大，截稿时间一般在1月或2月。 EMNLP:偏重经验主义方法，截稿时间一般在5月。 NAACL:面向北美地区，截稿时间一般在11月。 AACL:面向亚太地区，截稿时间一般在6月。 COLING:传统的三大会议之一，截稿时间一般在5月。 IJCAI:人工智能会议，截稿时间一般在1月。 AAAI:人工智能会议，截稿时间一般在9月。 ICLR:机器学习会议，截稿时间一般在9月。 NeurIPS:机器学习会议，截稿时间一般在6月。","link":"/blog/2022/09/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture1-%E7%BB%AA%E8%AE%BA/"},{"title":"模式识别学习笔记-lecture2-统计判别1","text":"作为统计判别问题的模式分类 模式识别的目的就是要确定某一个给定的模式样本属于哪一类 输入：被识别对象的特征向量 输出：被识别样本的类别贝叶斯判别原则两类模式集的分类 目的：要确定$x$是属于$\\omega_1$类还是$\\omega_2$类，要看$x$是来自于$\\omega_1$类的概率大还是来自$\\omega_2$类的概率大。 根据概率判别规则，有：若$P(\\omega_1|x) \\gt P(\\omega_2|x)$，则$x \\in \\omega_1$若$P(\\omega_1|x) \\lt P(\\omega_2|x)$，则$x \\in \\omega_2$由贝叶斯定理，后验概率$P(\\omega_i|x)$可由类别$\\omega_i$的先验概率$P(\\omega_i)$和$x$的条件概率密度$P(x|\\omega_i)$来计算，即：$$P(\\omega_i|x)=\\frac{P(x|\\omega_i)P(\\omega_i)}{P(x)}=\\frac{P(x|\\omega_i)P(\\omega_i)}{\\sum_{i=1}^2{P(x|\\omega_i)P(\\omega_i)}}$$这里$P(x|\\omega_i)$也称为似然函数，将该式代入上述判别式，有：若$P(x|\\omega_1)P(\\omega_1) \\gt P(x|\\omega_2)P(\\omega_2)$，则$x \\in \\omega_1$若$P(x|\\omega_1)P(\\omega_1) \\lt P(x|\\omega_2)P(\\omega_2)$，则$x \\in \\omega_2$或若$l_{12}(x)=\\frac{P(x|\\omega_1)}{P(x|\\omega_2)} \\gt \\frac{P(\\omega_2)}{P(\\omega_1)}$，则$x \\in \\omega_1$若$l_{12}(x)=\\frac{P(x|\\omega_1)}{P(x|\\omega_2)} \\lt \\frac{P(\\omega_2)}{P(\\omega_1)}$，则$x \\in \\omega_2$其中，$l_{12}$称为似然比，$\\frac{P(\\omega_2)}{P(\\omega_1)}$称为似然比的判决阈值，此判别称为贝叶斯判别。 例题：假设某地发生地震事件$\\omega_1$的概率为0.2，$P(\\omega_1)=0.2$，则不发生地震的概率$\\omega_2$为0.8，$P(\\omega_2)=0.8$，已知地震通常与生物异常反应之间有一定的联系，生物是否发生异常这一结果以模式$x$表示，有两种取值，包括异常和正常，假设地震前一周内发生生物异常的概率是0.6，$P(x=异常|\\omega_1)=0.6$，地震前一周生物正常的概率是0.4，$P(x=正常|\\omega_1)=0.4$，一周之内没有发生地震但是生物异常的概率是0.1，，$P(x=异常|\\omega_2)=0.1$，一周之内没有发生地震生物正常的概率是0.9，$P(x=正常|\\omega_2)=0.9$求解：发生生物异常情况下一周之内发生地震的概率$$\\begin{aligned}P(\\omega_1|x=异常)&amp;= \\frac{P(x=异常|\\omega_1)P(\\omega_1)}{\\sum_{i=1}^2P(x=异常|\\omega_i)P(\\omega_i)} \\&amp;= \\frac{P(x=异常|\\omega_1)P(\\omega_1)}{P(x=异常|\\omega_1)P(\\omega_1) + P(x=异常|\\omega_2)P(\\omega_2) }\\&amp;= \\frac{0.6 \\times 0.2}{0.6 \\times 0.2 + 0.1 \\times 0.8} \\&amp;= 0.6\\end{aligned} \\似然比：l_{12} = \\frac{P(x=异常|\\omega_1)}{P(x=异常|\\omega_2)}=\\frac{0.6}{0.1}=6 \\判决阈值：\\theta_{21}=\\frac{P(\\omega_2)}{P(\\omega_1)}=\\frac{0.8}{0.2}=4$$ 贝叶斯最小风险判别 目的：考虑到某些类别的错误判断比另外一些类的错误判断风险更大，需要对贝叶斯判别做一些修正。 $M$类分类问题的条件平均风险$r_j(x)=\\sum_{i=1}^ML_{ij}P(\\omega_i|x)$ $L_{ij}$称为本应属于$\\omega_i$类的模式判别成属于$\\omega_j$类的是非代价，若$i=j$即判别正确，得分，$L_{ij}$可以取负值或零，表示不失分，若$i\\neq j$即判别错误，应该取正值。 分类器对每一个模式$x$有$M$种可能的类别可供选择，将$x$指定为具有最小风险值的那一类，则这种分类器称为最小平均条件风险分类器 按照贝叶斯公式，平均条件风险可写成：$r_j(x)=\\frac{1}{p(x)}\\sum_{i=1}^{M}L_{ij}P(x|\\omega_i)P(\\omega_i)$，舍去$\\frac{1}{P(x)}$这个公共项简化为$r_j(x)=\\sum_{i=1}^{M}L_{ij}P(x|\\omega_i)P(\\omega_i)$，这也是贝叶斯分类器，但是这个不是按错误概率最小作为标准，而是按平均条件风险作为标准。两类的情况$M=2$即全部的模式样本只有$\\omega_1$和$\\omega_2$两类，则平均风险可以写成：当分类器将$x$判别为$\\omega_1$时：$$r_1(x)=L_{11}P(x|\\omega_1)P(\\omega_1)+L_{21}P(x|\\omega_2)P(\\omega_2)$$当分类器将$x$判别为$\\omega_2$时：$$r_2(x)=L_{12}P(x|\\omega_1)P(\\omega_1)+L_{22}P(x|\\omega_2)P(\\omega_2)$$若$r_1(x)\\lt r_2(x)$，则$x$被判定为属于$\\omega_1$，此时：$$L_{11}P(x|\\omega_1)P(\\omega_1)+L_{21}P(x|\\omega_2)P(\\omega_2) \\lt L_{12}P(x|\\omega_1)P(\\omega_1)+L_{22}P(x|\\omega_2)P(\\omega_2)$$即：$$(L_{12}-L_{11})P(x|\\omega_1)P(\\omega_1) \\gt (L_{21}-L_{22})P(x|\\omega_2)P(\\omega_2)$$当$\\frac{P(x|\\omega_1)}{P(x|\\omega_2)} \\gt \\frac{P(\\omega_2)}{P(\\omega_1)} . \\frac{L_{21}-L_{22}}{L_{12}-L_{11}}$该式左边为似然比：$l_{12}=\\frac{P(x|\\omega_1)}{P(x|\\omega_2)}$右边为阈值：$\\theta_21=\\frac{P(\\omega_2)}{P(\\omega_1)} . \\frac{L_{21}-L_{22}}{L_{12}-L_{11}}$ 若$l_{12}(x) \\gt \\theta_{21}$，则$x \\in \\omega_1$ 若$l_{12}(x) \\lt \\theta_{21}$，则$x \\in \\omega_2$ 通常，当判别正确时，不失分，可选常数$L_{11}=L_{22}=0$；判别错误时，可选$L_{12}=L_{21}=1$，此时$\\theta_{21}=\\frac{P(\\omega_2)}{P(\\omega_1)}$ 例：一信号通过一受噪声干扰的信道，信道输入信号为0或1，噪声为高斯型，其均值为$\\mu=0$，方差为$\\sigma^2$，信道输出为$x$，试求最优的判别规则，从观察值$x$的基础上判别它是0还是1，直观上可以看出，若$x \\lt 0.5$应该判为0，$x \\gt 0.5$应该判为1。用贝叶斯判别条件分析，设信号送0的先验概率为$P(0)$，送1的先验概率为$P(1)$。当输入信号为0时，受噪声为正态分布$N(0,\\sigma^2)$的干扰，其幅值大小的概率密度为：$$P(x|\\omega_1)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{x^2}{2\\sigma^2}}$$当输入信号为0时，其幅值大小的概率密度为：$$P(x|\\omega_2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-1)^2}{2\\sigma^2}}$$则似然比为：$l_{12}=\\frac{P(x|\\omega_1)}{P(x|\\omega_2)} = e^{\\frac{1-2x}{2\\sigma^2}}$若$l_{12} \\gt \\theta_{21}$，即$e^{\\frac{1-2x}{2\\sigma^2}} \\gt \\theta_{21}$，则$x \\lt \\frac{1}{2} - \\sigma^2 ln\\theta_{21}$，则$x \\in \\omega_1$，此时信号是0，即：$$x \\lt \\frac{1}{2}-\\sigma^2ln\\left(\\frac{L_{21}}{L_{12}}.\\frac{P(1)}{P(0)}\\right)$$若取$L_{21}=L_{21}=1, P(1)=P(0)$，则$x \\lt \\frac{1}{2}$判为0若无噪声干扰，即$\\sigma^2=0$，则$x \\lt \\frac{1}{2}$判为0一般多类的情况对于$M$类情况，若$r_i(x) \\lt r_j(x), j = 1,2,\\cdots,M,j\\neq i$，则$x \\in \\omega_1$$L$可如下取值，判对失分为0，判错失分为1记：$$L_{ij}=\\begin{cases}0 &amp; when\\ i = j \\1 &amp; when\\ i \\neq j\\end{cases}$$则条件平均风险可写成：$$\\begin{aligned}r_j(x)&amp;= \\sum_{i=1}^ML_{ij}P(x|\\omega_i)P(\\omega_i) \\&amp;= L_{1j}P(x|\\omega_1)P(\\omega_1) + \\cdots + L_{jj}P(x|\\omega_j)P(\\omega_j) + \\cdots + L_{Mj}P(x|\\omega_M)P(\\omega_M) \\&amp;= \\sum_{i=1}^MP(x|\\omega_i)P(\\omega_i) - P(x|\\omega_j)P(\\omega_j) \\&amp;= P(x) - P(x|\\omega_j)P(\\omega_j)\\end{aligned}$$由$r_i(x) \\lt r_j(x)$，有当$P(x|\\omega_i)P(\\omega_i) \\gt P(x|\\omega_j)P(\\omega_j)$时，$x \\in \\omega_i$，对应于判别函数为：取$d_i(x)=P(x|\\omega_i)P(\\omega_i),i=1,2,\\cdots,M$，则对于全部$j \\neq i$的值，若$d_i(x) \\gt d_j(x)$，则$x \\in \\omega_i$","link":"/blog/2022/09/05/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture2-%E7%BB%9F%E8%AE%A1%E5%88%A4%E5%88%AB1/"},{"title":"模式识别学习笔记-lecture2-统计判别2","text":"正态分布模式的贝叶斯分类器当已知或有理由设想类概率密度函数$P(x|\\omega_i)$是多变量的正态分布时，贝叶斯分类器可以导出一些简单的判别函数 $M$种模式类别的多变量正态类密度函数具有$M$种模式类别的多变量正态类密度函数为：$$P(x|\\omega_i)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|C_i|^{\\frac{1}{2}}}exp\\left{-\\frac{1}{2}(x - m_i)^TC_i^{-1}(x - m_i)\\right}\\ i = 1,2,\\cdots,M$$其中每一类模式的分布密度都完全被其均值向量m_i和协方差矩阵$C_i$所规定，其定义为：$$\\begin{aligned}m_i &amp;= E_i{x} \\C_i &amp;= E_i{(x - m_i)(x - m_i)^T}\\end{aligned}$$$E_i{x}$表示对类别属于$\\omega_i$的模型的数学期望。在上述公式中，$n$为模式向量的维数，$|C_i|$为矩阵$C_i$的行列式，协方差矩阵$C_i$是对称的正定矩阵，其对角线上的元素$C_{kk}$是模式向量第$k$个元素的方差，非对角线上的元素$C_{jk}$是$x$的第$j$个分量$x_j$和第$k$个分量$x_k$的协方差。当$x_j$和$x_k$统计独立时，$C_{jk}=0$。当协方差矩阵的全部非对角线上的元素都为0时，多变量正态类密度函数可简化为$n$个单变量正态类密度函数的乘积。已知类别$\\omega_i$的判别函数可写成如下形式：$$d_i(x)=P(x|\\omega_i)P(\\omega_i), \\ i=1,2,\\cdots,M$$对于正态密度函数，可取自然对数的形式以方便计算(因为自然对数是单调递增的，取对数后不影响相应的分类性能)，则有：$$d_i(x)=ln[P(x|\\omega_i)] + lnP(\\omega_i), \\ i=1,2,\\cdots,M$$代入正态类密度函数，有：$$d_i(x) = lnP(\\omega_i)-\\frac{n}{2}ln(2\\pi)-\\frac{1}{2}ln|C_i|-\\frac{1}{2}(x - m_i)^TC_i^{-1}(x - m_i),\\ i=1,2,\\cdots,M$$去掉和$i$无关的项(并不影响分类结果)，有：$$d_i(x)=lnP(\\omega_i)-\\frac{1}{2}ln|C_i|-\\frac{1}{2}(x - m_i)^TC_i^{-1}(x - m_i),\\ i=1,2,\\cdots,M$$即为正态分布模式的贝叶斯判别函数，判别函数是一个超二次曲面 两类问题且其类模式都是正态分布的特殊情况当$C_1 \\neq C_2$时，两类模式的正态分布为：$P(x|\\omega_1)$表示为$N(m_1,C_1)$，$P(x|\\omega_2)$表示为$N(m_2,C_2)$，$\\omega_1, \\omega_2$两类的判别函数对应为：$$d_1(x) = lnP(\\omega_1)-\\frac{1}{2}ln|C_1|-\\frac{1}{2}(x - m_1)^TC_1^{-1}(x - m_1) \\d_2(x) = lnP(\\omega_2)-\\frac{1}{2}ln|C_2|-\\frac{1}{2}(x - m_2)^TC_2^{-1}(x - m_2) \\d_1(x)-d_2(x)=\\begin{cases}\\gt 0 &amp; x \\in \\omega_1 \\\\lt 0 &amp; x \\in \\omega_1\\end{cases}$$判别界面是$x$的二次型方程，当$x$是二维模式时，判别界面为二次曲线，如椭圆、圆、抛物线或双曲线等当$C_1 = C_2 = C$时，有：$$d_i(x)=lnP(\\omega_i)-\\frac{1}{2}ln|C|-\\frac{1}{2}x^TC^{-1}x+\\frac{1}{2}x^TC^{-1}m_i + \\frac{1}{2}m_i^TC^{-1}x - \\frac{1}{2}m_i^TC^{-1}m_i,\\ i = 1,2$$因$C$为对称矩阵，上式可简化为：$$d_i(x)=lnP(\\omega_i)-\\frac{1}{2}ln|C|-\\frac{1}{2}x^TC^{-1}x + m_i^TC^{-1}x - \\frac{1}{2}m_i^TC^{-1}m_i,\\ i = 1,2$$由此可导出类别$\\omega_1$和$\\omega_2$间的判别界面为：$$d_1(x)-d_2(x) = lnP(\\omega_1)-lnP(\\omega_2)+(m_1-m_2)^TC^{-1}x- \\frac{1}{2}m_1^TC^{-1}m_1 + \\frac{1}{2}m_2^TC^{-1}m_2 = 0$$判别界面是$x$的线性函数，为一超平面，当$x$是二维时，判别界面为一直线 例题$P(\\omega_1) = P(\\omega_2) = \\frac{1}{2}$，求其判别界面模式的均值向量$m_i$和协方差矩阵$C_i$可用下式估计：$$\\begin{aligned}m_1 &amp;= \\frac{1}{N_i}\\sum_{j = 1}^{N_i}x_{ij} \\ i = 1,2 \\C_i &amp;= \\frac{1}{N_i}\\sum_{j = 1}^{N_i}(x_{ij}-m_i)(x_{ij}-m_i)^T \\ i = 1,2\\end{aligned}$$其中$N_i$为类别$\\omega_i$中模式的数目，$x_{ij}$代表在第$i$个类别中的第$j$个模式，由上式可求出：$$m_1 = \\frac{1}{4}(3\\ 1\\ 1)T \\m_2 = \\frac{1}{4}(1\\ 3\\ 3)^T \\C_1 = C_2 = C = \\frac{1}{16}\\left(\\begin{matrix}3 &amp; 1 &amp; 1 \\1 &amp; 3 &amp; -1 \\1 &amp; -1 &amp; 3\\end{matrix}\\right),\\C^{-1} = 4\\left(\\begin{matrix}2 &amp; -1 &amp; -1 \\-1 &amp; 2 &amp; 1 \\-1 &amp; 1 &amp; 2\\end{matrix}\\right)$$设$P(\\omega_1)=P(\\omega_2)=\\frac{1}{2}$，因$C_1=C_2$，则判别界面为：$$\\begin{aligned}d_1(x)-d_2(x)&amp;=(m_1-m_2)^TC^{-1}x-\\frac{1}{2}m_1^TC^{-1}m_1+\\frac{1}{2}m_2^TC^{-1}m_2 \\&amp;= 8x_1-8x_2-8x_3 + 4 = 0\\end{aligned}$$ 均值向量和协方差矩阵的参数估计在贝叶斯分类器中，构造分类器需要知道类概率密度函数$P(x|\\omega_i)$，如果按照先验知识已经知道其分布，则只需要知道分布的参数即可 将参数作为非随机变量均值和协方差矩阵的估计量定义设模式的类概率密度函数为$p(x)$，则其均值向量定义为：$$m = E(x) = \\int_xxP(x)dx$$其中，$x = (x_1,x_2,\\cdots,x_n)^T,m = (m_1,m_2,\\cdots,m_n)^T$。若以样本的平均值作为均值向量的近似值，则均值估计量$\\hat{m}$为：$$\\hat{m} = \\frac{1}{N}\\sum_{j=1}^Nx_j$$其中$N$为样本的数目。协方差矩阵为：$$C = \\left(\\begin{matrix}c_{11} &amp; c_{12} &amp; \\cdots &amp; c_{1n} \\c_{21} &amp; c_{22} &amp; \\cdots &amp; c_{2n} \\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\c_{n1} &amp; c_{n2} &amp; \\cdots &amp; c_{nn}\\end{matrix}\\right)$$其每个元素$c_{lk}$定义为：$$\\begin{aligned}c_{lk} &amp;= E{(x_l-m_l)(x_k-m_k)} \\&amp;= \\int^{\\infty}{-\\infty}\\int^{\\infty}{-\\infty}(x_l-m_l)(x_k-m_k)P(x_l，x_k)dx_ldx_k\\end{aligned}$$其中，$x_l,x_k,m_l,m_k$分别是$x,m$的第$l,k$个分量。协方差矩阵写成向量形式为：$$C = E{(x - m)(x - m)^T} = E{xx^T}-mm^T$$协方差矩阵的估计量(当$N \\gg 1$为：$$\\hat{C} \\approx \\frac{1}{N}\\sum^N_{k=1}(x_k-\\hat{m})(x_k-\\hat{m})^T$$这里样本模式总体为${x_1,x_2,\\cdots,x_k,\\cdots,x_N}$，为因为计算估计量时没有真实的均值向量$m$可用，只能用均值向量的估计量来代替，会存在偏差。均值和协方差矩阵估计量的迭代运算形式假设已经计算了$N$个样本的均值估计量，若再加上一个样本，其新的估计量$\\hat{m}(N+1)$为：$$\\hat{m}(N+1) = \\frac{1}{N+1}\\sum_{j=1}^{N+1}x_j = \\frac{1}{N+1}\\left[\\sum_{j=1}^Nx_j+x_{N+1}\\right] = \\frac{1}{N+1}\\left[N\\hat{m}(N) + x_{N+1}\\right]$$其中$\\hat{m}(N)$为从$N$个样本计算得到的估计量，迭代的第一步应取$\\hat{m}(1)=x_1$。协方差矩阵的估计量的迭代运算与上述相似，取$\\hat{C}(N)$表示$N$个样本时的估计量为：$$\\hat{C}(N) = \\frac{1}{N}\\sum_{j=1}^Nx_jx_j^T - \\hat{m}(N)\\hat{m}^T(N)$$加入一个样本，则：$$\\begin{aligned}\\hat{C}(N+1)&amp;= \\frac{1}{N+1}\\sum_{j=1}^{N+1}x_jx_j^T - \\hat{m}(N+1)\\hat{m}^T(N+1) \\&amp;= \\frac{1}{N+1}\\left[\\sum_{j=1}^{N}x_jx_j^T + x_{N+1}x_{N+1}^T\\right] - \\hat{m}(N+1)\\hat{m}^T(N+1) \\&amp;= \\frac{1}{N+1}\\left[N\\hat{C}(N)+N\\hat{m}(N)\\hat{m}^T(N) + x_{N+1}x_{N+1}^T\\right] - \\frac{1}{(N+1)^2}\\left[N\\hat{m}(N) + x_{N+1}\\right]\\left[N\\hat{m}(N) + x_{N+1}\\right]^T\\end{aligned}$$其中$\\hat{C}(1) = x_1x_1^T - \\hat{m}(1)\\hat{m}^T(1)=0$是零矩阵 将参数看做随机变量设${x_1,x_2,\\cdots,x_N}$为$N$个用于估计一未知参数$\\theta$的密度函数的样本，$x_i$被一个接一个的逐次给出，于是用贝叶斯定理，可以得到在给定了$x_1,x_2,\\cdots,x_N$之后，$\\theta$的后延概率密度的迭代表示式为：$$P(\\theta|x_1,\\cdots,x_N)=\\frac{P(x_N|\\theta,x_1,\\cdots,x_{N-1})P(\\theta|x_1,\\cdots,x_{N-1})}{P(x_N|x_1,\\cdots,x_{N-1})}$$其中对于$P(\\theta|x_1,\\cdots,x_N)$而言，$P(\\theta|x_1,\\cdots,x_{N-1})$是它的先验概率，当加入了新的样本$x_N$后，得到修正之后的新的概率密度$P(\\theta|x_1,\\cdots,x_N)$。如此一步步向前推，则$P(\\theta)$是最初的先验概率密度，当读入第一个样本$x_1$时，经过贝叶斯定理计算，可得到后验概率密度$P(\\theta|x_1)$。以此为新的一步，将$P(\\theta|x_1)$作为第二部计算的先验概率密度，读入样本$x_2$，又得到第二步的后验概率密度$P(\\theta|x_1,x_2)$，……，以此可以算出最终的后延概率密度$P(\\theta|x_1,\\cdots,x_N)$，从而得到最终的结果。这里需要知道最初始的概率密度$P(\\theta)$和全概率$P(x_N|x_1,\\cdots,x_{N-1})$，全概率可以通过下式算出：$$P(x_N|x_1,\\cdots,x_{N-1}) = \\int_xP(x_N|\\theta,x_1,\\cdots,x_{N-1})P(\\theta|x_1,\\cdots,x_{N-1})d\\theta$$这一个值和未知量$\\theta$无关，可以认为是一个定值。","link":"/blog/2022/09/10/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture2-%E7%BB%9F%E8%AE%A1%E5%88%A4%E5%88%AB2/"},{"title":"离散数学学习笔记-02-命题逻辑","text":"命题proposition非真既假的普通陈述句，真值true/false唯一确定，（本命题是假的）和（本命题是真的）不是命题 命题变元或命题变项proposition variables小写英文字母表示 原子命题：atom proposition/简单命题 simple proposition 复合命题：compound proposition 命题联结词proposition connective或命题运算符proposition operator 否定词：$\\sim p，非p$ 合取词：$conjunction,p \\bigwedge q,p且q$ 析取词：$disjunction,p \\bigvee q, p或q$ 异或词：$exclusive\\ or,p \\bigoplus q,p和q真值相同时为假$ 蕴涵词：$implication，p \\Rightarrow q，p真q假时p \\Rightarrow q为假$$p$：前提(premise)，$q$：结论(conclusion)$p$是$q$的充分条件，$q$是$p$的必要条件对于命题$p \\Rightarrow q$，$q \\Rightarrow p$为其逆命题，$-p \\Rightarrow -q$为其否命题，$-q \\Rightarrow -p$为其逆否命题 等价词：$equivalence，p \\Leftrightarrow q，p和q真值相同时为真，双条件，p是q的充分必要条件，p和q等价$ 命题公式及其分类命题公式(well formed formula, wff)合式公式，简称公式 单个命题变项变项$p,q,r,\\cdots$是命题公式 如果$A$是命题公式，那么$\\sim A$也是命题公式 如果$A$和$B$是命题公式，那么由逻辑联结词连接的符号串也是命题公式 有限次应用上面三条构成的符号串才是命题公式，只有用命题公式表示的符号串才是命题，且该公式的每一命题变项真值确定 ${\\sim} \\gt {\\bigwedge,\\bigvee} \\gt {\\Rightarrow,\\Leftrightarrow}$ n元命题公式n元命题公式：含有n个命题变项的命题公式$(p_1,p_2,p_3,\\cdots,p_n)$ 对变项组$(p_1,p_2,p_3,\\cdots,p_n)$指定的一组确定真值称为该公式的一个真值指派或赋值，若使之真值为真，则称这组值为成真指派或成真赋值，若使之真值为假，则称这组值为成假指派或成假赋值 真值表truth table：$2^n$行对应$2^n$个真值指派 若所有$2^n$个赋值都是成真赋值，则称A为永真式或重言式tautology 若所有$2^n$个赋值都是成假赋值，则称A为永假式或矛盾式contradiction 若至少存在一个成真指派，则称A为可满足式，satisfiable formula 若至少存在一个成真指派和一个成假指派，则称A为非重言的可满足式 两个重言式或两个矛盾式的析取或合取任然是重言式、矛盾式 结果可能为真可能为假为不定式contingency 如果一个命题公式是重言式，则一定是不定式 命题逻辑的等值演算等值equivalent，逻辑等值logically equivalent$A \\Leftrightarrow B$是一个重言式，记作$A \\equiv B$，称$A \\equiv B$为等值式，表示对于任意的真值指派，A、B的真值均相同证明两公式等值：真值表法、等值演算法证明两公式不等值：找出一个真值指派使一个真值为真一个为假 基本等值式假设$p,q,r$为任意命题 双重否定率：$p \\equiv \\sim(\\sim p)$ 幂等律：$p \\bigvee p \\equiv p,p \\bigwedge p \\equiv p$ 交换律：$p \\bigvee q \\equiv q \\bigvee p, p \\bigwedge q \\equiv q \\bigwedge p$ 结合律：$(p \\bigvee q) \\bigvee r \\equiv p \\bigvee (q \\bigvee r),(p \\bigwedge q) \\bigwedge r \\equiv p \\bigwedge (q \\bigwedge r)$ 分配率：$p \\bigvee (q \\bigwedge r) \\equiv (p \\bigvee q) \\bigwedge (p \\bigvee r),p \\bigwedge (q \\bigvee r) \\equiv (p \\bigwedge q) \\bigvee (p \\bigwedge r)$ 德摩根律：$\\sim(p \\bigvee q) \\equiv (\\sim p) \\bigwedge (\\sim q),\\sim(p \\bigwedge q) \\equiv (\\sim p) \\bigvee (\\sim q)$ 吸收律：$p \\bigvee (p \\bigwedge q) \\equiv p,p \\bigwedge (p \\bigvee q) \\equiv p$ 零律：$p \\bigvee T \\equiv T,p \\bigwedge F \\equiv F$ 同一律：$p \\bigvee F \\equiv p,p \\bigwedge T \\equiv p$ 排中律：$p \\bigvee \\sim p \\equiv T$ 矛盾律：$p \\bigwedge \\sim p \\equiv F$ 蕴含等值式：$p \\Rightarrow q \\equiv \\sim p \\bigvee q$ 等价等值式：$p \\Leftrightarrow q \\equiv (p \\Rightarrow q) \\bigwedge (q \\Rightarrow p)$ 假言易位：$p \\Rightarrow q \\equiv (\\sim q) \\Rightarrow (\\sim p)$ 等价否定等值式：$p \\Leftrightarrow q \\equiv \\sim p \\Leftrightarrow \\sim q$ 归谬论：$(p \\Rightarrow q) \\bigwedge (p \\Rightarrow \\sim q) \\equiv \\sim p$ 代入规则：假设A是一个重言式，那么A里的命题变项每一项替换为别的，不论真值如何，替换以后任然为重言式置换规则：若$A \\equiv B$，则用命题公式B替换命题公式$\\Phi(A)$中的A后形成的$\\Phi(B)$(不一定替换每一处），$\\Phi(A) \\equiv \\Phi(B)$","link":"/blog/2022/09/10/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-02-%E5%91%BD%E9%A2%98%E9%80%BB%E8%BE%91/"},{"title":"离散数学学习笔记-01-基础知识","text":"集合与序列集合的基本概念集合set：大写字母表示S 集合的元素：小写字母表示a，$a \\in S$集合的特点： 能够明确的判断一个元素是或不是属于某集合 集合的元素没有顺序 集合的元素之间不一定存在什么关系 规定：对任意集合A都有$A \\notin A$ 特殊集合：自然数：N，整数：Z，正整数：$Z^+$，非零整数集$Z^*$，有理数集$Q$，非零有理数集$Q^*$，实数$R$，非零实数集$R^*$，复数$C$ 集合的表示： 外延表示法（列举法）${\\cdots}$ 内涵表示法（描述法) ${x|P(x)}$ 子集和超集：$A \\subseteq B$$A = B:A \\subseteq B\\ and\\ B \\subseteq A$全集：$U$空集：$\\emptyset$ 基数、势cardinality：集合中的元素数$|A|, #A, card(A)$幂集（power set）：A的所有子集所组成的集合，$\\oint(A) = {x|x \\subset A}$其中有空集和A本身 证明$X \\subseteq Y$的基本方法是：对于任意的$x \\in X$，有$x \\in Y$证明两个集合相等的方法是分别证明$X \\subseteq Y \\ and\\ Y \\subseteq X$ 集合的运算 交集 $A \\bigcap B, intersection$ 并集 $A \\bigcup B, union$ B关于A的相对补(complement of B with respect to A)或A与B的差集(difference):$A - B=A\\overline{B}$ 补集$\\overline{A},\\sim A, complement$，A关于U的相对补 对称差$A\\bigoplus B = {x|x \\in A\\ or\\ x \\in B\\ and\\ x \\notin A\\bigcap B = (A - B) \\bigcup (B - A), symmetric\\ difference$ 集合运算的性质 交换律：$$A \\bigcup B = B \\bigcup A \\A \\bigcap B = B \\bigcap A \\A \\bigoplus B = B \\bigoplus A$$ 结合律：$$(A \\bigcup B) \\bigcup C = A \\bigcup (B \\bigcup C) \\(A \\bigcap B) \\bigcap C = A \\bigcap (B \\bigcap C) \\(A \\bigoplus B) \\bigoplus C = A \\bigoplus (B \\bigoplus C)$$ 分配率：$$A \\bigcup (B \\bigcap C) = (A \\bigcup B) \\bigcap ( A \\bigcup C) \\A \\bigcap (B \\bigcup C) = (A \\bigcap B) \\bigcup ( A \\bigcap C)$$ 吸收率：$$A \\bigcup (A \\bigcap B) = A \\A \\bigcap ( A \\bigcup B) = A$$ 德摩根律：绝对形式：$$\\overline{A \\bigcup B} = \\overline{A} \\bigcap \\overline{B} \\\\overline{A \\bigcap B} = \\overline{A} \\bigcup \\overline{B}$$相对形式：$$A - (B \\bigcup C) = (A - B) \\bigcap (A - C) \\A - (B \\bigcap C) = (A - B) \\bigcup (A - C)$$ 幂等律：$$A \\bigcap A = A \\A \\bigcup A = A$$ 零律：$$A \\bigcup U = U\\A \\bigcap \\emptyset = \\emptyset$$ 同一律：$$A \\bigcap U = A \\A \\bigcup \\emptyset = A$$ 排中律：$$A \\bigcup \\overline{A} = U$$ 矛盾律：$$A \\bigcap \\overline{A} = \\emptyset$$ 序列的基本概念 sequence：排成一列的对象，有顺序，里面的对象为项item； 对于给定的集合A，$A^*$为所有由A种元素生成的有限长度序列全体，$A^*$中的元素称为A上的词word或串string； 假设$A = {a,b,c,\\cdots,z}$，则$A^*$中包括的为若干单词，$A^*$中的空序列称作空串empty string，记作$\\lambda,\\varepsilon$； 假设A是集合，$w_1 = s_1s_2\\cdots s_n,w_2 = t_1t_2\\cdots t_n$都是$A^*$中的元素，则$w_1,w_2$的连接catenation为$s_1s_2\\cdots s_nt_1t_2\\cdots t_n$记作$w_1 \\circ w_2$ 布尔矩阵布尔矩阵boolean matrix，位矩阵bit matrix $A = [a_{ij}]$是一个$m \\times n$的布尔矩阵，则定义其补complement为$\\overline{A} = [\\overline{a_{ij}}] = [1- a_{ij}]$ $A = [a_{ij}]$和$B = [b_{ij}]$都是$m \\times n$的布尔矩阵$$A \\bigcap B \\A \\bigcup B$$ $A = [a_{ij}]$是$m \\times n$矩阵，$B = [b_{ij}]$是$n \\times r$矩阵，布尔积boolean product，$A \\odot B = C = [c_{ij}]$$$c_{ij} =\\begin{cases}1 &amp; 若存在k，1 \\leq k \\leq n 使得a_{ik} = 1且b_{kj} = 1\\0 &amp; otherwise\\end{cases}$$ 定理：$$A \\bigcup B = C = [c_{ij}] \\ c_{ij} = a_{ij} + b_{ij} - a_{ij}b_{ij} \\A \\bigcap B = D = [d_{ij}] \\ d_{ij} = a_{ij}b_{ij}$$ 交换律：$$A \\bigcap B = B \\bigcap A \\A \\bigcup B = B \\bigcup A$$ 结合律：$$(A \\bigcup B) \\bigcup C = A \\bigcup (B \\bigcup C) \\(A \\bigcap B) \\bigcap C = A \\bigcap (B \\bigcap C) \\(A \\odot B) \\odot C = A \\odot (B \\odot C)$$ 分配率：$$A \\bigcap (B \\bigcup C) = (A \\bigcap B) \\bigcup (A \\bigcap C) \\A \\bigcup (B \\bigcap C) = (A \\bigcup B) \\bigcap (A \\bigcup C) \\(A \\bigcup B)^T = A^T \\bigcup B^T \\(A \\bigcap B)^T = A^T \\bigcap B^T \\(A \\odot B)^T = B^T \\odot A^T \\$$","link":"/blog/2022/09/10/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-01-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"title":"离散数学学习笔记-02-对偶和范式","text":"对偶式dual，假设A为仅含有$\\sim,\\bigvee,\\bigwedge$的命题公式，若将$A$中的$\\bigwedge$换成$\\bigvee$，$\\bigvee$换成$\\bigwedge$，若包含$F$和$T$则相互取代，即得$A^*$，$A$的对偶式。$(A^*)^* = A$ 定理1：设$A$为一个仅含有$\\sim,\\bigvee,\\bigwedge$的命题公式，$p_1,p_2,\\cdots,p_n$为其命题变项，则：$\\sim A(p_1,p_2,\\cdots,p_n) \\equiv A^*(\\sim p_1, \\sim p_2,\\cdots,\\sim p_n)$若$A$为重言式，则$A^*$必为矛盾式 定理2：设$A$和$B$为仅含联结词$\\sim,\\bigvee,\\bigwedge$的命题公式，若$A \\equiv B$，则$A^* \\equiv B^*$ 析取范式与合取范式基础概念 文字：literal，$p,\\sim p$且$p$和$\\sim p$称为互补对 析取式：fundamental disjunction有限个文字的析取组成的公式 合取式：fundamental conjunction有限个文字的合取组成的公式 析取范式：disjunction normal form，$A_1 \\bigvee A_2 \\bigvee \\cdots \\bigvee A_n$，$A_1,A_2,\\cdots,A_n$为合取式，n个合取式的析取称为析取范式 合取范式：conjunction normal form，$A_1 \\bigwedge A_2 \\bigwedge \\cdots \\bigwedge A_n$，$A_1,A_2,\\cdots,A_n$为析取式，n个析取式的合取称为合取范式 定理：任何析取范式的对偶式为合取范式，任何合取范式的对偶式为析取范式，设$B$为$A^*$的析取范式，则$B^*$为$A$的合取范式 求范式的步骤： 将$\\Rightarrow,\\Leftrightarrow$联结词转换为$\\sim,\\bigvee,\\bigwedge$$$p \\Rightarrow q \\equiv \\sim{p} \\bigvee q \\p \\Leftrightarrow q \\equiv (\\sim p \\bigvee q) \\bigwedge (\\sim q \\bigvee p)求合取范式 \\p \\Leftrightarrow q \\equiv (p \\bigwedge q) \\bigvee (\\sim p \\bigwedge \\sim q)求析取范式$$ 简化$\\sim$，使$\\sim$仅作用于命题变式 利用分配率，使其最终变为合取范式或析取范式范式存在定理任一命题公式都存在着与之等值的析取范式和合取范式极小项minterm合取式中的命题变项中，$p$和$\\sim p$只存在一个，$n$个命题变项构成$2^n$个极小项，这些极小项仅在一种情况下为真例：3个命题变项，8个极小项$$\\begin{matrix}\\sim p \\bigwedge \\sim q \\bigwedge \\sim r &amp; 000-0 &amp; m_0 \\\\sim p \\bigwedge \\sim q \\bigwedge r &amp; 001-0 &amp; m_1 \\\\vdots &amp;\\ &amp; \\vdots \\p \\bigwedge q \\bigwedge r &amp; 111-7 &amp; m_7\\end{matrix}$$主析取范式full disjunctive normal form，用$\\sum$表示，由极小项的析取构成的析取范式求主析取范式的步骤： 求$A$的一个析取范式$A’$ 若$A’$的某合取式$B$不含命题变项$p_i$或$\\sim p_i$，则将$B$展开成$$B \\equiv B \\bigwedge (p_i \\bigvee \\sim p_i) \\equiv (B \\bigwedge p_i) \\bigvee (B \\bigwedge \\sim p_i)$$ 化简 将极小项由小到大排列$m_1 \\bigvee m_2 \\bigvee m_5$用$\\sum(1,2,5)$表示 极大项maxterm析取式中的命题变项中，$p$和$\\sim p$只存在一个，$n$个命题变项构成$2^n$个极小项，这些极小项仅在一种情况下为假例：3个命题变项，8个极大项$$\\begin{matrix}\\sim p \\bigvee \\sim q \\bigvee \\sim r &amp; 111-7 &amp; M_7 \\\\sim p \\bigvee \\sim q \\bigvee r &amp; 110-6 &amp; M_6 \\\\vdots &amp;\\ &amp; \\vdots \\p \\bigvee q \\bigvee r &amp; 000-0 &amp; M_0\\end{matrix}$$ 主合取范式full conjunctive normal form，用$\\prod$表示，由极大项的合取构成的合取范式求主合取范式的步骤： 求$A$的一个合取范式$A’$ 若$A’$的某析取式$B$不含命题变项$p_i$或$\\sim p_i$，则将$B$展开成$$B \\equiv B \\bigvee (p_i \\bigwedge \\sim p_i) \\equiv (B \\bigvee p_i) \\bigwedge (B \\bigvee \\sim p_i)$$ 化简 将极大项由小到大排列$M_1 \\bigwedge M_2 \\bigwedge M_5$用$\\prod(1,2,5)$表示 定理： 主析取范式和主合取范式是唯一的 得知主析取范式和主合取范式中的任意一个都可以很快的得到另一个。$\\sum(2,4,5,6,7) \\equiv \\prod(0,1,3)$ 恰由$2^n$个极小项构成的公式必为重言式 恰由$2^n$个极大项构成的公式必为矛盾式 命题逻辑的推理推理：从前提推出结论的思维过程，前提premise或称假设hypothesis是指已知的命题公式$A_1,A_2,\\cdots,A_n$，结论conclusion是从前提出发应用推理规则推出的命题公式$B$ 基本推理公式 附加率：$A \\Rightarrow (A \\bigvee B)$ 化简律：$(A \\bigwedge B) \\Rightarrow A$ 前后件附加：$$(A \\Rightarrow B) \\Rightarrow ((A \\bigvee C) \\Rightarrow (B \\bigvee C)) \\(A \\Rightarrow B) \\Rightarrow ((A \\bigwedge C) \\Rightarrow (B \\bigwedge C)) \\(A \\Rightarrow B) \\Rightarrow ((C \\Rightarrow A) \\Rightarrow (C \\Rightarrow B)) \\$$ 对偶：$(A \\Rightarrow B) \\Rightarrow (B^* \\Rightarrow A^*)$ 假言推理、分离式：$((A \\Rightarrow B) \\bigwedge A) \\Rightarrow B$ 拒取式：$(A \\Rightarrow B) \\bigwedge \\sim B \\Rightarrow \\sim A$ 析取三段论：$(A \\bigvee B) \\bigwedge \\sim B \\Rightarrow A$ 假言三段论：$(A \\Rightarrow B) \\bigwedge (B \\Rightarrow C) \\Rightarrow (A \\Rightarrow C)$ 等价三段论：$(A \\Leftrightarrow B) \\bigwedge (B \\Leftrightarrow C) \\Rightarrow (A \\Leftrightarrow C)$ 构造性二难：$(A \\Rightarrow B) \\bigwedge (C \\Rightarrow D) \\bigwedge (A \\bigvee C) \\Rightarrow (B \\bigvee D)$ 构造性二难（特殊形式）：$$(A \\Rightarrow C) \\bigwedge (B \\Rightarrow C) \\bigwedge (A \\bigvee B) \\Rightarrow C \\(A \\Rightarrow B) \\bigwedge (\\sim A \\Rightarrow B) \\Rightarrow B$$ 破坏性二难：$(A \\Rightarrow B) \\bigwedge (C \\Rightarrow D) \\bigwedge (\\sim B \\bigvee \\sim D) \\Rightarrow (\\sim A \\bigvee \\sim C)$ 附加前提证明法additional premise前提A，结论$C \\Rightarrow B$可以转化为证明前提A，C，结论B 归谬法(反证法）negation of conclusion前提A，结论B可以转为证明前提$A,\\sim B$，结论得到矛盾 归结法resolution 将$A \\bigwedge \\sim B$化为合取范式$C_1 \\bigwedge C_2 \\bigwedge \\cdots \\bigwedge C_n$，各个$C_i$构成子句集$S = {C_1,C_2,\\cdots,C_n}$ 对S中的子句作归结 直至归结出矛盾式","link":"/blog/2022/09/10/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-02-%E5%AF%B9%E5%81%B6%E5%92%8C%E8%8C%83%E5%BC%8F/"},{"title":"离散数学学习笔记-03-谓词","text":"谓词和量词个体词individual相当于名词，个体常项$a,b,c$，个体变项$x,y,z$，个体变项的取值范围$D$称为个体域或论域，全总个体域是指宇宙间一切事物组成的个体域 谓词 一元谓词：表示个体词的性质，属性$P(x),Q(x)$ 多元谓词：表示个体词之间的关系$P(x,y),R(x,y,z)$ 命题是确定谓词含义的零元谓词 量词 全称量词：universal quantification $(\\forall x)P(x)$ 存在量词：existential quantification $(\\exists x)P(x)$ 谓词公式及其分类谓词公式(合式) 若A是谓词公式，且A种无$\\forall x,\\exists x$出现则$(\\forall x)A(x),(\\exists x)A(x)$也是 若A是，$\\sim A$也是 若A，B是，则由逻辑联结词联结的也是 谓词公式的分类 普遍有效的公式、逻辑有效式：在任何解释下真值为真 不可满足式：在任何解释下真值均为假 可满足式：至少存在一个解释使之为真 定理：丘奇图灵，谓词逻辑是不可判定的，即对任一谓词公式而言，没有一个可行的办法判定它是否是普遍有效的 代换实例：设命题公式$A$含有命题变项$P_1,P_2,\\cdots,P_n$，用n个谓词公式$A_1,A_2,\\cdots,A_n$分别处处代换$P_1,P_2,\\cdots,P_n$所得公式是原公式的代换实例 定理：重言式、矛盾式的代换实例仍然为重言式、矛盾式 $(\\forall x)A(x),(\\exists x)A(x)$中$\\forall,\\exists$之后的x称为量词的指导变项或作用变项，$A(x)$称为相应量词的辖域或作用域，在$A(x)$中的x的出现称为约束出现，x称为约束变项，其他变项称为自由变项，若无自由变项即为命题 自然语言的形式化 将问题分解为一些原子命题和逻辑联结符 分解出各个原子命题的个体词、谓词、量词 按照合式公式的原则翻译出自然语句 例：令谓词$P(x)$表示x是整数，$Q(x)$表示x是奇数，$R(x)$表示x是偶数，$S(x)$表示x是素数，$E(x,y)$表示$x = y$，$G(x,y)$表示$x &gt; y$ 所有的素数都是整数：$(\\forall x)(S(x) \\Rightarrow P(x))$ 谓词逻辑的等值演算等值：谓词公式$A, B$等值，$A \\equiv B$，$A,B$在任何解释下真值均相同 消去量词等值式设论域$D = {a_1,a_2,\\cdots,a_m}$$$(\\forall x)A(x) \\equiv A(a_1) \\bigwedge A(a_2) \\bigwedge \\cdots \\bigwedge A(a_m) \\(\\exists x)A(x) \\equiv A(a_1) \\bigvee A(a_2) \\bigvee \\cdots \\bigvee A(a_m)$$ 量词否定等值式、德摩根律$$\\sim (\\forall x)A(x) \\equiv (\\exists x)\\sim A(x)\\\\sim (\\exists x)A(x) \\equiv (\\forall x)\\sim A(x)$$ 量词辖域收缩与扩张等值式$$(\\forall x)(A(x) \\bigvee B) \\equiv (\\forall x)A(x) \\bigvee B \\(\\forall x)(A(x) \\bigwedge B) \\equiv (\\forall x)A(x) \\bigwedge B \\(\\exists x)(A(x) \\bigvee B) \\equiv (\\exists x)A(x) \\bigvee B \\(\\exists x)(A(x) \\bigwedge B) \\equiv (\\exists x)A(x) \\bigwedge B$$ 量词分配等值式$$(\\forall x)(A(x) \\bigwedge B(x)) \\equiv (\\forall x)A(x) \\bigwedge (\\forall x)B(x) \\(\\exists x)(A(x) \\bigvee B(x)) \\equiv (\\exists x)A(x) \\bigvee (\\forall x)B(x) \\(\\forall x)(\\forall y)A(x,y) \\equiv (\\forall y)(\\forall x)A(x,y) \\(\\exists x)(\\exists y)A(x,y) \\equiv (\\exists y)(\\exists x)A(x,y)$$ 对偶式dual，在仅含联结词$\\sim, \\bigwedge,\\bigvee$的谓词公式$A$中，将$(\\bigwedge,\\bigvee$)、($\\forall,\\exists$)、($F,T$)互换，若$A \\equiv B$则$A^* \\equiv B^*$ 置换规则设$\\Phi(A)$是含谓词公式$A$的公式，$\\Phi(B)$是用谓词公式$B$取代$\\Phi(A)$中的$A$，不一定是每一处之后得到的谓词公式，若$A \\equiv B$则$\\Phi(A) \\equiv \\Phi(B)$ 代替规则将谓词公式中某个自由出现的个体变项的所有自由出现改成改成$A$中未曾出现的符号，所得$A’ \\equiv A$ 换名规则将谓词公式A中某量词的指导变项及其辖域内的所有约束出现改成A中未出现的符号所得$A’ \\equiv A$","link":"/blog/2022/09/10/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-03-%E8%B0%93%E8%AF%8D/"},{"title":"自然语言处理学习笔记-lecture2-数学基础1-微积分","text":"微积分函数设数集$D \\subset \\mathbb{R}$，则称映射$f : D \\subset \\mathbb{R}$为定义在$D$上的函数，通常记为 $y = f(x), x ∈ D$，其中$x$称为自变量，$y$称为因变量，$D$称为定义域，记作$D_f$，即$D_f = D$。对于每个$x ∈ D$，按对应法则$f$，总有唯一的值$y$与之相对应，这个值称为函数$f$在$x$处的函数值，记作$f(x)$，即$y = f(x)$。函数值$f(x)$的全体所构成的集合称为函数f的值域，记作$R_f$或$f (D)$，即 $$R_f =f(D)={y|y=f(x),x∈D}$$例如，$f(x) = 3x + 2$是一个函数，定义域是$R$，值域是$R$，自变量和因变量之间存在一一映射。表示函数的记号可以任意选取，除了常用的$f$以 外，还可以用其他的英文字母或希腊字母，如$g$、$F$和$\\phi$。 复合函数给定两个函数$f$和$g$，复合函数定义为:$$( f \\circ g)(x) = f(g(x))$$两个函数$f$和$g$能构成复合函数$f \\circ g$的条件是:函数$g$的值域$R_g$必须是函数$f$ 的定义域$D_f$的子集，即$R_g \\subseteq D_f$。例如，$y = f(u) = 3u + 2$的定义域为$\\mathbb{R}$，而$u = g(x) = x2 − 2$的定义域为$\\mathbb{R}$。由于$g(R) \\subseteq R$，因此$f$和$g$可以构成复合函数 导数设函数$y = f(x)$在点$x_0$的某个邻域内有定义，当自变量$x$在$x_0$处有增量$\\Delta x$， 而且$x_0 + \\Delta x$也在该邻域内时，函数取得增量$\\Delta y = f(x_0 + \\Delta x) − f(x_0)$。如 果$\\Delta y$与$\\Delta x$之比当$\\Delta x → 0$时极限存在，则称函数$y = f(x)$在点$x_0$处可导， 并称这个极限为函数$y = f(x)$在点$x_0$处的导数，记作:$$f’(x_0) = \\lim_{\\Delta x \\rightarrow 0} \\frac{f(x_0 + \\Delta x) − f(x_0)}{\\Delta x}$$ 导函数如果函数$y = f(x)$在开区间内每一点都可导，则称函数$f(x)$在区间内可导。这时函数$y = f(x)$对于区间内的每一个确定的$x$值，都对应着一个确定的导数值，这就构成一个新的函数。我们将该函数称之为原来函数的导函数，记作$y′$、$f′(x)$或$df(x)/dx$，简称导数。 导数的四则运算对于可导函数$f$和$g$，导数的四则运算规则如下: 加法: $(f + g)’ = f’ + g’$ 减法: $(f − g)’ = f’ − g’$ 乘法: $(fg)′ = f’g + fg’$ 除法: $(f/g)’ = (f’g − fg’)/g^2$ 复合函数的导数对于复合函数$(f \\circ g)(x)$，通常使用链式法则计算其导数:$$( f \\circ g)’(x) = f’(g(x))g’(x)$$令$u = g(x)$，则链式法则的另一种表述方式为:$$\\frac{df(g(x))}{dx} = \\frac{df(u)}{du} \\times \\frac{du}{dx}$$ 二阶导数一般而言，函数$y = f(x)$的导数$y′ = f′(x)$仍然是$x$的函数，可以进一步求 导。二阶导数是原函数导数的导数，即对原函数进行二次求导，记作:$$y’’ = (y’)’$$二阶导数的另一种常见的表示方法为$$y’’ = \\frac{d^2y}{dx^2}$$例如，$y = x^2$的一阶导数为$y’ = 2x$，而二阶导数则是一阶导数$y’ = 2x$的导数y′′ = 2。二阶导数反映了一阶导数的变化率。我们通常使用二阶导数来判断函数的凹凸性并计算极值。类似地，在条件允许的情况下，还可以计算函数的三阶导数、四阶导数或高阶导数。 函数的单调性设函数$f(x)$的定义域为$D$，区间$I \\subset D$。如果对于区间$I$上任意两点$x_1$和$x_2$， 当$x_1 &lt; x_2$时，恒有$f(x_1) &lt; f(x_2)$，则称函数$f(x)$在区间$I$上单调递增。反之，如果对于区间$I$ 上任意两点$x_1$ 和$x_2$ ，当$x_1 &lt;x_2$ 时，恒有 $f(x_1) &gt; f(x_2)$，则称函数$f(x)$在区间$I$上单调递减。 凹函数给定函数$f : \\mathbb{R} → \\mathbb{R}$，对于任意两个点$x_1$和$x_2$，如果满足下列条件：$$f\\left(\\frac{x_1+x_2}{2} \\right) \\leq \\frac{f(x_1) + f(x_2)}{2}$$ 凸函数给定函数$f : \\mathbb{R} → \\mathbb{R}$，对于任意两个点$x_1$和$x_2$，如果满足下列条件：$$f\\left(\\frac{x_1+x_2}{2} \\right) \\geq \\frac{f(x_1) + f(x_2)}{2}$$ 函数的极值设函数$f(x)$在点$x = x_0$及其附近有定义。如果对于$x_0$附近的所有点都有 $f(x) &lt; f(x_0)$，则$f(x_0)$是函数$f(x)$的一个极大值，$x_0$是函数$f(x)$的一个极大值点。如果对于$x_0$附近的所有点都有$f(x) &gt; f(x_0)$，则$f(x_0)$是函数$f(x)$的一个 极小值，$x_0$是函数$f (x)$的一个极小值点。 函数的最值函数在整个定义域内可能有许多极大值或极小值，而且某个极大值不一 定大于某个极小值。函数f(x)在整个定义域内的最小函数值$f(x_0)$称为函数 $f(x)$的最小值，$x_0$称为最小值点。类似地，函数$f(x)$在整个定义域内的最大函数值$f (x_0)$称为函数$f (x)$的最大值，$x_0$称为最大值点。如果函数$f(x)$在闭区间$[a, b]$上连续，则$f(x)$在$[a, b]$上必有最大值和最小值。在开区间$(a, b)$上连续的函数$f(x)$不一定有最大值和最小值，如函数$f(x) = 1/x$。函数的最值点必在函数的极值点或者区间的端点处获得。函数的极值可能有多个，但是最值最多只有一个。如果函数$f(x)$在闭区间$[a, b]$上有定义，在开区间$(a, b)$内有导数，则求函数f(x)在闭区间$[a, b]$上的最大值和最小值的步骤如下: 求函数$f(x)$在开区间$(a,b)$的导数$f’(x)$; 求方程$f’(x) = 0$在$(a, b)$内的解; 求在$(a,b)$内使$f’(x)=0$的所有点的函数值和$f(x)$在闭区间端点处的函数值$f (a)$和$f (b)$; 比较上面所求的所有值，其中最大值为函数$f(x)$在闭区间$[a, b]$上的最大值，最小值为函数$f(x)$在闭区间$[a, b]$上的最小值。 例如，可以使用上述方法计算函数$f(x) = x^2 − 2x + 1$在区间$[−2,2]$上的最大值和最小值，得到函数的最小值点是1，最大值点是−2。 不定积分函数$f(x)$的不定积分是一个导数等于$f(x)$的函数$F$，即$F’(x) = f(x)$。相应地，函数$F(x)$称为$f(x)$的原函数。一个函数通常有多个原函数。例如，函数$f(x) = 2x$的原函数可以是$F(x) = x^2 + 1$，也可以是$F(x) = x^2 + 2$。因此，我们通常将原函数写成以下的形式:$$\\int f(x)dx = F(x) + C$$其中，$C$表示任意常数。常见的积分公式如下: 定积分设函数$f(x)$在区间$[a, b]$上连续，将区间$[a, b]$分成$n$个长度相等的子区间，则 函数$f(x)$在区间$[a, b]$上的定积分定义为:$$\\int_a^b f(x)dx = \\lim_{n \\rightarrow +\\infty}f(a + \\frac{i}{n}(b-a))\\frac{b-a}{n}$$其中，$a$称为积分下限，$b$称为积分上限，$[a, b]$称为积分区间，$x$称为积分变 量，$f (x)$称为被积函数。从直观上理解，定积分计算的是包围区域的面积。 多元函数设$D$是一个非空的$n$元有序数组的集合，$f$为某一确定的对应法则，如果对于每一个有限数组$(x_1, x_2, …, x_n) \\in D$， 通过对应法则$f$，都有唯一确定的实数$y$与之对应，则称对应法则$f$为定义在$D$上的多元函数，记为:$$y = f(x_1,x_2,\\cdots,x_n)$$其中$x_1, x_2, …, x_n$称为自变量，$y$称为因变量。 偏导数设函数$z = f(x, y)$在点$(x_0, y_0)$的某一邻域内有定义，当$y$固定在$y_0$而$x$在$x_0$处 有增量$\\Delta x$时，相应地函数值有增量$f(x_0 + \\Delta x, y_0) − f(x_0, y_0)$。如果极限$$\\lim_{\\Delta x \\rightarrow 0}\\frac{f(x_0 + \\Delta x,y_0)-f(x_0,y_0)}{\\Delta x}$$存在，则称此极限为函数$z = f(x, y)$在点$(x_0, y_0)$处对$x$的偏导数，记为:$$\\frac{\\partial z}{\\partial x} | {x = x_0,y=y_0} = \\lim{\\Delta x \\rightarrow 0}\\frac{f(x_0 + \\Delta x,y_0)-f(x_0,y_0)}{\\Delta x}$$另一种形式是$f_x(x_0, y_0)$。同理可以定义函数在点$(x_0, y_0)$处对y的偏导数。如果函数$z = f(x, y)$在区域$D$内任意一点$(x, y)$处对$x$的偏导数都存在，那么这个偏导数是$x$和$y$的函数，成为函数$z = f(x, y)$对自变量$x$的偏导数，记为 $\\partial z/\\partial x$。 多元函数求导设$f(x, y) = x^2 + 3xy + y − 1$，求该函数对$x$和$y$的偏导在点$(4, − 5)$处的取值。求解方法如下。首先计算函数对$x$的偏导。在计算过程中，我们可以将$y$看作常量，然后对$x$求导:$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x}(x^2 + 3xy + y − 1) = 2x + 3y$$因此，$\\partial f/\\partial x$在$(4, − 5)$处的值为$2 \\times 4 + 3 \\times (−5) = − 7$。接下来计算函数对$y$的偏导，将$x$看作常量:$$\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y}(x^2 + 3xy + y − 1) = 3x + 1$$因此，$\\partial f/\\partial y$在$(4, − 5)$处的值为$3 \\times 4 + 1 = 13$ 多元复合函数求导首先来考虑一元函数与多元函数复合的情况。若函数$u = \\phi(x)$和函数 $v = \\psi(x)$都在点$x$可导，函数$z = f(u, v)$在对应点$(u, v)$具有连续偏导数，那 么复合函数$z = f(\\phi(x), \\psi(x))$在点$x$可导，其导数为:$$\\frac{dz}{dx} = \\frac{\\partial z}{\\partial u}\\frac{du}{dx} + \\frac{\\partial z}{\\partial v}\\frac{dv}{dx}$$例如，令$z = f(u, v) = u^2 − v^2$，$u = \\phi(x) = x^2 − 1$，$v = \\psi(x) = 3x + 2$，则 复合函数$z$对$x$的导数可计算为:$$\\begin{aligned}\\frac{dz}{dx} &amp;= \\frac{\\partial z}{\\partial u}\\frac{du}{dx} + \\frac{\\partial z}{\\partial v}\\frac{dv}{dx} \\&amp;= 2u \\times 2x + (-2v) \\times 3 \\&amp;= 4x^3 - 10x -12\\end{aligned}$$然后考虑多元函数与多元函数复合的情况。如果函数$u = \\phi(x, y)$与函数 $v = \\psi(x, y)$具有对$x$和$y$的偏导数，函数$z = f(u, v)$在对应点$(u, v)$具有连续偏导数，那么复合函数$z = f(\\phi(x, y), \\psi(x, y))$在点$(x, y)$的两个偏导数存在:$$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial u}\\frac{\\partial u}{\\partial x} + \\frac{\\partial z}{\\partial v}\\frac{\\partial v}{\\partial x} \\\\frac{\\partial z}{\\partial y} = \\frac{\\partial z}{\\partial u}\\frac{\\partial u}{\\partial y} + \\frac{\\partial z}{\\partial v}\\frac{\\partial v}{\\partial y}$$例如，令$z = f(u, v) = u + v$，$u = \\phi(x, y) = xy，v = \\psi(x, y) = x + y$，则复合函数$z$对$x$和$y$的偏导数分别是:$$\\frac{\\partial z}{\\partial x} = y + 1 \\\\frac{\\partial z}{\\partial y} = x + 1$$ 梯度设二元函数$z = f(x, y)$在平面区域$D$上具有一阶连续偏导数，则对于每一 个点$(x, y)$可以定义一个向量，称为函数$z = f(x, y)$在点$(x, y)$的梯度，记作:$$\\nabla f(x,y) = \\left(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}\\right)$$例如，令$z = f(x, y) = x^2 − y^3$，则$x$和$y$的偏导函数为:$$\\frac{\\partial f}{\\partial x} = 2x,\\frac{\\partial f}{\\partial y} = 3y^2$$因此，函数$f(x, y)$在点$(2,1)$处的梯度是一个二维向量$(4,3)$。多元函数的梯度可以类似地计算。梯队对于计算多元函数的极值而言非常重要，在深度学习的参数优化中被广泛使用。 多元函数极值设函数$z = f(x,y)$在点$(x_0,y_0)$的某个邻域内有定义，对于该邻域内异于 $(x_0, y_0)$的点，如果不等式$$f(x, y) \\lt f(x_0, y_0)$$成立，则称函数$f(x, y)$在点$(x_0, y_0)$处有极大值。如果不等式$$f(x, y) \\gt f(x_0, y_0)$$成立，则称函数$f(x, y)$在点$(x_0, y_0)$处有极小值。例如，函数$z = 3x^2 + 4y^2$在点$(0,0)$处有极小值，因为除了$(0,0)$以外所有的点的函数值均为正，只有在点$(0,0)$处的函数值为0。与之相反，函数$z = − \\sqrt{x^2 + y^2}$在点$(0,0)$处有极大值，因为除了$(0,0)$以外所有的点的函数值均为负，只有在点$(0,0)$处的函数值为0。 多元函数极值条件定理1(必要条件):设函数$z = f(x,y)$在点$(x_0,y_0)$处具有偏导数，且在点 $(x_0, y_0)$处有极值，则函数在该点的偏导数必然为0:$$f_x(x_0, y_0) = 0, f_y(x_0, y_0) = 0$$定理2(充分条件):设函数$z = f(x, y)$在点$(x_0, y_0)$的某邻域内连续且有一阶及二阶连续偏导数，并且$f_x(x_0, y_0) = 0，f_y(x_0, y_0) = 0$，令$$f_{xx}(x_0, y_0) = A, f_{xy}(x_0, y_0) = B, f_{yy}(x_0, y_0) = C$$则$f(x, y)$在$(x_0, y_0)$处是否取得极值的条件如下: 当$AC − B^2 &gt; 0$时有极值，当$A &lt; 0$时有极大值，$A &gt; 0$时有极小值。 当$AC − B^2 &lt; 0$时没有极值。 当$AC − B^2 = 0$时可能有极值，也可能没有极值。 求多元函数极值求二元函数$f(x, y) = x^3 − y^3 + 3x^2 + 3y^2 − 9x$的极值。首先求解一阶导数组成的方程组:$$f_x(x, y) = 3x^2 + 6x − 9 = 0 \\f_y(x, y) = −3y^2 + 6y = 0$$得到四组解:$(1, 0)、(1, 2)、(−3, 0)$和$(−3, 2)$。它们不一定都是极值点，需要进一步考察二阶导数:$$f_{xx}(x, y) = 6x + 6 \\f_{xy}(x, y) = 0 \\f_{yy}(x, y) = −6y + 6$$对四个解分别计算A、B和C，考察定理2的条件。 $(1,0):AC−B^2=12×6&gt;0$且$A=12&gt;0$，因此$(1,0)$是函数$f(x,y)$的一个极小值点，对应的极小值是$f(1,0) = − 5$。 $(1, 2):AC − B^2 = 12 × (−6) &lt; 0$，因此$(1, 2)$不是函数$f(x, y)$的极值点。 $(−3, 0):AC − B^2 = (−12) × 6 &lt; 0$，因此$(−3, 0)$不是函数$f(x, y)$的极值点。 $(−3,2):AC−B^2=(−12)×(−6)&gt;0$且$A=−12&lt;0$，因此$(−3,2)$是函数$f(x, y)$的一个极大值点，对应的极大值是$f(−3, 2) = − 31$。 拉格朗日乘子法求函数$z = f(x, y)$在满足$g(x, y) = 0$下的条件极值，可以转化为函数$$F(x, y, \\lambda) = f(x, y) + \\lambda g(x, y)$$的无约束条件极值问题。例如，给定双曲线$xy = 3$求该曲线上距离原点最近的点。这是一个典型的带约束的求极值问题。原始问题可以转化为:$$F(x, y, λ) = x^2 + y^2 + λ(xy − 3)$$计算函数$F(x, y, λ)$的一阶偏导，得到方程组:$$F_x(x, y, λ) = 2x + λy = 0 \\F_y(x, y, λ) = 2y + λx = 0 \\F_λ(x, y, λ) = xy − 3 = 0$$求解该方程组，可以得到$λ = 2$或$λ = − 2$。当$λ = 2$时，无法求解$x$和$y$，因为势必有$−x^2 = 3$。当$λ = − 2$时，有两组解:$( 3, 3)$和$(− 3, − 3)$。","link":"/blog/2022/09/10/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture2-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%801-%E5%BE%AE%E7%A7%AF%E5%88%86/"},{"title":"自然语言处理学习笔记-lecture2-数学基础2-概率论","text":"概率论随机试验具备以下三个特点的试验称为随机试验: 可以在相同的条件下重复地运行; 每次试验的可能结果可能不止一个，并且能事先明确试验的所有可能结果; 进行一次试验之前不能确定哪一个结果会出现。 以下是一些随机试验的例子: 抛一枚硬币，观察正面$H$、反面$T$出现的情况。 抛一颗骰子，观察出现的点数。 在一批灯泡里任意抽取一只，测试它的寿命。 样本空间对于随机试验，尽管在每次试验之前不能预知试验的结果，但试验的所有可能结果组成的集合是已知的。我们将随机试验$E$的所有可能结果组成的集合称为$E$的样本空间，记为$S$。样本空间中的元素，称为样本点。例如，给定以下随机试验 $E_1$:抛一枚硬币，观察正面$H$、反面$T$出现的情况。 $E_2$:抛一颗骰子，观察出现的点数。 $E_3$:在一批灯泡里任意抽取一只，测试它的寿命。对应的样本空间是: $S_1:{H,T}$ $S_2:{1,2,3,4,5,6}$ $S_3:{t|t \\geq 0}$ 随机事件试验$E$的样本空间$S$的子集称为$E$的随机事件，简称为事件。例如，令“将一枚硬币抛掷两次，观察正面$H$、反面$T$出现的情况”是一个随机试验$E$，则其样本空间总共包含四个元素:$$S = {HH, HT, TT, TH}$$我们可以定义一个事件“第一次出现的是$H$”，即$$A1 = {HH, HT}$$还可以定义另一个事件“两次出现的是同一面”，即$$A2 = {HH, TT}$$显然，$A_1$和$A_2$都是样本空间的子集。 概率设$E$是随机试验，$S$是样本空间。对于$E$的每一个事件$A$赋予一个实数，记为$P(A)$，称为事件$A$的概率。概率必须满足以下条件: 非负性:对于每一个事件$A$，有$P(A) ≥ 0$; 规范性:对于必然发生的事件$S$，有$P(S) = 1$; 可列可加性:设$A_1 、A_2 、…$是两两互不相容的事件，即对于 $A_i \\bigcap A_j =\\emptyset(i \\neq j)$，有$P(A_1 \\bigcup A_2 \\bigcup …)=P(A_1)+P(A_2)+…$。 令$A$和$B$为任意两个事件，$AB$表示两个事件同时发生，以下公式成立:$$P(A \\bigcup B) = P(A) + P(B) − P(AB)$$对于前面抛掷两次硬币的例子，如果$A$表示“第一次是$H$”，$B$表示“两次结果都一样”，那么$AB$表示“两次都是$H$”。 等可能概型等可能概型是指符合以下两个条件的随机试验: 试验的样本空间只能包含有限个元素; 试验中每个基本事件(即每个结果)发生的可能性基本相同。 例如，一个口袋里装有6只球，其中有4只白球和2只红球。从袋中取球两次，每次随机地取一只，假设每只球都有相等概率被抽中。第一次取一球不放回袋中，第二次从剩余的球中再取一球。计算:(1)取到的两只球都是白球的概率，(2)取到的两只球至少有一只是白球的概率。首先计算两只球都是白球的概率:$(4/6) × (3/5) = 2/5$。然后，先计算两只球都是红球的概率:$(2/6) × (1/5) = 1/15$，然后可以得到取到的两只球至少有一只是白球的概率:$1 − (1/15) = 14/15$。 条件概率设A和B是两个事件，且P(A) &gt; 0，称$$P(B|A) = \\frac{P(AB)}{P(A)}$$为在事件$A$发生的条件下事件$B$发生的条件概率。 不难验证，条件概率符合概率定义中的三个条件: 非负性:对于每一个事件$B$，有$P(B|A) ≥ 0$; 规范性:对于必然发生的事件$S$，有$P(S|A) = 1$; 可列可加性:设$B_1 、B_2 、…$是两两互不相容的事件，则有：$$P(\\bigcup_{i=1}^\\infty B_i|A) = \\sum_{i=1}^\\infty P(B_i|A)$$ 例如，一个口袋里装有6只球，其中有4只白球和2只红球。从袋中取球两次，每次随机地取一只，假设每只球都有相等的概率被抽中。第一次取一球不放回袋中，第二次从剩余的球中再取一球。设事件$A$为“第一次取到白球”，事件$B$为“第二次取到白球”，计算条件概率$P(B | A)$。首先计算$P(A)$。由于开始口袋中有6只球，其中有4只白球，因此第一次取到白球的概率$P(A) = 4/6$。然后计算$P(AB)$，即事件“两次都抽到白球”的概率:$$P(AB) = \\frac{4}{6} \\times \\frac{3}{5} = \\frac{2}{5}$$因此，条件概率计算如下:$$P(B|A) = \\frac{P(AB)}{P(A)} = \\frac{2}{5} \\times \\frac{6}{4} = \\frac{3}{5}$$ 全概率公式设$S$为试验$E$的样本空间，$B_1, B_2, …, B_n$为事件$E$的一组事件，如果以下两个条件成立 $B_i \\bigcap B_j = \\emptyset,i \\neq j,i,j = 1,\\cdots, n$ $B_1 \\bigcup B_2 \\bigcup \\cdots \\bigcup B_n = S$ 则称$B_1,B_2,\\cdots,B_n$为样本空间$S$的一个划分。例如，试验$E$“掷一颗骰子观察其点数”样本空间为$S = {1, 2, 3, 4, 5, 6}$，则 $B_1 = {1, 2, 3}，B_2 = {4, 5}和B_3 = {6}$是$S$的一个划分。设$A$是试验$E$的一个事件，$B_1, B_2, …, B_n$是其样本空间的一个划分，则以下全概率公式成立:$$P(A) = \\sum_{i = 1}^nP(A|B_i)P(B_i)$$ 贝叶斯公式设$A$和$B$是随机试验$E$的任意两个事件，以下贝叶斯公式成立:$$P(B|A) = \\frac{P(A|B)P(B)}{P(A)}$$可以进一步与全概率公式结合起来。令$B_1, B_2, …, B_n$是$S$的一个划分，而且$P(B_i) &gt; 0 (i = 1, 2,…, n)$，则有:$$P(B_i|A) = \\frac{P(A|B_i)P(B_i)}{\\sum_{j = 1}^nP(A|B_j)P(B_j)}$$贝叶斯公式在人工智能中非常重要，产生了重要的贝叶斯学派。贝叶斯公式对于揭示信息认知加工过程与规律、实现有效的学习和判断决策都具有十分重要的理论意义和实践价值。 独立性设$A$和$B$是两个随机事件，如果满足等式$P(AB) = P(A)P(B)$则称事件$A$和$B$相互独立。两个事件相互独立的含义是其中一个事件已发生，不影响另一个事件发生的概率。在实际应用中，对于事件的独立性通常是根据事件的实际意义去判断。如果根据实际情况分析，两个事件之间没有关联或者关联很弱，那么就认为它们之间是相互独立的。例如，如果甲、乙两人同一天感冒，甲在中国，乙在美国，双方并未接触，则可以认为两个事件是独立的。如果甲、乙是住在同一个宿舍的舍友，那么就不能认为是相互独立的。 随机变量将一枚硬币抛掷两次，观察出现正面$H$和反面$T$的情况，样本空间是$$S = {HH, HT, TT, TH}$$以$X$表示两次投掷得到正面$H$的总数，则$X$的取值是一个随机变量: $X = 0$:当投掷结果是${TT}$时; $X = 1$:当投掷结果是${HT}$或${TH}$时; $X = 2$:当投掷结果是${HH}$时。 随机变量的取值随试验的结果而定，在试验之前不能预知取什么值，并且其取值有一定的的概率。随机变量的引入，使我们能够描述各种随机现象，并能利用数学方法对随机试验的结果进行深入分析。 离散型随机变量取值是有限个或可列举无限个的随机变量称为离散型随机变量。例如，抛掷一枚硬币，只可能取正面和反面两个取值，因此是离散型随机变量。设离散型随机变量$X$可能的取值为$x_k (k = 1, 2,…) $，$X$取各个可能值的概率，即事件${X = x_k}$的概率，为:$$P(X = x_k) = p_k,k=1,2,\\cdots$$上式称为离散型随机变量$X$的分布律。注意，根据概率的定义，$p_k$满足以下两个条件: $p_k \\geq 0,k=1,2,\\cdots$ $\\sum_{k=1}^\\infty p_k = 1$ 离散型随机变量分布以下两种离散型随机变量经常被使用。第一个是$(0 − 1)$分布。设随机变量$X$只能取0和1两个值，其分布律为$$P(X = k) = p^k(1-p)^{1-k}$$其中，$k$的取值是0或1，$0 &lt; p &lt; 1$。第二个是二项分布。设$n$是一个正整数，$k$是一个不大于$n$的非负整数，即 $0 ≤ k ≤ n$，某个随机事件$A$发生的概率为$p$，则在$n$次试验中事件$A$发生$k$ 次的概率为:$$P(X = k) = \\left( \\begin{matrix} n \\ k \\end{matrix} \\right) p^k(1-p)^{1-k}$$显然，当$n = 1$时，二项分布等价于$(0 − 1)$分布。 随机变量的分布函数对于非离散型随机变量，其取值不能一一列举，因此需要采用新的形式对离散型和非离散型随机变量进行统一描述。设$X$是一个随机变量，$x$是任意实数，函数$$F(x) = P(X \\leq x)$$称为$X$的分布函数。对于任意两个实数$x_1$和$x_2$且满足$x_1 &lt; x_2$，均有:$$\\begin{aligned}P(x_1 \\leq X \\leq x_2) &amp;= P(X \\leq x_2) - P(X \\leq x_1) \\&amp;= F(x_2) - F(x_1)\\end{aligned}$$因此，如果已知$X$的分布函数，我们就知道$X$落在任意区间$(x1, x2]$的概率。从这个意义上说，分布函数完整地描述了随机变量的统计规律性。 分布律与分布函数 x -1 2 3 $p_k$ 0.25 0.50 0.25 给定上表所示的分布律，相应的分布函数定义如下:$$F(x) =\\begin{cases}0.00 &amp; x \\lt -1 \\0.25 &amp; -1 \\leq x \\lt 2 \\0.75 &amp; 2 \\leq x \\lt 3 \\1.00 &amp; x \\geq 3\\end{cases}$$由此可见，分布函数可以全面地描述离散型随机变量。 连续型随机变量如果对于随机变量$X$的分布函数$F(x)$，存在非负函数$f(x)$，使对于任意实数$x$有$$F(x) = \\int_{-\\infty}^x f(t)dt$$则称$X$为连续型随机变量。$f (x)$称为$X$的概率密度函数，具有以下性质: $f(x) \\geq 0$ $\\int_{-\\infty}^{\\infty} f(x)dx = 1$ 对于任意实数$x_1$和$x_2(x_1 ≤ x_2)$，$P(x_1 &lt; X ≤ x_2) = F(x_2) − F(x_1)$; 若$f(x)$在点$x$处连续，则有$F’(x) = f(x)$。 均匀分布若连续型随机变量$X$具有概率密度$$f(x) =\\begin{cases}\\frac{1}{b-a} &amp; a \\lt b \\0 &amp; otherwise\\end{cases}$$则称$X$在区间$(a, b)$上服从均匀分布，记为$X ∼ U(a, b)$。 正态分布若连续型随机变量$X$具有概率密度$$f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$$其中$\\mu$和$\\sigma$实常数且$\\sigma &gt; 0$，则称$X$服从参数为$\\mu$和$\\sigma$的正态分布或高斯分布，记作$X ∼ N(\\mu, \\sigma^2)$。 二维随机变量之前只限于讨论单个随机变量的情况，实际问题中经常出现多个随机变 量的情况。例如，为了研究某一地区某一年龄段儿童的发育情况，需要 统计儿童的身高和体重。设$(X, Y )$是二维随机变量，对于任意实数$x$和$y$，二元函数$$F(x,y) = P(X \\leq x,Y \\leq y)$$称为二维随机变量$(X, Y )$的分布函数，或随机变量$X$和$Y$的联合分布函数。$$P(x_1 \\lt X \\leq x_2，y_1 \\lt Y \\leq y_2) = F(x_2,y_2) - F(x_2,y_1) - F(x_1,y_2) + F(x_1,y_1)$$ 二维离散型随机变量如果二维随机变量$(X, Y )$全部可能的取值是有限对或可列无限多对，则称 $(X,Y)$是离散型的随机变量。设$(X,Y)$所有的可能取值为$(x_i,y_j)，i, j = 1, 2,…$，则$X$和$Y$的联合分布律定义为$$P(X = x,Y = y) = p_{ij}$$联合分布律通常使用表格的方式来表示: $x_1$ $x_2$ $\\cdots$ $x_i$ $\\cdots$ $y_1$ $p_{11}$ $p_{21}$ $\\cdots$ $p_{i1}$ $\\cdots$ $y_2$ $p_{12}$ $p_{22}$ $\\cdots$ $p_{i2}$ $\\cdots$ $\\vdots$ $\\vdots$ $\\vdots$ $\\ddots$ $\\vdots$ $\\cdots$ $y_j$ $p_{1j}$ $p_{2j}$ $\\cdots$ $p_{ij}$ $\\cdots$ $\\vdots$ $\\vdots$ $vdots$ $\\cdots$ $\\vdots$ $\\cdots$ 二维连续型随机变量对于二维随机变量$(X,Y)$的分布函数$F(x,y)$，如果存在非负的函数$f(x,y)$使 得对于任意$x$和$y$都有:$$F(x,y) = \\int_{-\\infty}^{y}\\int_{-\\infty}^{x}f(u,v)dudv$$则称$(X,Y)$是连续型的二维随机变量，函数$f(x,y)$称为二维随机变量$(X,Y)$的概率密度，或成为随机变量X和Y的联合概率密度。例如，给定概率密度:$$f(x,y) =\\begin{cases}2e^{-(2x+y)} &amp; x \\gt 0,y \\gt 0 \\0 &amp; otherwise\\end{cases}$$可计算分布函数为$F(x, y) = (1 − e^{−2x})(1 − e^{−y})$，当$x &gt; 0$且$y &gt; 0$时。 边缘分布律二维随机变量$(X, Y )$作为一个整体，具有分布函数$F(x, y)$，而$X$和$Y$都是随机变量，各自也有分布函数，分别记为$F_X(x)$和$F_Y(y)$，分别称为二维随机变量 $(X, Y )$关于X和关于Y的边缘分布函数，定义如下:$$F_X(x) = P(X \\leq x,Y \\lt \\infty) = F(x,\\infty) \\F_Y(y) = P(X \\lt \\infty,Y \\leq y) = F(\\infty,y)$$随机变量$X$和$Y$的分布律分别定义为:$$P(X = x_i) = \\sum_{j = 1}^\\infty P_{ij} \\P(Y = y_j) = \\sum_{i = 1}^\\infty P_{ij}$$上述式子也称为二维离散型随机变量$(X, Y )$关于$X$和$Y$的边缘分布律。 边缘概率密度对于连续型随机变量$(X,Y)$，设其概率密度为$f(x,y)$，由于$$F_X(x) = F(x,\\infty) = \\int_{-\\infty}^x\\left(\\int_{-\\infty}^\\infty f(x,y)dy\\right)dx$$由此可知$X$是一个连续型随机变量，而且其概率密度函数为:$$f_X(x) = \\int_{-\\infty}^\\infty f(x,y)dy$$同样，$Y$也是一个连续型随机变量，而且其概率密度函数为:$$f_Y(y) = \\int_{-\\infty}^\\infty f(x,y)dx$$$f_X(x)$和$f_Y(y)$分别是关于$X$和关于$Y$的边缘概率密度。 条件分布律下面来考虑事件${Y = y_j}$在已发生的条件下事件${X = x_i}$发生的概率，也就是求事件${X = x_i | Y = y_j}$的概率。设$(X, Y )$是二维离散型随机变量，对于固定的$j$，若$P(Y = y_j) &gt; 0$，则称:$$P(X = x_i|Y = y_j) = \\frac{P(X = x_i,Y = y_i)}{P(Y = y_j)}$$为在$Y = y_j$条件下随机变量$X$的条件分布律。类似地，对于固定的$i$，若$P(X = x_i) &gt; 0$，则称:$$P(Y = y_j|X = x_i) = \\frac{P(X = x_i,Y = y_i)}{P(X = x_i)}$$为在$X = x_i$条件下随机变量$Y$的条件分布律。 条件概率密度设二维随机变量$(X,Y)$的概率密度为$f(x,y)$，$(X,Y)$关于$Y$的边缘概率密度为$f_Y(y)$。若对于固定的y，fY(y) &gt; 0，则在Y = y条件下X的条件概率密度定义为:$$f_{X|Y}(x|y) = \\frac{f(x,y)}{f_Y(y)}$$与之对应地，在$Y = y$条件下$X$的条件分布函数定义为:$$F_{X|Y}(x|y) = \\int_{-\\infty}^x \\frac{f(x,y)}{f_Y(y)}dx$$类似地，我们也可以定义在X = x条件下Y的条件概率密度和条件分布函数。 相互独立的随机变量设$F(x,y)$、$F_X(x)$和$F_Y(y)$分别是二维随机变量$(X,Y)$的分布函数及边缘概率分布，如果对于所有的$x$和$y$有:$$P(X \\leq x,Y \\leq y) = P(X \\leq x)P(Y \\leq y) \\F(x, y) = F_X(x)F_Y(y)$$则称随机变量$X$和$Y$相互独立当$X$和$Y$是离散型随机变量时，$X$和$Y$相互独立的条件是:$$P(X = x_i, Y = y_j) = P(X = x_i)P(Y = y_j)$$当$X$和$Y$是连续型随机变量时，$X$和$Y$相互独立的条件是:$$f(x, y) = f_X(x)f_Y(y)$$ 数学期望设离散型随机变量$X$的分布律为$P(X = x_k) = p_k(k ≥ 1)$，其数学期望定义为:$$\\mathbb{E}(X) = \\sum_{k = 1}^\\infty x_kp_k$$类似地，设连续型变量$X$的概率密度为$f (x)$，其数学期望定义为:$$\\mathbb{E}(X) = \\int_{-\\infty}^\\infty xf(x)dx$$例如，假定$P(X = 0) = 0.3，P(X = 1) = 0.5，P(X = 2) = 0.2$，则$X$的数学期望计算如下:$$\\mathbb{E}(X) = 0 × 0.3 + 1 × 0.5 + 2 × 0.2 = 0.9$$ 随机变量函数的数学期望设$Y$是随机变量$X$的连续函数，即$Y = g(X)$。如果$X$是离散型随机变量，其分布律为$P(X = x_k) = p_k(k ≥ 1)$，则$Y$的数学期望定义为:$$\\mathbb{E}(Y) = \\mathbb{E}(g(X))= \\sum_{k = 1}^\\infty g(x_k)p_k$$如果$X$是连续型随机变量，其概率密度为$f (x)$，则$Y$的数学期望定义为:$$\\mathbb{E}(Y) = \\mathbb{E}(g(X))= \\int_{-\\infty}^\\infty g(x)f(x)dx$$ 数学期望的性质 设$C$为实常数，则有$\\mathbb{E}(C) = C$。 设$X$是一个随机变量，$C$是常数，则有$\\mathbb{E}(CX) = C\\mathbb{E}(X)$。 设$X$和$Y$是两个随机变量，则有$\\mathbb{E}(X + Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)$。这一性质可以推广到任意有限个随机变量之和的情况。 设$X$和$Y$是两个相互独立的随机变量，则有$\\mathbb{E}(XY) = \\mathbb{E}(X)\\mathbb{E}(Y)$。这一 性质可以推广到任意有限个相互独立的随机变量之积的情况。 方差方差用于度量随机变量与其均值的偏离程度。设$X$是一个随机变量，$X$的方差定义为:$$D(X) = Var(X) = \\mathbb{E}((X - \\mathbb{E}(X))^2)$$我们通常将 $\\sqrt{D(X)}$记为$\\sigma(X)$，称为标准差或者均方差。对于离散型随机变量，方差计算公式为$$D(X) = \\sum_{k = 1}^\\infty(x_k - \\mathbb{E}(X))^2p_k$$对于连续型随机变量，方差计算公式为:$$D(X) = \\int_{-\\infty}^{\\infty}(x - \\mathbb{E}(X))^2f(x)dx$$","link":"/blog/2022/09/10/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture2-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%802-%E6%A6%82%E7%8E%87%E8%AE%BA/"},{"title":"自然语言处理学习笔记-lecture2-数学基础3-线性代数","text":"线性代数向量$n$个有次序的数$a_1, a_2, …, a_n$所组成的数组称为$n$维向量。这$n$个数称为该向量的$n$个分量，第$i$个数$a_i$称为第$i$个分量。向量通常表示为: $$a = (a_1,a_2,\\cdots,a_n)$$向量的模也称为向量的大小，定义如下:$$||a|| = \\sqrt{a_1^2+\\cdots+a_n^2}$$给定两个$n$维向量$a = (a_1, …, a_n)$和$b = (b_1, …, b_n)$，主要运算公式如下: 加法:$a+b=(a_1+b_1,…,a_n+b_n)$ 与数的乘法:设$\\lambda$是一个实数，则$\\lambda a = (\\lambda a_1, …, \\lambda a_n)$ 内积:$a\\cdot b=\\sum_{i = 1}^N a_ib_i$ 元素级乘法:$a\\circ b=(a_1b_1,\\cdots,a_nb_n)$ 矩阵由$m × n$个数$a_{ij}(i = 1,…, m; j = 1,…, n)$排成的$m$行$n$列的数表称为$m × n$ 矩阵，记作$$A = \\left(\\begin{matrix}a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\\\end{matrix}\\right)$$这$m × n$个数称为矩阵$A$的元素。行数和列数都等于$n$的矩阵称为$n$阶方阵。只有一行的矩阵称为行向量:$$A = (a_1,a_2,\\cdots,a_n)$$只有一列的矩阵称为列向量:$$A = \\left(\\begin{matrix}a_{1}\\a_{2}\\\\vdots\\a_{m}\\\\end{matrix}\\right)$$ 矩阵的加法设有两个$m × n$矩阵$A = (a_{ij})$和$B = (b_{ij})$，那么矩阵$A$和$B$的和记为:$$A + B = \\left(\\begin{matrix}a_{11} + b_{11} &amp; a_{12} + b_{12} &amp; \\cdots &amp; a_{1n} + b_{1n} \\a_{21} + b_{21} &amp; a_{22} + b_{22} &amp; \\cdots &amp; a_{2n} + b_{2n} \\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\a_{m1} + b_{m1} &amp; a_{m2} + b_{m2} &amp; \\cdots &amp; a_{mn} + b_{mn} \\\\end{matrix}\\right)$$需要注意，只有两个矩阵的行数和列数相同时，才可以进行加法运算。设$A、B$和$C$都是$m × n$矩阵，则矩阵加法满足以下运算律: 交换律:$A+B=B+A$。 结合律:$(A+B)+C=A+(B+C)$ 数与矩阵的乘法实数$\\lambda$与矩阵$A$的乘积记作$\\lambda A$或$A\\lambda$，计算如下:$$\\lambda A = A\\lambda = \\left(\\begin{matrix}\\lambda a_{11} &amp; \\lambda a_{12} &amp; \\cdots &amp; \\lambda a_{1n} \\\\lambda a_{21} &amp; \\lambda a_{22} &amp; \\cdots &amp; \\lambda a_{2n} \\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\lambda a_{m1} &amp; \\lambda a_{m2} &amp; \\cdots &amp; \\lambda a_{mn} \\\\end{matrix}\\right)$$设$A$和$B$为$m × n$矩阵，$\\lambda$和$\\mu$为实数，则数与矩阵的乘法满足以下规律: $(\\lambda \\mu)A = \\lambda(\\mu A)$ $(\\lambda + \\mu)A = \\lambda A + \\mu A$ $\\lambda(A + B) = \\lambda A + \\lambda B$ 矩阵与矩阵相乘设$A$是一个$m × s$矩阵，$B$是一个$s × n$矩阵，那么矩阵$A$与矩阵$B$的乘积是 一个$m × n$矩阵$C = AB$，其中:$$c_{ij} = \\sum_{k = 1}^s a_{ik}b_{kj}$$其中，$a_{ik}$是矩阵$A$的元素，$b_{kj}$是矩阵$B$的元素，$c_{ij}$是矩阵$C$的元素。注意，当且仅当左矩阵的列数等于右矩阵的行数时，两个矩阵才能相乘。 矩阵的转置把矩阵$A$的行换成同序数的列得到一个新矩阵，称为$A$的转置矩阵，记作 $A^T$。矩阵的转置满足下述运算规律: $(A^T)^T = A$ $(A + B)^T = A^T + B^T$ $(\\lambda A)^T = \\lambda A^T$ $(AB)^T = B^TA^T$ 方阵的行列式由$n$阶方阵$A$的元素所构成的行列式，称为方阵$A$的行列式，记作 $| A |$ 或 $detA$。给定一个两行两列的方阵，其行列式计算公式为:$$A =\\left(\\begin{matrix}a_{11} &amp; a_{12} \\a_{21} &amp; a_{22}\\end{matrix}\\right) \\|A| = detA =\\left|\\begin{matrix}a_{11} &amp; a_{12} \\a_{21} &amp; a_{22}\\end{matrix}\\right| = a_{11}a_{22} - a_{12}a_{21}$$ 三行行列式三行三列的方阵的行列式的计算更复杂一些，基本规律是先按照正向 (即从上方往右下方)对角线求和，再按照反向(即从上方往左下方) 对角线求和，最后计算两者之差。$$\\left|\\begin{matrix}a_{11} &amp; a_{12} &amp; a_{13} \\a_{21} &amp; a_{22} &amp; a_{23} \\a_{31} &amp; a_{32} &amp; a_{33}\\end{matrix}\\right| \\\\begin{aligned}= &amp;+ a_{11}a_{22}a_{33} - a_{11}a_{23}a_{32} \\&amp;-a_{12}a_{21}a_{33} + a_{12}a_{23}a_{31} \\&amp;+a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31}\\end{aligned}$$ 逆矩阵对于$n$阶矩阵$A$，如果有一个$n$阶矩阵$B$$$AB = BA = E$$则说矩阵$A$是可逆的，并把矩阵$B$称为$A$的逆矩阵。$A$的逆矩阵通常记为 $A^{−1}$。对于可逆矩阵，有以下性质: 如果矩阵$A$是可逆的，那么$A$的逆矩阵是唯一的。 如果矩阵 $A$可逆，则$|A| \\neq 0$。 如果$AB = E$或$BA = E$，则$B = A^{−1}$。 如果$A$可逆，则$A^{−1}$亦可逆，且$(A^{−1})^{−1} = A$。 如果$A$和$B$为同阶矩阵且均可逆，则$AB$亦可逆，且$(AB)^{−1} = B^{−1}A^{−1}$。 矩阵的初等变换给定一个矩阵，以下三种变换称为初等行变换: 对调第$i$行和第$j$行，记作$r_i \\leftrightarrow r_j$。 第$i$行的所有元素乘以实数$k$，记作$kr_i$。 把第$j$行所有元素的$k$倍加到第$i$行对应的元素上，记作$r_i + kr_j$。 同理，可以定义矩阵的初等列变换: 对调第$i$列和第$j$列，记作$c_i \\leftrightarrow c_j$。 第$i$列的所有元素乘以实数$k$，记作$kc_i$。 把第$j$列所有元素的$k$倍加到第$i$列对应的元素上，记作$c_i + kc_j$。 标准型与矩阵的秩对于$m × n$矩阵$A$，总可以经过初等行变换和列变换将其化简为以下形式:$$F = \\left(\\begin{matrix}E_r &amp; O \\O &amp; O\\end{matrix}\\right)$$其中，$E_r$表示维度为$r$的单元方阵，$O$表示元素全为$0$的矩阵。$F$称为标准形，$r$称为矩阵的秩。 方阵的特征值和特征向量设$A$是$n$阶矩阵，如果存在实数$\\lambda$和$n$维非零列向量$x$使得以下等式成立:$$Ax = \\lambda x$$则称$\\lambda$是矩阵$A$的特征值，非零向量$x$为$A$的对应于特征$\\lambda$的特征向量。","link":"/blog/2022/09/10/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture2-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%803-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"title":"自然语言处理学习笔记-lecture2-数学基础4-信息论","text":"信息论信息量什么是信息量?假设我们听到了两件事，分别如下: 事件A:巴西队获得了2022年FIFA世界杯冠军。 事件B:中国队获得了2022年FIFA世界杯冠军。仅凭直觉来说，显而易见事件B的信息量比事件A的信息量要大(也就是“大新闻”)。究其原因，是因为事件A发生的概率很大，事件B发生的概率很小。所以当越不可能的事件发生了，我们获得的信息量就越大，而越可能发生的事件发生了，我们获得的信息量就越小。因此，信息量应该和事件发生的概率有关。熵如果$X$是一个离散型随机变量，其概率分布为$P(X = x) = p(x)，x ∈ \\mathscr{X}$。 其中，$\\mathscr{X}$表示随机变量所有取值的集合，则该随机变量的熵为:$$H(X) = -\\sum_{x \\in \\mathscr{X}}p(x)log_2p(x)$$我们约定$0 log_2 0 = 0$。熵表示信源每发出一个符号所提供的平均信息量。一个随机变量的熵越 大，其不确定性越大，相应地正确估计其值的可能性就越小。越不确定 的随机变量需要越大的信息量来确定其值。相对熵两个概率分布$p(x)$和$q(x)$的相对熵也称为KL散度(英文全称:Kullback- Leibler divergence)，定义如下:$$KL(p||q) = \\sum_{x \\in \\mathscr{X}}p(x)log\\frac{p(x)}{q(x)}$$约定$0 log(0/q) = 0，p log(p/0) = \\infty$。相对熵通常用于衡量两个概率分布的差距。当两个随机分布相同时，其相对熵为0。当两个随机分布的差别增加时，其相对熵也增加。交叉熵相对熵的公式可以表述为$$\\begin{aligned}KL(p||q)&amp;= \\sum_xp(x)\\log p(x) - \\sum_xp(x) \\log q(x) \\&amp;= -H(p(x)) - \\sum_xp(x) \\log q(x)\\end{aligned}$$等式的前一部分是$p$的熵，而后一部分则是交叉熵:$$H(p,q) = -\\sum_xp(x) \\log q(x)$$在人工智能中，往往需要评估模型分布和真实分布之间的差距，使用$KL$ 散度非常合适。但由于$KL$散度的前一部分跟真实分布相关，在优化过程中不变化，因此一般使用交叉熵作为损失函数并评估模型。","link":"/blog/2022/09/10/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture2-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%804-%E4%BF%A1%E6%81%AF%E8%AE%BA/"},{"title":"高级人工智能-lecture2-搜索问题","text":"本文主要讨论基于目标的Agent中的一种，称为问题求解Agent，要进行问题求解，首先要讨论的是对问题及其解的精确定义，将通过一些实例来说明如何描述一个问题及其解，接着介绍一些求解此类问题的通用的搜索算法，包括无信息搜索算法和有信息的搜索算法。 搜索问题目标是世界的一个状态集合，是目标被满足的那些状态的集合，任务是找到现在和未来如何行动，以使达到一个目标状态。 搜索问题构成： 输入：问题 输出：问题的解 构成（五个部分）问题的输入 Agent的初始状态 描述Agent的可能行动：在给定的状态$s$下，Agent可以采取的动作$ACTIONS(s)$ 转移模型：对每个模型的描述，在状态$s$下采取动作$a$后达到的状态$RESULT(s,a)$，也称为后继状态 状态空间：初始状态、行动和转移模型无疑就定义了问题的状态空间，即从初始状态可以达到的所有的状态的集合。状态空间形成一个有向网络或图，其中结点表示状态，结点之间的弧表示行动 目标测试：确定给定的状态是不是目标状态 路径耗散函数为每条路径赋一个耗散值，即边加权。 输出：解是一个行动序列，将初始状态转换成目标状态，解的质量由路径耗散函数度量，所有解里路径耗散值最小的解即为最优解。 搜索树 根节点对应了初始状态 子节点对应了父节点的后继 节点显示状态，但对应的是到达这些状态的行动 一般的树搜索搜索算法特性： 完备性，当问题有解时，保证能找到一个解 最优性：保证能找到最优解 时间复杂度 空间复杂度 以下面三个值来计算 $b$：分支因子，任何节点的最多后继数 $d$：目标节点所在的最浅的深度，如从根节点到目标状态的步数 $m$：状态空间中任何路径的最大长度 节点的类别： 扩展的节点：扩展之后就是访问过的节点 生成的节点：入队列的节点，等待被扩展 图搜索主要思想：不要扩展一个状态两次执行： 树搜索+扩展过的状态集closed set 将节点扩展成搜索树 扩展节点之前，检查确保它的状态在之前没有被扩展 如果不是新的状态，忽略；如果是新的，加入到closed set 无信息的搜索算法uninformed无信息搜索指的是除了问题定义中提供的状态信息外没有任何附加信息。 广度优先搜索 完备性：可以保证 最优性：只有当每个路线的代价都是一样的时候 时间复杂度：$b+b^2 + b^3 + \\cdots + b^d = O(b^d)$ 空间复杂度：对任何类型的图搜索，每个已扩展的结点都保存在探索集中，空间复杂度总是在时间复杂度的 $b$ 分之一内。特别对于宽度优先图搜索，每个生成的结点都在内存中。那么将有$O(b^{d - 1})$个结点在探索集中，$O(b^d)$个结点在边缘结点集中。所以空间复杂度为 $O(b^d)$ 可以在节点扩展时或者生成时进行目标测试 代价一致搜索uniform cost search深度优先搜索知识找到了行动次数最少得路线，但是没有考虑到代价这一个因素-也就是最小生成树 完备性：可以满足 最优性：可以满足 一致代价搜索由路径代价而不是深度来引导，所以算法复杂度不能简单地用$b$和$d$来表示。引入$C^*$表示最优解的代价，假设每个行动的代价至少为$\\varepsilon$，那么最坏情况下，算法的时间和空间复杂度为$O(b^{1+\\lfloor C^* / \\varepsilon \\rfloor})$要比$O(b^d)$大得多。这是因为一致代价搜索在探索包含代价大的行动之前，经常会先探索代价小的行动步骤所在的很大的搜索树。当所有的单步耗散都相等的时候，$O(b^{1+\\lfloor C^* / \\varepsilon \\rfloor})$就是$O(b^d)$。此时，一致代价搜索与宽度优先搜索类似，除了算法终止条件，宽度优先搜索在找到解时终止，而一致代价搜索则会检査目标深度的所有结点看谁的代价最小;这样，在这种情况下一致代价搜索在深度无意义地做了更多的工作。 在节点被扩展时进行目标测试 深度优先搜索 完备性：深度可能是无限的，需要避免环的出现 最优性：没有，总是寻找最左边的路线，没有考虑深度和代价 时间复杂度：深度优先搜索的时间复杂度受限于状态空间的规模(当然，也可能是无限的 )另一方面，深度优先的树搜索，可能在搜索树上生成所有 $O(b^m)$个结点，其中 $m$ 指的是任一结点的最大深度;这可能比状态空间大很多。要注意的是 $m$ 可能比 $d$ (最浅解的深度)大很多， 并且如果树是无界限的，$m$ 可能是无限的。 空间复杂度：深度优先搜索只需要存储一条从根结点到叶结点的路径，以及该路径上每个结点的所有未被扩展的兄弟结点即可。一旦一 个结点被扩展，当它的所有后代都被探索过后该结点就从内存中删除。考虑状态空间分支因子为$b$最大深度为$m$ 深度优先搜索只需要存储$O(bm)$个结点. 深度受限搜索在无限状态空间深度优先搜索会令人尴尬地失败，而这个问题可以通过对深度优先搜索设置界限 $l$来避免。就是说，深度为$l$的结点被当作没有后继对待。这种方法称为深度受限搜索 (depth-limited search)。 完备性：如果我们选 择了$l \\lt d$，即是说，最浅的目标结点的深度超过了深度限制，那么这种搜索算法是不完备的。 最优性：如果选择的$l \\gt d$，深度受限搜索同样也不是最优的。 时间复杂度：$O(b^l)$ 空间复杂度：$O(bl)$ 深度优先搜索可以看作是特殊的深度受限搜索，其深度$l = \\infty$ 迭代深入搜索iterative deepening结合DFS的空间优势和BFS的时间优势 首先限制深度为1，进行深度优先搜索 然后限制深度为2，进行深度优先搜索 然后限制深度为3，进行深度优先搜索 …… 当深度界限达到$d$，即最浅的目标结点所在深度时，就能找到目标结点。 浪费冗余：通常绝大多数的节点都在底层，所以上层节点生成多次影响不是很大 完备性：和宽度优先搜索一样，当分支因子有限时是该搜索算法是完备的 最优性：当路径代价是结点深度的非递减函数时该算法是最优的。 时间复杂度：$O(b^d)$ 空间复杂度：$O(bd)$ 双向搜索同时运行两个搜索，一个从初始状态向前搜索同时另一个从目标状态向后搜索，希望它们在中间某点相遇，此时搜索终止。理由是$b^{d/2} + b^{d/2}$要比$b^d$小很多。 时间复杂度：$O(b^{\\frac{d}{2}})$ 空间复杂度：$O(b^{\\frac{d}{2}})$ 一般来讲，当搜索空间较大并且不知道解所在深度时，迭代加深的深度优先搜索是首选的无信息搜索方法。 搜索算法总结b表示树的宽度，是分支因子，m表示最大深度，d表示最浅目标节点的深度 有信息(启发式)的搜索策略informed search使用问题本身的定义之外的特定知识，比无信息的搜索策略更有效地进行问题求解。结点是基于评价函数$f(n)$值被选择扩展的。评估函数被看作是代价估计，因此评估值最低的结点被选择首先进行扩展。最佳优先图搜索的实现与一致代价搜索类似, 不过最佳优先是根据$f$值而不是$g$值对优先级队列排队。对$f$的选择决定了搜索策略，大多数的最佳优先搜索算法$f$由启发函数(heuristic function) 构成:$$h(n) = 结点n到目标结点的最小代价路径的代价估计值$$ 估计一个状态到目标距离的函数 问题给予算法的额外信息，为特定搜索问题而设计 贪婪最佳优先搜索 策略：扩展最接近目标状态的节点，理由是这样可能可以很快找到解。因此，它只用启发式信息，即$f(n) = h(n)$，例如在罗马尼亚问题中是到目的地的距离。 通常情况下可以很快到达目标 最坏情况下类似于深度优先搜索 完备性：和深度优先搜索类似，不完备 最优性：不具备 时间复杂度：最坏情况下$O(b^m)$ 空间复杂度：最坏情况下$O(b^m)$ 复杂度决定于启发式函数的质量$A^*$搜索 结合使用代价一致搜索(代价$g(n)$)和贪心搜索(代价$h(n)$)，$g(n)$是从开始结点到结点$n$的路径代价，$h(n)$是从结点 $n$ 到目标结点的最小代价路径的估计值$$f(n) = d(n) + h(n)$$ 算法和一致代价类似，只是使用的代价变为$f$ 保证最优性的条件：可采纳性和一致性 一致的启发式都是可采纳的 如果$h(n)$是可采纳的，那么$A^*$的树搜索版本是最优的 如果$h(n)$是一致的，你们图搜索的$A^*$算法是最优的算法的结束条件：当目标入列时不停止，只有当目标出列时才停止可采纳启发式保障最优性的第一个条件是$h(n)$是一个可采纳启发式。可采纳启发式是指它从不会过高估计到达目标的代价。因为$g(n)$是当前路径到达结点$n$的实际代价，而$f(n) = g(n) + h(n)$，我们可以得到直接结论: $f(n)$永远不会超过经过结点$n$的解的实际代价。 启发函数h是可采纳的，那么：$$0 \\leq h(n) \\leq h^*(n)$$其中$h^*(n)$是最接近目标的真实耗散，例如到目标节点的直线距离肯定是最短的，那么当$h(n)$取直线距离的时候就是可采纳的 想出可采纳的启发函数是$A^*$算法实际使用中的重点 一致性启发式对于每个节点$n$和通过任一行动$a$生成的$n$的每个后继结点$n’$，从结点$n$到达目标的估计代价不大于从$n$到$n’$的单步代价与从$n’$到达目标的估计代价之和$$h(n) \\leq c(n,a,n’) + h(n’)$$ $A^*$算法的最优性 首先证明如果$h(n)$是一致的，那么沿着任何路径的$f(n)$值是非递减的，假设有$n’$是$n$的后继结点，那么有：$$f(n’) = g(n’) + h(h’) = g(n) + c(n,a,n’) + h(n’) \\geq g(n) + h(n) = f(n)$$ 下一步则需要证明:若$A^*$选择扩展结点$n$时，就已经找到到达结点$n$的最优路径。否则，在到达结点$n$的最优路径上就会存在另一边缘结点$n’$，因为$f$在任何路径上都是非递减的，$n’$的$f$代价比$n$小，会先被选择。 所以算法以$f(n)$的非递减序扩展接点，所以第一个被选择扩展的目标结点一定是最优解，之后扩展的目标结点代价都不会低于它 算法是效率最优的，也就是说没有其他的最优算法可以保证扩展的结点数少于$A^*$算法 $A^*$算法的完备性假设$C^*$是最优解路径的代价值，那么可以得到： $A^*$算法扩展所有$f(n) \\lt C^*$的结点； 算法在扩展目标结点之前可能会扩展一些正好处于等值线$f(n) = C^*$上的结点，这要这样的结点的数目是有穷的，算法就是完备的 $A^*$算法的目标损耗假设绝对误差定义为$\\Delta \\equiv h^* - h$，$h^*$表示从根结点到目标结点的实际代价，相对误差定义为$\\varepsilon = (h^* - h) / h^*$，当模型是一个只有一个目标状态的状态空间时，时间复杂度为$O(b^\\Delta)$，考虑每步骤代价均为常量，我们可以把这记为$O(b^{\\varepsilon d})$ 存储受限的启发式搜索$A^*$算法减少内存需求的简单办法就是将迭代加深的思想用在启发式搜索上，即迭代加深$A^*(IDA^*)$算法 判断启发式函数的好坏 有效分支因子$b^*$，对于某一问题，如果$A^*$算法生成的总结点数为$N$，解的深度为$d$，那么$b^*$就是深度为$d$的标准搜索树为了能够包括$N + 1$个结点所必需的分支因子。即：$$N + 1 = 1 + b^* + (b^*)^2 + \\cdots + (b^*)^d$$有效分支因子越小，启发式函数越好，设计良好的启发式会使$b^*$的值接近于1 对于两个启发式函数，如果对于任一结点$n$，有$h_2(n) \\geq h_1(n)$，那么$h_2$启发式函数更好， 因为使用$h_2$的$A^*$算法永远不会比使用$h_1$的$A^*$算法扩展更多的结点 从松弛问题出发设计可采纳的启发式 减少了行动限制的问题称为松弛问题。松弛问题的状态空间图是原有状态空间的超图，原因是减少限制导致图中边的增加。 由于松弛问题增加了状态空间的边，原有问题中的任一最优解同样是松弛问题的最优解;但是松弛问题可能存在更好的解，理由是增加的边可能导致捷径。所以，一个松弛问题的最优解代价是原问题的可采纳的启发式。","link":"/blog/2022/09/11/%E9%AB%98%E7%BA%A7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-lecture2-%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98/"},{"title":"高级人工智能-lecture2-局部搜索问题","text":"适用于那些关注解状态而不是路径代价的问题，如果到目标的路径是无关紧要的，我们可能考虑不同的算法，这类算法不关心路径。 局部搜索算法从单个当前结点(而不是多条路径)出发，通常只移动到它的邻近状态。一般情况下不保留搜索路径。局部搜索算法家族包括由统计物理学带来的模拟退火法(simulated annealing) 和进化生物学带来的遗传算法 (genetic algorithms)。除了找到目标，局部搜索算法对于解决纯粹的最优化问题十分有用，其目标是根据目标函数找到最佳状态。 爬山法(贪婪局部搜索)是简单的循环过程，不断向值增加的方向持续移动—— 即，登高。算法在到达一个“峰顶”时终止，邻接状态中没有比它值更高的。 爬山法经常会陷入困境： 局部极大值:局部极大值是一个比它的每个邻接结点都高的峰顶，但是比全局最大值要小。爬山法算法到达局部极大值附近就会被拉向峰顶，然后就卡在局部极大值处无处可走。 山脊:下图显示了山脊的情况。山脊造成一系列的局部极大值，贪婪算法很难处理这种情况。图中的状态(黑色圆点)叠加在从左到右上升的山脊上，创造了一个不直接相连的局部极大值序列。从每个局部极大点出发，可能的行动都是指向下山方向的 高原:高原是在状态空间地形图上的一块平原区域。它可能是一块平的局部极大值，不存在上山的出口，或者是山肩，从山肩还有可能取得进展。爬山法在高原可能会迷路。解决这个问题的一个方法是在最佳后继值和当前状态值相等的时候侧向移动，此时可能陷入循环，所以可以同时限制侧向移动的次数。 爬山法的变形 随机爬山法在上山移动中随机地选择下一步，随机地生成后继结点直到生成一个优于当前结点的后继;被选中的概率可能随着上山移动的陡峭程度不同而不同。这种算法通常比最陡上升算法的收敛速度慢不少，但是在某些状态空间地形图上它能找到更好的解。 随机重启爬山法(random restart hill climbing) ：原爬山法是不完备的，该算法通过随机生成初始状态来导引爬山法搜索，直到找到目标。这个算法完备的概率接近于 1, 理由是它最终会生成一个目标状态作为初始状态。 模拟退火搜索模拟退火算法的内层循环与爬山法类似。只是它没有选择最佳移动，选择的是随机移动。如果该移动使情况改善，该移动则被接受。否则，算法以某个小于 1 的概率接受该移动。如果移动导致状态“变坏”，概率则成指数级下降一评估值$\\Delta E$变坏。这个概率也随“温度’’$T$降低而下降:开始$T$高的时候可能允许“坏的”移动，$T$越低则越不可能发生。如果调度让$T$下降得足够慢，算法找到全局最优解的概率逼近于 1。 局部束搜索 局部束搜索(local beam search) 算法记录$k$个状态而不是只记录一个。它从$k$个随机生成的状态开始。每一步全部$k$个状态的所有后继状态全部被生成。如果其中有一个是目标状态，则算法停止。否则， 它从整个后继列表中选择$k$个最佳的后继，重复这个过程。 如果是最简单形式的局部束搜索，那么由于这$k$个状态缺乏多样性，它们很快会聚集到状态空间中的一小块区域内，使得搜索代价比高昂的爬山法版本还要多。随机束搜索(stochastic beam search) 为解决此问题的一种变形，它与随机爬山法相类似。随机束搜索并不是从候选后继集合中选择最好的$k$个后继状态，而是随机选择$k$个后继状态，其中选择给定后继状态的概率是状态值的递增函数。 遗传算法遗传算法(genetic algorithm, 或 GA) 是随机束搜索的一个变形，它通过把两个父状态结合来生成后继，而不是通过修改单一状态进行。像束搜索一样，遗传算法也是从$k$个随机生成的状态开始，我们称之为种群。每个状态，或称个体，用一个有限长度的字符串表示，通常是 0、1 串。例如，八皇后问题的状态必须指明 8 个皇后的位置，每列有 8 个方格，所以需要 $8 \\times \\log_28 = 24$比特来表示。","link":"/blog/2022/09/11/%E9%AB%98%E7%BA%A7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-lecture2-%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98/"},{"title":"卜算法学习笔记-02-分而治之算法01","text":"给定一个问题，如何求解？首先查看最简单的实例能否求解，假如可以求解的话，下一步就是思考能否将大的实例分解成小的实例，以及能否将小的实例组合成为大的实例，如果都可以的话就称实例能归约，这个问题具有递归结构，可以设计递归算法进行求解 排序问题：对数组的归约排序问题： 输入：一个包含 $n$ 个元素的数组 $A[0..n − 1]$，其中每个元素都是整数; 调整元素顺序后的数组 $A$，使得对任意的两个下标 $0 ≤ i &lt; j ≤ n − 1$，有 $A[i] ≤ A[j]$。 依据元素下标拆分数组：插入排序与归并排序第一种拆分方案及插入排序算法 算法分析我们只需执行一个简单操作即可将数组 $A[0..n − 1]$ 分解成两部分:前 $n − 1$ 个元素 $A[0..n − 2]$，以及单独一个元素 $A[n − 1]$。前 $n − 1$ 个元素组成一个小 的数组，是原给定实例的子实例。在将原给定实例拆分成子实例之后，我们假定子实例已被求解，对数组来说，所谓子实例的解就是已经排好序的小的数组 $A[0..n − 2]$。要想完成对整个数组 $A[0..n−1]$ 的排序，我们只需将最后一个元素 $A[n−1]$和小数组 $A[0..n−2]$ 中的元素逐个比较，然后将 $A[n − 1]$ 插入到合适的位置即可。连续应用递归调用，最终会到达基始情形:当 $n = 1$ 时，数组 $A$ 只有一个元素，此时无需排序，直接返回即可。 时间复杂度：$$T(n) =\\begin{cases}1 &amp; n = 1 \\T(n - 1) + O(n) &amp; otherwise\\end{cases}$$将上述递归式展开：$$\\begin{aligned}T(n) &amp;\\leq T(n - 1) + cn \\&amp;\\leq T(n - 2) + c(n - 1) + cn \\&amp;\\cdots \\&amp;\\leq c + \\cdots + c(n - 1) + cn \\&amp;= O(n^2)\\end{aligned}$$算法低效的原因是:在运行过程中，问题规模是呈线性下降的，即每次递归操作都是将规模为 $n$ 的问题分解成一个规模为 $n − 1$ 的子问题。第二种拆分方案及归并排序算法 算法分析：将大的数组 $A[0..n − 1]$按下标拆分成规模相同的两半，即 $A[0..⌈ \\frac{n}{2} ⌉ − 1]$和 $A[⌈ \\frac{n}{2} ⌉..n − 1]$;每一半依然是数组，形式相同，但是规模变小，因此是原给定实例的子实例。通过迭代执行分解操作，即可使得问题规模呈指数形式下降。在使用递归调用将小的数组排好序之后，我们只需依据这两个已排好序的小的数组，“归并”(Merge)出整个数组。这里的归并包括两层意思:合并、以及排序。归并过程：循环不变量，是指关于程序行为的一个断言;这个断言在循环起始时成立，并且每执行一轮循环时都保持成立，因此可以推论出当循环结束时，断言必定成立。 时间复杂度分析$$T(n) =\\begin{cases}1 &amp; n = 1 \\2T(\\frac{n}{2}) + O(n) &amp; otherwise\\end{cases} = O(n \\log n)$$ 分而治之算法时间复杂度分析及Master定理在分而治之算法中，一种常见的情况是将一个规模为 $n$ 的实例归约成 $a$ 个子实例，每个子实例规模都相同(设为 $\\frac{n}{b}$ )。假如“组合”子实例解的时间开销是 $O(n^d)$，则我们可将时间复杂度 $T(n)$ 的递归关系及基始情形表示如下:$$T(n) =\\begin{cases}1 &amp; n = 1 \\aT(\\frac{n}{b}) + O(n^d) &amp; otherwise\\end{cases}$$对于子问题比较规整的情况，即每个子问题的规模都相同，T(n) 上界的显式表达式已被总结成 Master 定理:$$\\begin{aligned}T(n)&amp;= aT(\\frac{n}{b}) + O(n^d) \\&amp;\\leq aT(\\frac{n}{b}) + cn^d \\&amp;\\leq a(aT(\\frac{n}{b^2}) + c(\\frac{n}{b})^d) + cn^d \\&amp;\\leq \\cdots \\&amp;\\leq cn^d(1 + \\frac{a}{b^d} + (\\frac{a}{b^d})^2 + \\cdots + (\\frac{a}{b^d})^{\\log_b n}) + a^{\\log_b n} \\&amp;=\\begin{cases}O(n^{\\log_b a}) &amp; d &lt; \\log_b a \\O(n^{\\log_b a}\\log n) &amp; d = \\log_b a \\O(n^{d}) &amp; d &gt; \\log_b a \\\\end{cases}\\end{aligned}$$依据元素的值拆分数组-快速排序算法分析依据元素的数值将大数组拆分成小数组，即选定一个元素作“中心元”(Pivot)，比中心元数值小的元素组成一个小数组，比中心元数值大的那些元素组成另一个小数组。时间复杂度称排序后的数组 $A$ 为 $\\tilde{A}$，因此数组 &amp;A$ 的最小元是 $\\tilde{A}[0]$， 最大元是 $\\tilde{A}[n − 1]$，中位数是 $\\tilde{A}[⌈ \\frac{n}{2} ⌉]$。我们在选择中心元时可能面临如下两种情况:(1) $\\tilde{A}[n − 1]$/ $\\tilde{A}[0]$: 这样只会生成一个子实例，规模减少了 1，呈线性降低。如果在每一次迭代都是如此选择的话， 运行过程就与 InsertionSort 算法相同，时间复杂度为:$$T(n) = T(n - 1) + O(n) = O(n^2)$$(2) $\\tilde{A}[⌈ \\frac{n}{2} ⌉]$: 这样会生成两个子实例，每个子实例的规模都是原来的一半，呈指数下降。如果在每一次迭代都是如此选择的话，运行过程就与 MergeSort 算法相同，时间复杂度为:$$T(n) = 2T(\\frac{n}{2}) + O(n) = O(n\\log n)$$证明运行时间的期望值依然是 $O(n \\log n)$Modified-QuickSort 算法只做了一点修改:随机选择一个元素做中心元之后，先检验一下这个中心元是否足够好;如果足够好，则继续执行后续的比较和排序，否则重新选择一个元素做中心元。所谓的中心元足够好，是指它位于A ̃的中间区域，即$\\tilde{A}[⌈ \\frac{n}{4} ⌉] \\cdots \\tilde{A}[⌈ \\frac{3n}{4} ⌉]$，修改后的算法时间复杂度为：$$\\begin{aligned}T(n)&amp;\\leq T(\\frac{n}{4}) + T(\\frac{3n}{4}) + 2n \\&amp;\\leq (T(\\frac{n}{16}) + T(\\frac{3n}{16}) + 2\\frac{n}{4}) + (T(\\frac{3n}{16}) + T(\\frac{9n}{16}) + 2\\frac{3n}{4}) + 2n \\&amp;= (T(\\frac{n}{16}) + T(\\frac{3n}{16}) ) + (T(\\frac{3n}{16}) + T(\\frac{9n}{16})) + 2n + 2n \\&amp;\\leq \\cdots \\&amp;= O(n \\log_{\\frac{4}{3}} n)\\end{aligned}$$接下来我们分析 QuickSort 算法的时间复杂度。在做具体的分析之前，我们先陈述关于运行时间的 3 点事实:(1) 运行时间由比较次数界定:我们的目标就是计算期望运行时间 $\\mathbb{E}[X]$。(2) 任意两个元素 $\\tilde{A}[i]$ 和 $\\tilde{A}[j]$ 最多只会比较一次(3) 两个元素$\\tilde{A}[i]$ 和 $\\tilde{A}[j]$发生比较的概率是$\\frac{2}{j - i + 1}$$$\\begin{aligned}Pr[\\tilde{A}[i]与\\tilde{A}[j]进行比较]&amp;= \\frac{1}{n} + \\frac{1}{n} + \\frac{n - (j - i + 1)}{n} \\times \\frac{2}{j - i + 1} \\&amp;= (\\frac{j - i + 1}{n} + \\frac{n - (j - i + 1)}{n}) \\times \\frac{2}{j - i + 1} \\&amp;= \\frac{2}{j - i + 1}\\end{aligned}$$由此计算时间复杂度为：$$\\begin{aligned}\\mathbb{E}[X]&amp;= \\mathbb{E}[\\sum_{i=0}^{n-1}\\sum_{j = i + 1}^{n - 1}X_{ij}] \\&amp;= \\sum_{i=0}^{n-1}\\sum_{j = i + 1}^{n - 1}\\mathbb{E}[X_{ij}] \\&amp;= \\sum_{i=0}^{n-1}\\sum_{j = i + 1}^{n - 1}\\frac{2}{j - i + 1} \\&amp;= \\sum_{i=0}^{n-1}\\sum_{k = 1}^{n - i - 1}\\frac{2}{k + 1} \\&amp;\\leq \\sum_{i=0}^{n-1}\\sum_{k = 1}^{n - 1}\\frac{2}{k + 1} \\= O(n \\log n)\\end{aligned}$$空间复杂度需要创建两个辅助数组 $S_−$ 和 $S_+$，这样一来，除了数组本身之外还要额外占用 $n$ 个内存单元，导致当 $n$ 比较大时，内存需求有时难以满足。所以有了原位排序算法，为避免开辟辅助数组 $S_−$ 和 $S_+$，Lomuto 算法直接将数组 A 的左半部分当做 $S_−$，存放比中心元小的元素;把数组 A 的右半部分当做 $S_+$，存放比中心元大的元素","link":"/blog/2022/09/12/%E5%8D%9C%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-02-%E5%88%86%E8%80%8C%E6%B2%BB%E4%B9%8B%E7%AE%97%E6%B3%9501/"},{"title":"统计学习方法学习笔记-02-感知机","text":"首先介绍感知机模型，然后叙述感知机的学习策略，特别是损失函数，最后介绍感知机学习算法，包括原始模式和对偶模式，并证明算法的收敛性 感知机模型输入空间(特征空间)：$\\mathcal{X} \\subseteq R^n$，假设$x \\in \\mathcal{X}$输出空间：$\\mathcal{Y} = {+1,-1}$，假设$y \\in \\mathcal{Y}$由输入空间到输出空间的如下函数称为感知机：$$f(x) = sign(\\omega \\cdot x + b)$$其中$\\omega \\in R^n$叫做权值，$b \\in R$叫做偏置，$\\omega \\cdot x$表示内积，sign表示符号函数：$$sign(x) =\\begin{cases}+1 &amp; x\\geq 0 \\-1 &amp; x \\lt 0\\end{cases}$$ 感知机是一种线性分类模型，属于判别模型，对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面。 感知机模型的假设空间时定义在特征空间中的所有线性分类函数或线性分类器，即函数集合$f|f(x) = \\omega \\cdot x + b$ 感知机学习策略目标：确定模型参数$\\omega,b$损失函数：误分类点到超平面$S$的总距离$$\\frac{1}{||\\omega||}|\\omega \\cdot x_i + b|$$$||\\omega||$是指$\\omega$的$L_2$范数又因为误分类点有如下定义：$$-y_i(\\omega \\cdot x_i + b) \\gt 0$$所以误分类点到超平面的距离为：$$-\\frac{1}{||\\omega||}y_i(\\omega \\cdot x_i + b)$$假设误分类点集合$M$，那么所有误分类点到超平面的总距离为：$$-\\frac{1}{||\\omega||}\\sum_{x_i \\in M}y_i(\\omega \\cdot x_i + b)$$得到损失函数：$$L(\\omega,b) = -\\sum_{x_i \\in M}y_i(\\omega \\cdot x_i + b)$$ 感知机学习算法感知机学习问题转化为求解损失函数式的最优化问题，最优化的方法是随机梯度下降法 感知机学习算法的原始形式输入：训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in \\mathcal{X} = R^n,y_i \\in \\mathcal{Y} = {-1,+1},i=1,2,\\cdots,N$；学习率$\\eta(0 \\lt \\eta \\leq 1)$输出：$\\omega,b$；感知机模型$f(x) = sign(\\omega \\cdot x + b)$ 选取初值$\\omega_0,b_0$； 在训练集中选取数据$(x_i,y_i)$； 如果$y_i(\\omega \\cdot x_i + b) \\leq 0$:$$\\omega \\leftarrow \\omega + \\eta y_ix_i \\b \\leftarrow b + \\eta y_i$$ 转至第二步，直到没有误分类点 算法的收敛性收敛性证明：误分类的次数$k$是有上界的，经过有限次搜索可以找到将训练数据完全正确分开的超平面假设$\\hat{\\omega} = (\\omega^T,b)^T,\\hat{x} = (x^T,1)^T,\\hat{\\omega} \\in R^{n + 1},\\hat{x} \\in R^{n + 1}$，则$\\hat{\\omega} \\cdot \\hat{x} = \\omega \\cdot x + b$定理： 存在满足条件$||\\hat{\\omega}{opt}|| = 1$的超平面$\\hat{\\omega} {opt}\\cdot \\hat{x} = \\omega{opt} \\cdot x + b{opt} = 0$将训练数据集完全正确分开；且存在$\\gamma \\gt 0$对所有$i = 1,2,\\cdots,N$$$y_i(\\hat{\\omega} {opt}\\cdot \\hat{x_i}) =y_i(\\omega{opt} \\cdot x_i + b_{opt}) \\geq \\gamma$$ 令$R = \\mathop{max}\\limits_{1 \\leq i \\leq N}||\\hat{x}_i||$，则感知机算法在训练数据集上的误分类次数$k$满足不等式：$$k \\leq \\left( \\frac{R}{\\gamma} \\right)^2$$ 证明： 由于训练数据集是线性可分的，所以存在超平面可将训练数据集完全正确分开，取此超平面为$\\hat{\\omega} {opt}\\cdot \\hat{x} = \\omega{opt} \\cdot x + b_{opt} = 0$，使$||\\hat{\\omega}{opt}|| = 1$，由于对有限的$i = 1,2,\\cdots,N$，均有$$y_i(\\hat{\\omega} {opt}\\cdot \\hat{x_i}) =y_i(\\omega{opt} \\cdot x_i + b{opt}) \\gt 0$$所以存在$$\\gamma = \\mathop{min}\\limits_{i}{y_i(\\omega_{opt} \\cdot x_i + b_{opt})}$$使$$y_i(\\hat{\\omega} {opt}\\cdot \\hat{x_i}) =y_i(\\omega{opt} \\cdot x_i + b_{opt}) \\geq \\gamma$$ 感知机算法从$\\hat{\\omega}0 = 0$开始，如果实例被误分类，则更新权重，令$\\hat{\\omega}{k-1}$是第$k$个误分类实例之前的扩充权重向量，即：$$\\hat{\\omega}{k-1} = (\\omega{k-1}^T,b_{k-1})^T$$则第$k$个误分类实例的条件是：$$y_i(\\hat{\\omega}{k-1} \\cdot \\hat{x}i) = y_i(\\omega{k-1} \\cdot x_i + b{k - 1}) \\leq 0$$若$(x_i,y_i)$是被$\\hat{\\omega}{k-1} = (\\omega{k-1}^T,b_{k-1})^T$误分类的数据，则$\\omega$和$b$的更新是：$$\\omega_k \\leftarrow \\omega_{k-1} + \\eta y_ix_i \\b_k \\leftarrow b_{k-1} + \\eta y_i$$即：$$\\hat{\\omega}k = \\hat{\\omega}{k-1} + \\eta y_i \\hat{x}i$$递推1：$$\\begin{aligned}\\hat{\\omega}k \\cdot \\hat{\\omega}{opt} &amp;= \\hat{\\omega}{k-1} \\cdot \\hat{\\omega}{opt} + \\eta y_i \\hat{\\omega}{opt} \\cdot \\hat{x}i \\&amp;\\geq \\hat{\\omega}{k-1} \\cdot \\hat{\\omega}{opt} + \\eta\\gamma \\&amp;\\geq \\hat{\\omega}{k-2} \\cdot \\hat{\\omega}{opt} + 2\\eta\\gamma \\&amp;\\geq \\cdots \\&amp;\\geq k\\eta\\gamma\\end{aligned}$$递推2：$$\\begin{aligned}||\\hat{\\omega}k||^2 &amp;= ||\\hat{\\omega}{k-1}||^2 + 2\\eta y_i\\hat{\\omega}{k-1} \\cdot \\hat{x}i + \\eta^2||\\hat{x}i||^2 \\&amp;\\leq ||\\hat{\\omega}{k-1}||^2 + \\eta^2||\\hat{x}i||^2 \\&amp;\\leq ||\\hat{\\omega}{k-1}||^2 + \\eta^2R^2 \\&amp;\\leq ||\\hat{\\omega}{k-2}||^2 + 2\\eta^2R^2 \\&amp;\\leq \\cdots \\&amp;\\leq k\\eta^2 R^2\\end{aligned}$$结合两个递推式：$$k\\eta\\gamma \\leq \\hat{\\omega}k \\cdot \\hat{\\omega}{opt} \\leq ||\\hat{\\omega}k||\\ ||\\hat{\\omega}{opt}|| \\leq \\sqrt{k}\\eta R \\k^2\\gamma^2 \\leq kR^2 \\k \\leq \\left( \\frac{R}{\\gamma} \\right)^2$$感知机算法的对偶形式不失一般性，假设$\\omega_0,b_0$均为0，已知下式$$\\omega \\leftarrow \\omega + \\eta y_ix_i \\b \\leftarrow b + \\eta y_i$$逐步修改$\\omega,b$，设修改$n$次，则$\\omega,b$关于$(x_i,y_i)$的增量分别是$\\alpha_iy_ix_i,\\alpha_iy_i$，这里$\\alpha_i = n_i\\eta$，可以得到：$$\\omega = \\sum_{i = 1}^N\\alpha_iy_ix_i \\b = \\sum_{i = 1}^N\\alpha_iy_i$$输入：训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in \\mathcal{X} = R^n,y_i \\in \\mathcal{Y} = {-1,+1},i=1,2,\\cdots,N$；学习率$\\eta(0 \\lt \\eta \\leq 1)$输出：$\\alpha,b$感知机模型：$$f(x) = sign\\left(\\sum_{j = 1}^N\\alpha_jy_jx_j \\cdot x + b \\right),\\alpha = (\\alpha_1,\\alpha_2,\\cdots,\\alpha_N)^T$$ $\\alpha \\leftarrow 0,b \\leftarrow 0$; 在训练集中选取数据$(x_i,y_i)$ 如果$y_i\\left(\\sum_{j = 1}^N\\alpha_jy_jx_j \\cdot x_i + b \\right) \\leq 0$，则：$$\\alpha_i \\leftarrow \\alpha_i + \\eta \\b \\leftarrow b + \\eta y_i$$ 转至第二步直到没有误分类数据 注：在计算的过程中训练实例以内积的形式出现，可以预先计算储存下来，这个矩阵叫做Gram矩阵，$G = [x_i \\cdot x_j]_{N \\times N}$","link":"/blog/2022/09/14/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-02-%E6%84%9F%E7%9F%A5%E6%9C%BA/"},{"title":"统计学习方法学习笔记-03-k近邻法","text":"首先叙述$k$近邻算法，然后讨论$k$近邻模型及三个基本要素，最后讲述$k$近邻法的一个实现方法，$kd$树，介绍构造和搜索$kd$树的算法。 k近邻算法输入：训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中，$x_i \\in \\mathcal{X} \\subseteq R^n$为实例的特征向量，$y_i \\in \\mathcal{Y} = {c_1,c_2,\\cdots,c_K}为实例的类别$，$i = 1,2,\\cdots,N$；实例特征向量$x$输出：实例$x$所属的类$y$ 根据给出的距离度量，在训练集中找到和$x$最近的$k$个点，涵盖这$k$个点的$x$的邻域记作$N_k(x)$ 在$N_k(x)$中根据分类决策规则(如多数表决)决定$x$的类别$y$:$$y = arg\\ \\mathop{max}\\limits_{c_j}\\sum_{x_i \\in N_k(x)}I(y_i = c_j),i = 1,2,\\cdots,N;j = 1,2,\\cdots,K$$$I$为指示函数，当$y_i = c_j$时$I$为1，否则为0 k近邻模型的三要素$k$近邻法使用的模型实际上对应着对特征空间的划分，模型三要素为距离度量、$k$值的选择和分类决策规则 距离度量特征空间中两个实例点的距离是两个实例点相似程度的反映。设特征空间$\\mathcal{X}$是$n$维实数向量空间$R^n$，$x_i,x_j \\in \\mathcal{X},x_i = (x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(n)})^T,x_j = (x_j^{(1)},x_j^{(2)},\\cdots,x_j^{(n)})^T$ $L_p$距离：$$L_p(x_i,x_j) = \\left(\\sum_{l=1}^n|x_i^{(l)} - x_j^{(l)}|^p \\right)^{\\frac{1}{p}}$$ 欧式距离Euclidean distance：$p = 2$$$L_2(x_i,x_j) = \\left(\\sum_{l=1}^n|x_i^{(l)} - x_j^{(l)}|^2 \\right)^{\\frac{1}{2}}$$ 曼哈顿距离Manhattan distance：$p = 1$$$L_1(x_i,x_j) = \\sum_{l=1}^n|x_i^{(l)} - x_j^{(l)}|$$ 各个坐标距离的最大值：$p = \\infty$$$L_{\\infty}(x_i,x_j) = \\mathop{max}\\limits_l|x_i^{(l)} - x_j^{(l)}|$$ k值的选择 较小的$k$值：学习的近似误差会减小，估计误差会增大，预测结果会对邻近的实例点非常敏感，如果该点恰好是噪声，预测就会出错，也就是说$k$值的减小会使模型变得复杂，容易发生过拟合。 较大的$k$值：学习的近似误差会增大，估计误差会减小，也就是说$k$值的增大会使模型变得简单 一般使用交叉验证法来确定该值 分类决策规则 多数表决majority voting rule：如果分类的损失函数为0-1损失函数，分类函数为：$$f:R^n \\rightarrow {c_1,c_2,\\cdots,c_k}$$那么误分类的概率是$$P(Y \\neq f(X)) = 1 - P(Y = f(X))$$对于给定的实例$x \\in \\mathcal{X}$，其最邻近的$k$个训练实例点构成集合$N_k(x)$，如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是：$$\\frac{1}{k}\\sum_{x_i \\in N_k(x)}I(y_i \\neq c_j) = 1 - \\frac{1}{k}\\sum_{x_i \\in N_k(x)}I(y_i = c_j)$$要使误分类率最小即经验风险最小，就要使$\\sum_{x_i \\in N_k(x)}I(y_i = c_j)$最大，所以多数表决规则等价于经验风险最小化。 k近邻法的实现：kd树目的：对训练数据进行快速$k$近邻搜索 构造$kd$树输入：$k$维空间数据集$T = {x_1,x_2,\\cdots,x_N}$，其中$x_i = (x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(k)})^T,i = 1,2,\\cdots,N$输出：平衡$kd$树 构造根节点，使根节点对应于$k$维空间中包含所有实例点的超矩形区域； 对于深度为$j$的树结点，选择$x^{(l)}$为切分的坐标轴，$l = j(mod\\ k) + 1$，以该结点的区域中的所有实例点的$x^{(l)}$坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域，对应两个子结点，左子结点对应坐标$x^{(l)}$小于切分点的子区域，右子结点对应坐标$x^{(l)}$大于切分点的子区域，将落在切分超平面上的实例点保存在该结点； 重复第二步，直到两个子区域内没有实例点时终止； 搜索$kd$树输入：已构造的$kd$树，目标点$x$；输出：$x$的最近邻；更适用于训练实例数远大于空间维数的情况，平均计算复杂度为$O(\\log N)$ 在$kd$树中找到包含目标点$x$的叶结点：从根节点出发，递归的向下访问$kd$树。若目标点$x$的当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点，直到子结点为叶结点为止 以此叶结点为当前最近点 递归的向上回退，在每个结点进行以下操作：如果该结点保存的实例点比当前最近点距离目标更近，则以该实例点为当前最近点；当前的最近点一定存在于该结点一个子结点对应的区域，检查该子结点的父节点的另一个子结点对应的区域是否有更近的点，具体的，检查另一个子结点对应的区域是否与以目标点为球心，以目标点与当前最近点间的距离为半径的超球体相交，如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点，接着递归的进行最近邻搜索，如果不相交，向上回退， 当回退到根结点时，搜索结束，当前最近点即为$x$的最近邻点","link":"/blog/2022/09/14/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-03-k%E8%BF%91%E9%82%BB%E6%B3%95/"},{"title":"统计学习方法学习笔记-04-朴素贝叶斯法","text":"朴素贝叶斯的学习与分类，朴素贝叶斯的参数估计算法。 朴素贝叶斯法的学习与分类设输入空间$\\mathcal{X} \\subseteq R^n$为$n$维向量的集合，输出空间为类标记集合$\\mathcal{Y} = {c_1,c_2,\\cdots,c_K}$，输入为特征向量$x \\in \\mathcal{X}$，输出为类标记$y \\in \\mathcal{Y}$,$X$是定义在输入空间$\\mathcal{X}$上的随机向量，$Y$是定义在输出空间$\\mathcal{Y}$上的随机变量，$P(X,Y)$是$X$和$Y$的联合概率分布，训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$由$P(X,Y)$独立同分布产生。 先验概率分布：$$P(Y = c_k),k = 1,2,\\cdots,K$$ 条件概率分布：$$P(X = x|Y = c_k) = P(X^{(1)} = x^{(1)},\\cdots,X^{(n)} = x^{(n)}|Y = c_k),k = 1,2,\\cdots,K$$ 条件独立性假设下的概率分布：$$\\begin{aligned}P(X = x|Y = c_k) &amp;= P(X^{(1)} = x^{(1)},\\cdots,X^{(n)} = x^{(n)}|Y = c_k) \\&amp;= \\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)\\end{aligned}$$ 后验概率分布：$$\\begin{aligned}P(Y = c_k|X = x)&amp;= \\frac{P(X = x|Y = c_k)P(Y = c_k)}{\\sum_kP(X = x|Y = c_k)P(Y = c_k)} \\&amp;= \\frac{P(Y = c_k)\\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)}{\\sum_kP(Y = c_k)\\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)}\\end{aligned}$$ 朴素贝叶斯分类器：将实例分到后验概率最大的类中，这等价于期望风险最小化$$y = f(x) = arg \\mathop{max}\\limits_{c_k}\\frac{P(Y = c_k)\\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)}{\\sum_kP(Y = c_k)\\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)}$$分母与类别无关所以：$$y = f(x) = arg \\mathop{max}\\limits_{c_k}P(Y = c_k)\\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)$$ 朴素贝叶斯法的参数估计极大似然估计在朴素贝叶斯法中，学习意味着估计先验概率$P(Y = c_y)$和条件概率分布$P(X^{(j)} = x^{(j)}|Y = c_k)$ 先验概率的学习：$$P(Y = c_k) = \\frac{\\sum_{i = 1}^NI(y_i = c_k)}{N},k = 1,2,\\cdots,K$$ 条件概率的学习：$$P(X^{(j)} = a_{jl}|Y = c_k) = \\frac{\\sum_{i = 1}^NI(x_i^{(j)} = a_{jl},y_i = c_k)}{\\sum_{i = 1}^NI(y_i = c_k)} \\j = 1,2,\\cdots,n;\\ l = 1,2,\\cdots,S_j;\\ k = 1,2,\\cdots,K$$第$j$个特征$x^{(j)}$的可能取值的集合为${a_{j1},a_{j2},\\cdots,a_{jS_j}}$，$x_i^{(j)}$是第$i$个样本的第$j$个特征，$a_{jl}$是第$j$个特征可能取的第$l$个值，$I$是指示函数。 学习与分类算法 计算先验概率和条件概率 对于给定的实例$x = (x^{(1)},x^{(2)},\\cdots,x^{(n)})^T$，计算：$$P(Y = c_k)\\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)$$ 确定实例的类别$$y = arg \\mathop{max}\\limits_{c_k}P(Y = c_k)\\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)$$ 贝叶斯估计目的：用极大似然估计可能会出现所要估计的概率值为0的情况，解决的办法是采用贝叶斯估计 条件概率的贝叶斯估计是：$$P_\\lambda(X^{(j)} = a_{jl}|Y = c_k) = \\frac{\\sum_{i = 1}^NI(x_i^{(j)} = a_{jl},y_i = c_k) + \\lambda}{\\sum_{i = 1}^NI(y_i = c_k) + S_j\\lambda} \\$$式中$\\lambda \\geq 0$，当$\\lambda = 0$时就是极大似然估计，常取$\\lambda = 1$，这时称为拉普拉斯平滑 先验概率的贝叶斯估计：$$P_\\lambda(Y = c_k) = \\frac{\\sum_{i = 1}^NI(y_i = c_k) + \\lambda}{N + K\\lambda}$$分母中$\\lambda$前面的系数是用来保证概率和为1","link":"/blog/2022/09/14/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-04-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"}],"tags":[{"name":"hexo","slug":"hexo","link":"/blog/tags/hexo/"},{"name":"icarus","slug":"icarus","link":"/blog/tags/icarus/"},{"name":"makefile","slug":"makefile","link":"/blog/tags/makefile/"},{"name":"基础","slug":"基础","link":"/blog/tags/%E5%9F%BA%E7%A1%80/"},{"name":"机器学习","slug":"机器学习","link":"/blog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"docker","slug":"docker","link":"/blog/tags/docker/"},{"name":"算法","slug":"算法","link":"/blog/tags/%E7%AE%97%E6%B3%95/"},{"name":"模式识别","slug":"模式识别","link":"/blog/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"},{"name":"自然语言处理","slug":"自然语言处理","link":"/blog/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"离散数学","slug":"离散数学","link":"/blog/tags/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/"},{"name":"高级人工智能","slug":"高级人工智能","link":"/blog/tags/%E9%AB%98%E7%BA%A7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"统计学习方法","slug":"统计学习方法","link":"/blog/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}],"categories":[{"name":"网站创建","slug":"网站创建","link":"/blog/categories/%E7%BD%91%E7%AB%99%E5%88%9B%E5%BB%BA/"},{"name":"个人小记","slug":"个人小记","link":"/blog/categories/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E8%AE%B0/"},{"name":"linux","slug":"linux","link":"/blog/categories/linux/"},{"name":"刷题笔记","slug":"刷题笔记","link":"/blog/categories/%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"},{"name":"机器学习-李宏毅","slug":"机器学习-李宏毅","link":"/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"卜算法","slug":"卜算法","link":"/blog/categories/%E5%8D%9C%E7%AE%97%E6%B3%95/"},{"name":"模式识别与机器学习","slug":"模式识别与机器学习","link":"/blog/categories/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"自然语言处理","slug":"自然语言处理","link":"/blog/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"离散数学","slug":"离散数学","link":"/blog/categories/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/"},{"name":"高级人工智能","slug":"高级人工智能","link":"/blog/categories/%E9%AB%98%E7%BA%A7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"统计学习方法","slug":"统计学习方法","link":"/blog/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}]}