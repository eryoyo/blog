{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/blog/2022/04/28/hello-world/"},{"title":"使用hexo-icarus快速创建自己的博客网站","text":"使用hexo+icarus快速搭建属于自己的博客网站 准备环境 安装nodejs✅ 安装git✅ 安装hexo✅ 12# 此为全局安装，可能需要sudo权限npm install -g hexo-cli 创建git仓库直接在github主页创建一个新的仓库，此处假设仓库名称为blog_tensorrt 使用hexo建初始博客首先初始化一个博客项目，此处blog可以换成自己想要起的名称。该操作之后在当前目录下会出现一个叫做blog的新的文件夹 1hexo init blog 进入blog文件夹下 1cd blog 可以看到当前的文件夹下有一个themes的文件夹，此时看到里面没有文件，下载icarus主题代码到其中 1git clone git@github.com:ppoffice/hexo-theme-icarus.git /themes/icarus 之后修改_config.yml文件，将theme修改为icarus 1theme: icarus 之后在命令行进行构建 1hexo g 输入生成命令可能会报错，提示有没有安装的包，安装确实的包 1yarn add bulma-stylus@0.8.0 hexo-component-inferno@^1.1.0 hexo-pagination@^2.0.0 hexo-renderer-inferno@^0.1.3 inferno@^7.3.3 inferno-create-element@^7.3.3 接着生成 12# 该命令多执行几次，知道没有新的文件生成hexo g 查看网页初始效果 1hexo s 打开网页http://localhost:4000 自定义博客设计此时博客目录下有文件_config.icarus.yml，修改该文件即可，每一项在icarus官网https://ppoffice.github.io/hexo-theme-icarus/Configuration/icarus%E7%94%A8%E6%88%B7%E6%8C%87%E5%8D%97-%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/#more均有详细的说明，在此不做赘述。 部署网站首先修改_config.yml文件 123456789101112# Sitetitle: eryoyo的博客subtitle: 坚持✊description: tensorrt笔记整理keywords: author: eryoyolanguage: zh-CNtimezone: Asia/Shanghai# URL## Set your site url here. For example, if you use GitHub Page, set url as 'https://username.github.io/project'url: https://eryoyo.github.io/blog_tensorrt 之后进行本地查看 123hexo cleanhexo ghexo s 网站可以在http://localhost:4000/blog_tensorrt里面查看到 之后接着修改_config.yml文件 1234deploy: type: git repo: git@github.com:eryoyo/blog_tensorrt.git branch: master 安装部署需要的包 1npm install hexo-deployer-git --save 之后部署 1hexo deploy 在仓库里面setting里面修改github pages的none为master分支，点击save，等待一会之后就可以在访问自己刚刚部署到的网站了","link":"/blog/2022/04/28/%E4%BD%BF%E7%94%A8hexo-icarus%E5%BF%AB%E9%80%9F%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/"},{"title":"博客链接整理","text":"存放自己博客相关的链接 发博客平台 GitHub: https://eryoyo.github.io/blog/ 知乎: https://www.zhihu.com/creator/manage/creation/all 博客园： https://i.cnblogs.com/posts CSDN: https://mp.csdn.net/ bilibili: https://member.bilibili.com/platform/home eryoyo: https://eryoyo.xyz/admin/login 微信公众号： https://mp.weixin.qq.com/cgi-bin/home 创建博客使用 github博客评论区管理： https://disqus.com/ https://disqus.com/home/ github博客markdown书写前front-matter管理和文档: https://hexo.io/zh-cn/docs/front-matter hexo主题市场: https://hexo.io/themes/ icarus主题配置: https://ppoffice.github.io/hexo-theme-icarus/Configuration/icarus%E7%94%A8%E6%88%B7%E6%8C%87%E5%8D%97-%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/#%E5%AF%BC%E8%88%AA%E6%A0%8F icarus快速上手: https://ppoffice.github.io/hexo-theme-icarus/uncategorized/icarus%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/#install-source icarus源码: https://github.com/ppoffice/hexo-theme-icarus 腾讯云控制台: 域名：https://console.cloud.tencent.com/domain 备案：https://console.cloud.tencent.com/beian/manage SSL证书：https://console.cloud.tencent.com/certoverview 服务器：https://console.cloud.tencent.com/lighthouse/instance/index eryoyo服务器模板源码: https://github.com/ZHENFENG13/My-Blog eryoyo网站配图： https://en.wikipedia.org/wiki/Limerick","link":"/blog/2022/04/28/%E5%8D%9A%E5%AE%A2%E9%93%BE%E6%8E%A5%E6%95%B4%E7%90%86/"},{"title":"linux系统中makefile书写规则","text":"本文自https://blog.csdn.net/haoel/article/details/2886整理而来，便于自己快速回顾makefile的书写。 基础规则makefile使得工程完成自动化编译，告诉make命令如何编译和链接程序。编译过程是指生成.o中间代码文件，链接是指将中间代码文件合起来成为可执行文件。编译关注的是语法正确和声明正确，链接会关注函数实现，也就是在所有的中间代码文件当中寻找函数实现，如果没有找到就会报错，需要指定中间代码文件位置。 下面是makefile的基础书写规则，也就是利用command来从prerequisites生成target。target表示目标文件，prerequisites表示生成target需要的文件，command表示任意的shell命令，以Tab键开头。 12target:prerequisites command 遵循着一个规律，也就是当target不存在的时候会直接生成，假如target存在，就会查看prerequisites是否比target新，假如是就重新生成target，也就是重新执行command。 常用的点 /表示换行","link":"/blog/2022/04/29/linux%E7%B3%BB%E7%BB%9F%E4%B8%ADmakefile%E4%B9%A6%E5%86%99%E8%A7%84%E5%88%99/"},{"title":"刷题基础1","text":"主要总结使用C++刷题时会使用的一些基础知识 cstdio头文件：程序中处理输入输出 scanf printf %d %lld f% lf% c% s% 变量类型：int(9), long long(18), float(128), double(1024), char(127), bool(0,1) 0~9(48~57),A~Z(65~90),a~z(97~122) ^相同为0，不同为1 scanf(“%d:%d:%d”, &amp;hh, &amp;mm, &amp;ss) 除了%c以外，scanf对其他格式符例如%d、s%的输入是以空白符（空格、Tab）为结束判断标志的 %md:保持m位右对齐输出，不足m位用空格补齐 %0md:用0补齐 %.mf:使浮点数保留m位小数输出 getchar:输入单个字符，可识别换行符，putchar:输出单个字符 typedef long long ll:给long long起一个别名ll math函数：fabs(取浮点数绝对值),floor,ceil(向下、上取整)，pow(a, b):返回a的b次方，sqrt:算术平方根，log:以自然对数为底的对数，没有对任意底数求对数的函数，使用换底公式：以a为底b的对数=以自然对数为底b的对数除以以自然对数为底a的对数，sin,cos,tan,asin,acos,atan,pi=acos(-1.0)，round四舍五入 给数组赋初值0：int a[10] = {0} 当数组较大时应该定义在主函数的外面10^6 memset(数组名， 值， sizeof(数组名)):为数组里面的每一个元素赋相同的初值, string.h头文件，值=0赋全0，值=-1赋全1 字符串数组赋初值可以用字符串”hello world” scanf,printf输入输出：c%识别空格和换行，s%以空格和换行作为分隔符 getchar(),putchar(char)输入输出：识别换行符 gets(str),puts(str)输入输出：以换行符作为分隔符 字符数组的末尾有\\0,表示空字符，占用一个字符位 gets和printf会在末尾自动添加，此外需要自己添加\\0 string.h strlen(字符数组):返回\\0字符前的字符个数，不包含\\0 strcmp(字符数组a，字符数组b)：字典序排序, a == b:返回0, a &lt; b:返回负数, a &gt; b:返回正数 strcpy(字符数组a，字符数组b):将b复制给a,包括\\0 strcat(字符数组a，字符数组b):将b拼接到a后面 sscanf(str, “%d”, &amp;n):将str以整数的格式输出到n sprintf(str, “%d”, n):将n以整数的格式写到字符数组中 数组作为函数参数的时候，第一维不需要填写长度，第二维需要, 不允许返回数组 指针变量支持加减法操作和自增自减操作，操作单位为其基类型，例如int为其基类型, 储存的是int类型变量的地址，那么＋1之后指向当前变量的下一个int 数组名称可作为数组的首地址来使用 指针的引用：int* &amp;p指针本质也是一个无符号整数，所以可以使用引用 cin和cout:iostream char str[100]; cin.getline(str, 100); string str; getline(cin, str); cout设置小数精度：头文件iomanip, cout&lt;&lt;setiosflags(ios::fixed)&lt;&lt;setprecision(2)&lt;&lt;123.4567&lt;&lt;endl输出123.46 浮点数的比较： const double eps = 1e-8 #define Equal(a, b) ((fabs((a) - (b))) &lt; (eps)) 圆周率π：const double pi = acos(-1.0) 0.00输出为-0.00只能先将结果存到字符串中然后与-0.00比较配合eps修正为0.00 sqrt,asin,acos配合eps使得变量在定义域内 scanf有返回值，代表成功读入的参数个数，如果读取失败则返回EOF while(scanf(“%d”, &amp;n) != EOF) while(gets(str) != NULL) 用一个结构体给另一个结构体赋值的时候，结构体里面假如有字符串数组，赋值的过程中不是用的地址，而是地址上面的值","link":"/blog/2022/04/29/%E5%88%B7%E9%A2%98%E5%9F%BA%E7%A1%801/"},{"title":"刷题基础2","text":"总结使用C++刷题时STL的使用 vector 初始化： 123vector&lt;typename&gt; name;// vector数组:vector&lt;typename&gt; arrayname[size]; 访问： 12345678910// 通过下标vec[i];// 通过迭代器vector&lt;typename&gt;::iterator it = name.begin(); *it; it++;// 在常用的STL容器中只有vector和string允许使用it+i的用法*(vec.begin() + i); 操作： 1234567vec.push_back(x)vec.pop_back()vec.size()vec.clear() // 清空vector中所有元素vec.insert(it, x) // 在it所指位置处插入一个元素，当前位置原先的元素及其之后的元素后移vec.erase(it) // 删除it所指位置的元素vec.erase(it1, it2) //删除[it1,it2)区间内的所有元素 set 初始化： 123set&lt;typename&gt; name;// set数组set&lt;typename&gt; arrayname[size]; 访问： 1234// 通过迭代器set&lt;typename&gt;::iterator it = name.begin();*it; it++; 操作： 1234567name.insert(x) // 复杂度O(logn)name.find(x) // 返回集合中值为x的迭代器，时间复杂度为O(logn)name.erase(it) // 删除it所指位置的元素，时间复杂度为O(1)name.erase(x) // 删除集合中值为x的元素，复杂度为0(logn)name.erase(it1, it2) // 删除[it1, it2)区间内的元素name.size() // 获取元素的个数，复杂度为O(1)name.clear() // 清空所有元素，复杂度为O(n) 注： set的作用是去重并升序排序 multiset允许元素重复 unordered_set:其只实现去重不实现排序，速度更快，原理为散列实现代替set的红黑树（自平衡二叉树）实现 string 初始化： 12345string str = &quot;abcd&quot;;cin&gt;&gt;str; cout&lt;&lt;str;// scanf不能输入stringprintf(&quot;%s&quot;, str.c_str()); 访问元素： 123456// 通过下标// 通过迭代器：string::iterator it = str.begin();it++;*it; 常用函数： 12345678910111213141516171819str1 += str2; // 将str2拼接到str1后面str1 = str1 + str2;// ==, !=, &lt;, &lt;=, &gt;, &gt;=:以字典序比较两个字符串str.length();str.size();str.insert(pos, string); // 从下标为pos的位置开始插入字符串，该位置及其之后的元素全部后移str.insert(it, it1, it2); // 从it所指位置开始插入it1到it2之间的元素str.erase(it); // 删除单个元素str.erase(it1, it2); // 删除一个区间内的所有元素str.erase(pos, length); // 删除下标为pos开始的length个元素str.clear(); // 清空元素str.substr(pos, len); // 取出len个元素string::npos; // find函数失配时的返回值，常量-1或4294967295str1.find(str2); // 当str2是str1的子串的时候，返回第一次出现的位置，否则返回string::npos,复杂度为O(mn)str.replace(pos, len, str2);str.replace(it1, it2, str2); // 将范围内的字符串替换为str2，修改了原字符串 map 初始化： 1map&lt;typename1, typename2&gt; mp; // key可以为STL类型，但是不能用数组类型 访问元素： 12345678// 元素按照key的大小升序排序// 通过下标访问// 通过迭代器：map&lt;typename1, typename2&gt;::iterator it;it-&gt;first; it-&gt;second; 常用函数： 123456mp.find(key); // 返回key对应的元素的迭代器mp.erase(it);mp.erase(key);mp.erase(it1, it2);mp.size();mp.clear(); queue 初始化： 1queue&lt;typename&gt; name; 访问元素： 12q.front(); // 最先进入的元素; q.back(); 常用函数： 123456q.front();q.back();q.pop();q.push(x);q.empty();q.size(); priority_queue 初始化： 1priority_queue&lt;typename&gt; name; // 大顶堆 访问元素： 1q.top(); 常用函数： 12345q.push(x); // O(logn)q.top(); // O(1)q.pop(); // O(logn)q.empty(); // O(1)q.size() 自定义优先级队列： 12345678910111213141516171819202122// 基本数据类型：// 小顶堆：priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; q;// 大顶堆(默认)：priority_queue&lt;int, vector&lt;int&gt;, less&lt;int&gt;&gt; q;// 结构体：// 重载&lt;符号：写在结构体里面friend bool operator &lt; (student a, student b){ return a.age &lt; b.age;//大顶堆 return a.age &gt; b.age;//小顶堆}priority_queue&lt;student&gt; q;// 重载：写在结构体外面struct cmp{ bool operator () (fruit f1, fruit f2){ return f1.price &lt; f2.price;//大顶堆 return f1.price &gt; f2.price;//小顶堆 }}priority_queue&lt;fruit, vector&lt;fruit&gt;, cmp&gt; q; stack 初始化： 1stack&lt;typename&gt; s; 访问元素： 1s.top(); 常用函数： 12345s.push(x);s.top();s.pop();s.empty();s.size(); pair 初始化： 123// utility头文件，包含了map头文件也就是包含了utility, map的元素为元组pair&lt;typename1, typename2&gt; name(e1, e2);p = make_pair(e1, e2); 访问元素： 12p.first;p.second; 常用函数： ==, !=, &lt;, &lt;=, &gt;, &gt;=,先比较first，再比较second algorithm12345678910111213max(a, b);min(a, b);abs(x);swap(a, b);reverse(it1, it2); // 将[it1, it2)范围内的元素翻转next_permutation(a, a + 5); // 返回下一个全排列fill(a, a + 5, 233); // 将数组指定范围内填充为指定数字sort(it1, it2, cmp); // it为指针或者迭代器bool cmp(int i1, int i2){ return i1 &gt; i2;//递减排列}lower_bound(it1, it2, val); // 返回第一个&gt;=val的元素, 否则返回可以插入该元素的指针或者迭代器, 找到则返回指针或迭代器upper_bound(it1, it2, val); // 返回第一个&gt;val的元素","link":"/blog/2022/04/29/%E5%88%B7%E9%A2%98%E5%9F%BA%E7%A1%802/"},{"title":"机器学习基本概念","text":"机器学习：寻找一个函数的能力 在语音识别中，这个函数将一段语音转化为文字；在图像分类中，这个函数得到图片的类别；在下围棋中，这个函数获取下一步的棋子应该下在什么地方 Regression：寻找输出连续值的函数 classification：寻找输出离散值的函数，alpha go也可以看作一个分类的任务 structure learning：让机器创造一些东西，包括结构化的图片或者文档 模型: $y=b+wx_1$ 特征$x_1$，权重$w$，偏置$b$，标签$\\hat{y}$，损失函数$L(b,w)$ ​ mean absolute errorMAE: $e=|y-\\hat{y}|$ ​ Mean square error(MSE): $e=(y-\\hat{y})^2$ Optimization: 找到最佳的$w$, $b$使得$w^*,b^*=argmin\\ L$ ​ Gradient descent: 首先随机初始化，之后计算关于$w,b$的梯度，更新$w,b$，$w^1\\leftarrow W^0-\\eta\\frac{\\partial L}{\\partial w}$","link":"/blog/2022/05/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"},{"title":"深度学习基础概念","text":"普通的线性模型不能满足更加复杂的需求 更加复杂的曲线可以用一系列曲线叠加的方式得到，这样的曲线抽象为$y=c\\frac{1}{1+e^{-(b+wx_1)}}$，即sigmoid函数$$r = b + Wx\\a = \\sigma(r)\\y = b + c^Ta\\y = b + c^T \\ \\sigma(b + Wx)$$除了模型变复杂之外，损失函数和优化的过程是一样的 另外一种激活函数ReLU：$c\\ max(0, b + wx_1)$ rectified linear unit，两个ReLU可以生成一个hard sigmoid 可以使模型变得更加复杂，也就是增加层数。但是不能将网络变得过于复杂，会出现过拟合现象 为什么不将网络直接变胖，而是需要变深呢？","link":"/blog/2022/05/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"},{"title":"Colab和Pytorch","text":"李宏毅课程中Colab, Pytorch总结 Colab 点击+ Code来添加code cell，点击+ Text来添加text cell，还有将cell上移以及下移的选项，可以将cell删除 在code cell中有两种命令可以输入，一种是python代码，直接输入即可；另一种是shell命令，在命令的前面需要加上! 命令的前面加上!，会先执行这个命令，之后就会杀掉这个进程，而在命令的前面加上%就会影响和这个jupyter相关的进程，称为magic command，可以查看https://ipython.readthedocs.io/en/stable/interactive/magics.html来知道更多的magic command 更换为使用GPU：点击Runtime-点击Change Runtime Type-选择GPU，更换runtime的操作会重启这个session 点击cell前面的运行按钮就会执行这个cell的代码，也可以点击runtime里面的选项来运行代码 查看抽到的GPU类型!nvidia-smi，有三种可能性P100&gt;T4&gt;k80 通过google drive下载文件，google drive分享的文件如下链接https://drive.google.com/open?id=xxxxxx，这个xxxxxx就是文件ID，使用下列命令来下载`!gdown –id ‘xxxxxx’ –output pikachu.png` 下载之后的文件可以通过点击文件夹图标查看，这些文件都是暂时储存的，在session结束之后就会被清理 可以点击上载图标来上传本地文件，可以下载文件 可以不用每次都上传以及下载文件，直接将google drive挂载到colab即可，首先导入包from google.colab import drive之后挂载drive.mount('/content/drive')","link":"/blog/2022/05/06/Colab%E5%92%8CPytorch/"},{"title":"Docker总结","text":"总结Docker的使用小知识 Docker使得程序的部署变得简单，Docker依赖的底层技术支持包括namespace实现进程之间的隔离，Control Groups来完成资源控制，资源包括核心数、内存和硬盘等等。依赖Union file sstem完成Container和image的分层。 镜像：分层的，是只读的，镜像可以理解为一个树状的结构，依赖关系体现在dockerfile文件中 容器：通过image来创建，在image上又添加了一层，这一层可以读写 仓库：dockerhub 容器的启动过程： 检查镜像是否存在，假如不存在下载 利用镜像创建一个容器 启动刚刚创建的容器 分配一个文件系统给容器，相当于在镜像层外挂载一个可读可写层 从宿主机的网桥接口中桥接一个给容器 从网桥中分一个IP地址给容器 执行用户指定的yinys","link":"/blog/2022/05/16/Docker%E6%80%BB%E7%BB%93/"},{"title":"卜算法学习笔记-lecture1-绪论","text":"算法的概念算法是指给出解决问题的操作步骤之后，无论是人还是机器都可以按照步骤机械性的执行得到问题的结果。我们在日常生活中回遇到各种的实际问题遇到之后的解决流程如下： 首先在一系列世纪问题中找到一个特定的topic，得到一个实际问题，在这个实际问题的基础之上我们可以抽象出数学问题，之后通过对该数学问题的观察，特别是对输入输出结构的观察，得到解决问题的算法。面对一个数学问题，我们首先问自己以下几个问题。 这个问题的最简单版本是否可以解决，假如不可以，我们可以降低问题的难度直到简单版本可以解决； 问题是否可以拆分为小问题，假如可以拆分，那么采用devide andconquer方法，如果有最优子结构，可以尝试动态规划，还有短视的策略贪心可以使用； 可行解的形式是什么，是否采用枚举办法，枚举时需要注意剪枝； 问题的解是否可以通过微小扰动变为另一种形式，这种可以采用逐步改进的办法解决问题，包括线性规划、局部搜索和半退火、网络流等； 基于旅行商问题介绍三种算法设计过程traveling salesman problem, TSP 输入：结点集合$V={v_1,…,v_n}$，以及结点之间的距离矩阵$D = (d_{i,j}) \\in R^{n\\times n}$,其中$d_{i,j}$表示节点$i$与节点$j$之间的距离； 输出：最短的环游路线，即从任意节点作为出发点，经过每个节点一次且仅一次，最终返回出发点的里程最短的环游；分而治之算法设计过程简介我们需要观察这个问题是否可以分解成简单实例，并且简单实例的解是否可以组合出复杂实例的解。 第一种尝试，我们尝试减少结点数，可以看到，我们可以很容易的将五个结点的实例变为四个结点的实例，但是简单实例不太容易组合成复杂实例; 第二种尝试，求解一个辅助问题，计算从起始结点$s$出发，经过中间结点集合$S$，最终达到目的节点$x$的最短路径，其长度计为$M(s,S, x)$; 可以看到在第二种尝试之下可以将这个问题分解为简单实例并可以合起来组成复杂实例。以5个结点的实例为例，包含结点$a,b,c,d,e$，以$a$为起始节点，那么可以从$b,c,e$返回起点，最短里程可以表示为$min{d_{b,a} + M(a,{c,d},b), d_{c,a} + M(a,{b,d},c), d_{e,a} + M(a,{c,b},e)}$这个算法可以表示为以下伪代码计算$M(s,S,x)$的伪代码算法的复杂度计算如下，我们需要枚举所有的结点子集$S$，所以总共有$2^n$个子集，路线起点确定，但是终点$x$有$n$种可能，所以总共要$O(2^nn)$才能计算出$M(s,S,x)$表格的值，计算出来之后还要经过$n$次比较的到最终的结果，Bellman-held-karp的复杂度为$O(2^n n^2)$. 逐步改进的算法设计流程基本过程是从问题的一个粗糙的，质量不太高的完整可行解开始，不断改进，直到获得满意的解为止，一般性框架为。求解过程中需要关注三个方面： 初始粗糙可行解的选择； 可行解的改进办法，也即扰动办法； 算法的终止条件，常见的终止条件包括当前的可行解无法进一步改进；迭代次数超过预先定义的值；当前可行解的质量超过预先定义的阈值； 智能枚举算法设计流程通过观察解的形式来枚举 枚举边的各种情况，最终的解的形式是边的集合，枚举各种边是否存在的情况，在这个过程中可以进行剪枝 枚举点的各种情况，也就是将环游表示成$X=x_1,x_2,…,x_{n-2},x_i \\in V （1\\leq i \\leq n - 2)$. 算法的复杂度 时间复杂度：算法执行过程中总的基本操作次数 空间复杂度：算法所使用的存储单元数目 因为仅凭复杂度在一个或几个特定实例上的时间和空间复杂度难以评估其性能，因此，常用的方式是考虑具有同等规模的所有实例 最坏情况时间复杂度：$worst-case\\ time\\ complexity$在所有实例上基本操作次数的最大值作为时间复杂度 最坏情况空间复杂度：$worst-case\\ space\\ complexity$在所有实例上使用存储单元数目的最大值作为空间复杂度 还有平均情况时间复杂度和平均情况空间复杂度的概念，但是需要知道问题实例的概率分布 算法根据时间复杂度分为指数时间算法和多项式时间算法，时间复杂度为$O(n^{\\log n})$的算法是指数时间算法 大O记号 大O记号:考虑两个函数 $f(n)$ 和 $g(n)$，其定义域是正整数，值域是正实数。如果存在一个正数 $c &gt; 0$ 以及 $N &gt; 0$，使得对任意的 $n &gt; N$，总有 $f(n) ≤ cg(n)$成立，则记为 $f(n) = O(g(n))$。 $\\Omega$记号：考虑两个函数 $f(n)$ 和 $g(n)$，其定义域是正整数，值域是正实数。如果$f(n) = O(g(n))$，则可以记为$g(n) = \\Omega(f(n))$ $\\Theta$记号：如果 $f(n) = O(g(n))$ 和 $g(n) = O(f(n))$ 同时成立，则可以记为 $f(n) = \\Theta(g(n))$。","link":"/blog/2022/08/31/%E5%8D%9C%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture1-%E7%BB%AA%E8%AE%BA/"},{"title":"模式识别与机器学习学习笔记-lecture1-绪论","text":"概述模式：存在于世间和空间中可观察的物体，如果我们可以区别他们是否相同或是否相似，都可以称之为模式。 模式的直观特性： 可观察性 可区分性 相似性 模式识别：对周围物体的认识、人的识别、声音的辨别、气味的分辨 数据聚类目标：用某种相似性度量的方法见原始数据组成有意义的和有用的各种数据集是一种非监督学习的方法，解决方案是数据驱动的 统计分类基于概率统计模型得到各类别的特征向量的分布，以取得分类的方法；特征向量分布的获得是基于一个类别已知的训练样本集；是一种监督分类的方法，分类器是概念驱动的； 结构模式识别该方法通过考虑识别对象的各部分之间的联系来达到识别分类的目的；识别采用结构匹配的形式，通过计算一个匹配程度值（matching score）来评估一个未知的对象或位未知对象某些部分与某种典型模式的关系如何；当制定出来一组可以描述对象部分之间关系的规则后，可以应用一种特殊的结构模式识别方法-句法模式识别，来检查一个模式基元的序列是否遵守某种规则，即句法规则或语法； 神经网络由一系列互相联系的、相同的单元（神经元）组成，相互间的联系可以在不同的神经元之间传递增强或抑制信号；增强或抑制是通过调整神经元相互间联系的权重系数来实现；神经网络可以实现监督和非监督学习条件下的分类； 监督学习监督学习是从有标记的训练数据来推断或建立一个模型 无监督学习无监督学习与监督学习的不同之处在于，没有任何训练样本，需要直接对数据进行建模，寻找数据的内在结构及规律，如类别和聚类； 半监督学习利用少量的标注样本和大量的未标注样本进行训练和分类 增强学习机器人选择一个动作用于环境，环境接受该动作后状态发生变化，同时产生一个强化信号反馈回来，机器人根据强化信号和环境当前状态再选择下一个动作； 集成学习ensemble learning指联合训练多个弱分类器并通过集成策略将弱分类器组合使用；常见的集成策略有：boosting, bagging, random subspace, stacking；常见的算法主要有：决策树、随机森林、adaboost, GBDT, DART等； 深度学习源于人工神经网络的研究，通过层次化模型结构可从低层原始特征中逐渐抽象出高层次的语义特征； 元学习meta learning学会学习，利用以往的知识经验来指导新任务的学习，具有学会学习的能力 多任务学习联合训练多个学习任务 多标记学习处理的数据集中的每个样本可同时存在多个类标 对抗学习系统构成模式识别系统 机器学习","link":"/blog/2022/08/31/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture1-%E7%BB%AA%E8%AE%BA/"},{"title":"模式识别与机器学习学习笔记-数学知识","text":"数学期望(均值)和方差随机变量X的数学期望(或称均值）记作$E(x)$，它描述了随机变量的取值中心，随机变量$(X-E(X))^2$的数学期望称为$X$的方差，记作$\\sigma^2$，而$\\sigma$称为$X$的均方差（标准差）。它描述了随机变量与均值的偏差的疏密程度。 若$X$是连续型随机变量，其分布密度为$p(x)$，则当积分绝对收敛的时候 $$m = E(X)=\\int_{-\\infty}^{\\infty}xp(x)dx \\\\sigma^2 = E{(X-m)^2} = \\int_{-\\infty}^{\\infty}(x-m)^2p(x)dx$$ 若$X$是离散型随机变量，其可能取值为$x_k,k=1,2,…,$且$P(X=x_k)=p_k$，则（当级数是绝对收敛时）$$m = E(X)=\\sum^{\\infty}{k=1}x_kp_k \\D(X) = \\sum^{\\infty}{k=1}(x_k-m)^2p_k$$协方差矩阵协方差矩阵说明随机向量$X$的各分量的分散情况，定义为：$$\\begin{aligned}c&amp;=E{(X-m)(X-m)^T} \\&amp;=E\\left{\\left[\\begin{matrix}(X_1-m_1) \\\\vdots \\(X_1-m_1)\\end{matrix}\\right]\\left[\\begin{matrix}(X_1-m_1)\\cdots(X_n-m_n)\\end{matrix}\\right]\\right}\\&amp;=\\left[\\begin{matrix}E[(X_1-m_1)(X_1-m_1)]\\cdots E[(X_1-m_1)(X_n-m_n)] \\\\vdots \\ddots \\vdots \\E[(X_n-m_n)(X_1-m_1)]\\cdots E[(X_n-m_n)(X_n-m_n)]\\end{matrix}\\right] \\&amp;=\\left(\\begin{matrix}\\lambda_{11} \\cdots \\lambda_{1n} \\\\vdots \\ddots \\vdots \\\\lambda_{n1} \\cdots \\lambda_{nn}\\end{matrix}\\right)\\end{aligned}$$其中，协方差矩阵的各分量为：$$\\lambda_{ij} = E[(X_i-m_i)(X_j-m_j)]$$若$i \\neq j$，则$\\lambda_{ij}$是$X$的第$i$个分量与第$j$个分量的协方差；若$i = j$，则$\\lambda_{ij}$是随机变量$X_i$的方差，即协方差矩阵的对角分量；一维正态密度函数一维随机变量$X$的正态密度函数表示为：$$p(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left[-\\frac{(x-m)^2}{2\\sigma^2}\\right]$$其中均值$m=E(X)=\\int_{-\\infty}^{\\infty}xp(x)dx$；方差$\\sigma^2=E{(X-m)^2}=\\int_{-\\infty}^{\\infty}(x-m)^2p(x)dx$，$\\sigma$为标准差。在$m$左右各为$k\\sigma$的范围内，概率为：$$\\begin{aligned}p{m-k\\sigma\\leq x\\leq m+k\\sigma}&amp;= \\int_{m-k\\sigma}^{m+k\\sigma}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left[-\\frac{1}{2}\\left(\\frac{x-m}{\\sigma}\\right)^2\\right]dx \\&amp;= \\frac{1}{\\sqrt{2\\pi}}\\int_{-k}^k exp\\left[-\\frac{y^2}{2}\\right]dy\\end{aligned}$$其中，$y=(x-m)/\\sigma$，此时$p$与$k$的关系：$$p{m-k\\sigma \\leq x \\leq m + k\\sigma}=\\begin{cases} 0.683 &amp; k = 1 \\0.954 &amp; k = 2 \\0.997 &amp; k = 3\\end{cases}$$因此，在区间$|x-m|\\leq 3\\sigma$内，差不多包含了全部由正态样本取样的子样本。正态密度函数可完全由均值和方差所决定，因此可以由下式表示：$p(x)\\sim N(m,\\sigma^2)$ 多维正态密度函数$n$维随机向量的正态密度函数表示为：$$p(x)=\\frac{1}{(\\sqrt{2\\pi})^{\\frac{n}{2}}|C|^\\frac{1}{2}}exp\\left{-\\frac{1}{2}(x-m)^TC^{-1}(x-m)\\right}$$其中$$x=\\left[\\begin{matrix}x_1 \\\\vdots \\x_n\\end{matrix}\\right],m=\\left[\\begin{matrix}m_1 \\\\vdots \\m_n\\end{matrix}\\right],C=\\left[\\begin{matrix}\\sigma_{11}^2 &amp; \\cdots &amp; \\sigma_{1n}^2 \\\\vdots &amp; \\ddots &amp; \\vdots \\\\sigma_{n1}^2 &amp; \\cdots &amp; \\sigma_{nn}^2\\end{matrix}\\right]$$$|C|$为协方差矩阵$C$的行列式。多维正态密度函数由其均值$m$和协方差矩阵$C$确定，因此可用下式表示：$$p(x) \\sim N(m, C)$$","link":"/blog/2022/09/01/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/"},{"title":"卜算法学习笔记-latex必备知识","text":"开始第一个project创建一个.tax文件或者在overleaf中创建一个project 123456\\documentclass{article}\\begin{document}First document. This is a simple example, with no extra parameters or packages included.\\end{document} 第一行表示文章的类别class为article，还有一些类别，如book、report等； 之后的内容是文章的正文，包裹在\\begin(document}和\\end{document}中； 正文里面的内容可以进行编辑，编辑之后的内容要看效果需要点击recompile，在一些简单的编辑器里面，可以在命令行输入pdflatex &lt;your document&gt;之后就可以看到编辑之后的效果； 绪论preamble\\begin{document}之前的部分都是绪论，定义了文章类型、文章使用的语言等；首先看一下一个常见的绪论： 12\\documentclass[12pt, letterpaper]{article}\\usepackage[utf8]{inputenc} \\documentclass[12pt, letterpaper]{article}定义了字体大小12pt，还有一些尺寸(9pt, 11pt, 12pt)，默认为10pt;定义了纸张尺寸letterpaper，还有尺寸a4paper 和 legalpaper如果还有疑问访问page size and margins \\usepackage[utf8]{inputenc}定义编码 添加作者、标题、日期需要在正文中添加\\maketitle 1234567891011121314\\documentclass[12pt, letterpaper, twoside]{article}\\usepackage[utf8]{inputenc}\\title{First document}\\author{Hubert Farnsworth \\thanks{funded by the Overleaf team}}\\date{February 2017}\\begin{document}\\maketitleWe have now added a title, author and date to our first \\LaTeX{} document!\\end{document} 添加注释在注释前添加% 黑体、斜体、下划线 黑体：\\textbf{…} 斜体：\\textit{…} 下划线：\\underline{…} \\emph{…}：在正常字体中斜体，在斜体中就是正常字体 加载图片123456789101112131415\\documentclass{article}% 在文章中加载图片需要添加的包\\usepackage{graphicx}% 指示图片所在的文件夹\\graphicspath{ {images/} }\\begin{document}The universe is immense and it seems to be homogeneous, in a large scale, everywhere we look at.% 真正将图片添加在文章中的地方\\includegraphics{universe}There's a picture of a galaxy above\\end{document} 图片题目、说明和引用12345678910111213\\begin{figure}[h] \\centering \\includegraphics[width=0.25\\textwidth]{mesh} % 图片名称 \\caption{a nice plot} % 给图片标签 \\label{fig:mesh1}\\end{figure}% \\ref引用As you can see in the figure \\ref{fig:mesh1}, the function grows near 0. Also, in the page \\pageref{fig:mesh1} is the same example. 创建列表无序列表1234\\begin{itemize} \\item The individual entries are indicated with a black dot, a so-called bullet. \\item The text in the entries may be of any length.\\end{itemize} 有序列表1234\\begin{enumerate} \\item This is the first entry in our list \\item The list numbers increase with each entry we add\\end{enumerate} 数学公式需要添加\\usepackage{amsmath} 内联公式1\\( ... \\), $ ... $ or \\begin{math} ... \\end{math} 行内公式1\\[ ... \\], \\begin{displaymath} ... \\end{displaymath} or \\begin{equation} ... \\end{equation} 文章格式摘要1234567\\begin{document}\\begin{abstract}This is a simple paragraph at the beginning of the document. A brief introduction about the main subject.\\end{abstract}\\end{document} 开始一个新段落按两次换行 换行\\\\, \\newline chapter、section次序：part和chapter只可以在report和book中 1234567-1 \\part{part}0 \\chapter{chapter}1 \\section{section}2 \\subsection{subsection}3 \\subsubsection{subsubsection}4 \\paragraph{paragraph}5 \\subparagraph{subparagraph} 表格1234567\\begin{center}\\begin{tabular}{ c c c } cell1 &amp; cell2 &amp; cell3 \\\\ cell4 &amp; cell5 &amp; cell6 \\\\ cell7 &amp; cell8 &amp; cell9 \\end{tabular}\\end{center} \\begin{tabular}{ c c c }里面的{c c c}表示有三列，c表示居中，还有r和l 以下是一个添加分割线的例子 1234567891011121314151617\\begin{center} \\begin{tabular}{||c c c c||} \\hline Col1 &amp; Col2 &amp; Col2 &amp; Col3 \\\\ [0.5ex] \\hline\\hline 1 &amp; 6 &amp; 87837 &amp; 787 \\\\ \\hline 2 &amp; 7 &amp; 78 &amp; 5415 \\\\ \\hline 3 &amp; 545 &amp; 778 &amp; 7507 \\\\ \\hline 4 &amp; 545 &amp; 18744 &amp; 7560 \\\\ \\hline 5 &amp; 88 &amp; 788 &amp; 6344 \\\\ [1ex] \\hline\\end{tabular}\\end{center} 添加表标题123456789101112131415161718Table \\ref{table:data} is an example of referenced \\LaTeX{} elements.\\begin{table}[h!]\\centering\\begin{tabular}{||c c c c||} \\hline Col1 &amp; Col2 &amp; Col2 &amp; Col3 \\\\ [0.5ex] \\hline\\hline 1 &amp; 6 &amp; 87837 &amp; 787 \\\\ 2 &amp; 7 &amp; 78 &amp; 5415 \\\\ 3 &amp; 545 &amp; 778 &amp; 7507 \\\\ 4 &amp; 545 &amp; 18744 &amp; 7560 \\\\ 5 &amp; 88 &amp; 788 &amp; 6344 \\\\ [1ex] \\hline\\end{tabular}\\caption{Table to test captions and labels}\\label{table:data}\\end{table} 目录 section chapter自动识别目录，但是section*需要添加\\addcontentsline{toc}{section}{Unnumbered Section}123456789101112131415161718192021222324252627282930313233343536\\documentclass{article}\\usepackage[utf8]{inputenc} \\title{Sections and Chapters}\\author{Gubert Farnsworth}\\date{ } \\begin{document} \\maketitle \\tableofcontents\\section{Introduction} This is the first section. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Etiam lobortisfacilisis sem. Nullam nec mi et neque pharetra sollicitudin. Praesent imperdietmi nec ante. Donec ullamcorper, felis non sodales... \\section*{Unnumbered Section}\\addcontentsline{toc}{section}{Unnumbered Section}Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Etiam lobortis facilisissem. Nullam nec mi et neque pharetra sollicitudin. Praesent imperdiet mi necante...\\section{Second Section} Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Etiam lobortis facilisissem. Nullam nec mi et neque pharetra sollicitudin. Praesent imperdiet mi necante... \\end{document}","link":"/blog/2022/09/03/%E5%8D%9C%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-latex%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86/"},{"title":"自然语言处理学习笔记-lecture1-绪论","text":"基本概念 语言是个体之间由于沟通需要而制定的指令。 自然语言:人类之间用于沟通交流的语言。 自然语言的特点 线性:自然语言呈现为一种线性的符号序列。 层次性:自然语言内部存在层次结构。 歧义性:同一个自然语言句子存在多种不同的理解。 演化性:自然语言随着时代不断演化。 典型任务中文分词 输入:一段不带空格的汉语文本。 输出:以空格隔开词语的汉语文本。 示例程序：示例程序 词性标注 输入:给定一个词语的序列。 输出:输出一个对应的词性的序列。 示例程序：示例程序 文本分类 输入:一段文本 输出:该文本的类别。 示例程序：示例程序 语言模型 输入:给定一个词语序列 输出:预测下一个词 示例程序：示例程序1 示例程序2 语法改错 输入:一段可能包含语法错误的文本。 输出:识别出文本中的语法错误并进行修改。 示例程序：示例程序 句法分析 输入:一个自然语言句子 输出:句子的句法结构(短语结构或依存结构) 示例程序：示例程序1 示例程序2 拼音输入法 输入:拼音符号的序列 输出:汉字序列 示例程序：示例程序 情感分析 输入:一段自然语言文本。 输出:情感的类别(如正面、中性、负面) 示例程序：添加链接描述 语义角色标注 输入:一个自然语言句子。 输出:标出句子的谓语及相关语义角色。 示例程序：示例程序 语义分析 输入:一个自然语言处理句子 输出:该句子的语义表示形式 示例程序：示例程序 指代消解 输入:一段自然语言文本 输出:该文本中代词所指向的名词 示例程序：示例程序 机器翻译 输入:一段源语言文本 输出:一段目标语言文本 示例程序：示例程序 文本摘要 输入:一段自然语言长文本。 输出:一段能概括长文本核心意思的短文本。 示例程序：示例程序 对联生成 输入:对联的上联 输出:对联的下联以及横批 示例程序：示例程序 诗词生成 输入:诗词的关键词 输出:五绝、七绝、律诗或者词 示例程序：示例程序 问答系统 输入:一个自然语言问题。 输出:该问题的答案。 示例程序：示例程序 对话系统 输入:一个自然语言句子 输出:另一个自然语言句子作为回复 示例程序：示例程序 图像标题生成 输入:一张图像 输出:一个自然语言句子，对该图像的内容进行描述。 示例程序：示例程序 发展历史 1943：Warren McCulloch与Walter Pitts提出神经网络。 1949：Warren Weaver提出利用计算机自动翻译人类语言。 1950：Alan Turing提出“图灵测试”检验机器是否具备智能。 1955：Noam Chomsky提出形式语言体系，用数学描述语言。 1957：Frank Rosenblatt提出了感知机，推动了神经网络的发展。 1964：Joseph Weizenbaum研制聊天机器人ELIZA。 1965：Edward Feigenbaum提出专家系统DENDRAL。 1966：Leonard Baum和Lloyd Welch提出隐马尔科夫模型。 1970：CYK算法被提出并广泛用于上下文无关语言的分析。 1974：Paul Werbos为神经网络提出后向传播算法。 1984：Douglas Lenat提出了常识知识库Cyc。 1989：IBM公司提出著名的统计机器翻译IBM模型。 1993：宾夕法尼亚大学推出宾州树库，对句法分析研究起到极大推动作用。 1995：Vladimir Vapnik提出了支持向量机。 1996：Adwait Ratnaparkhi将最大熵模型引入自然语言处理 2001：Tim Berners-Lee提出语义网。 2003：Yoshua Bengio将分布式表示用于语言模型。 2006：Geoffrey Hinton引领了深度学习的兴起。 2011：IBM公司研制的“沃森”系统在知识问答任务中获胜。 2012：Google公司推出了知识图谱并在搜索引擎中使用。 2013：Google公司推出word2vec模型。 2014：Yoshua Bengio将注意力机制引入自然语言处理。 2017：Google公司提出Transformer模型。 2018：Google公司提出BERT预训练语言模型。小结 理性主义方法和经验主义方法交相辉映，齐头并进 – 理性主义:形式文法、专家系统、知识图谱 – 经验主义:隐马可夫模型、最大熵模型、神经网络 当前挑战 – 模型:过于依赖人工设计。 – 数据:标注数据严重不足。 – 训练:训练成本过于高昂。 – 推断:难以保证可靠可信。 相关资源学术机构 Association for Computational Linguistics – 创建时间:1962年– 机构网站:https://www.aclweb.org/– 自然语言处理领域影响力最大的国际学术机构。 中国中文信息学会 – 创建时间:1981年– 期刊网站:http://www.cipsc.org.cn/– 自然语言处理领域影响力最大的国内学术机构。 学术期刊 Computational Linguistics – 创建时间:1974年– 期刊网站:https://www.mitpressjournals.org/loi/coli– 自然语言处理领域传统上最好的国际期刊。 Transactions of the Association for Computaional Linguistcs – 创建时间：2013年– 期刊网站:https://transacl.org/index.php/tacl– 自然语言处理领域广受好评的顶级国际期刊。 学术会议 ACL:影响力最大，截稿时间一般在1月或2月。 EMNLP:偏重经验主义方法，截稿时间一般在5月。 NAACL:面向北美地区，截稿时间一般在11月。 AACL:面向亚太地区，截稿时间一般在6月。 COLING:传统的三大会议之一，截稿时间一般在5月。 IJCAI:人工智能会议，截稿时间一般在1月。 AAAI:人工智能会议，截稿时间一般在9月。 ICLR:机器学习会议，截稿时间一般在9月。 NeurIPS:机器学习会议，截稿时间一般在6月。","link":"/blog/2022/09/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture1-%E7%BB%AA%E8%AE%BA/"},{"title":"模式识别学习笔记-lecture2-统计判别1","text":"作为统计判别问题的模式分类 模式识别的目的就是要确定某一个给定的模式样本属于哪一类 输入：被识别对象的特征向量 输出：被识别样本的类别贝叶斯判别原则两类模式集的分类 目的：要确定$x$是属于$\\omega_1$类还是$\\omega_2$类，要看$x$是来自于$\\omega_1$类的概率大还是来自$\\omega_2$类的概率大。 根据概率判别规则，有：若$P(\\omega_1|x) \\gt P(\\omega_2|x)$，则$x \\in \\omega_1$若$P(\\omega_1|x) \\lt P(\\omega_2|x)$，则$x \\in \\omega_2$由贝叶斯定理，后验概率$P(\\omega_i|x)$可由类别$\\omega_i$的先验概率$P(\\omega_i)$和$x$的条件概率密度$P(x|\\omega_i)$来计算，即：$$P(\\omega_i|x)=\\frac{P(x|\\omega_i)P(\\omega_i)}{P(x)}=\\frac{P(x|\\omega_i)P(\\omega_i)}{\\sum_{i=1}^2{P(x|\\omega_i)P(\\omega_i)}}$$这里$P(x|\\omega_i)$也称为似然函数，将该式代入上述判别式，有：若$P(x|\\omega_1)P(\\omega_1) \\gt P(x|\\omega_2)P(\\omega_2)$，则$x \\in \\omega_1$若$P(x|\\omega_1)P(\\omega_1) \\lt P(x|\\omega_2)P(\\omega_2)$，则$x \\in \\omega_2$或若$l_{12}(x)=\\frac{P(x|\\omega_1)}{P(x|\\omega_2)} \\gt \\frac{P(\\omega_2)}{P(\\omega_1)}$，则$x \\in \\omega_1$若$l_{12}(x)=\\frac{P(x|\\omega_1)}{P(x|\\omega_2)} \\lt \\frac{P(\\omega_2)}{P(\\omega_1)}$，则$x \\in \\omega_2$其中，$l_{12}$称为似然比，$\\frac{P(\\omega_2)}{P(\\omega_1)}$称为似然比的判决阈值，此判别称为贝叶斯判别。 例题：假设某地发生地震事件$\\omega_1$的概率为0.2，$P(\\omega_1)=0.2$，则不发生地震的概率$\\omega_2$为0.8，$P(\\omega_2)=0.8$，已知地震通常与生物异常反应之间有一定的联系，生物是否发生异常这一结果以模式$x$表示，有两种取值，包括异常和正常，假设地震前一周内发生生物异常的概率是0.6，$P(x=异常|\\omega_1)=0.6$，地震前一周生物正常的概率是0.4，$P(x=正常|\\omega_1)=0.4$，一周之内没有发生地震但是生物异常的概率是0.1，，$P(x=异常|\\omega_2)=0.1$，一周之内没有发生地震生物正常的概率是0.9，$P(x=正常|\\omega_2)=0.9$求解：发生生物异常情况下一周之内发生地震的概率$$\\begin{aligned}P(\\omega_1|x=异常)&amp;= \\frac{P(x=异常|\\omega_1)P(\\omega_1)}{\\sum_{i=1}^2P(x=异常|\\omega_i)P(\\omega_i)} \\&amp;= \\frac{P(x=异常|\\omega_1)P(\\omega_1)}{P(x=异常|\\omega_1)P(\\omega_1) + P(x=异常|\\omega_2)P(\\omega_2) }\\&amp;= \\frac{0.6 \\times 0.2}{0.6 \\times 0.2 + 0.1 \\times 0.8} \\&amp;= 0.6\\end{aligned} \\似然比：l_{12} = \\frac{P(x=异常|\\omega_1)}{P(x=异常|\\omega_2)}=\\frac{0.6}{0.1}=6 \\判决阈值：\\theta_{21}=\\frac{P(\\omega_2)}{P(\\omega_1)}=\\frac{0.8}{0.2}=4$$ 贝叶斯最小风险判别 目的：考虑到某些类别的错误判断比另外一些类的错误判断风险更大，需要对贝叶斯判别做一些修正。 $M$类分类问题的条件平均风险$r_j(x)=\\sum_{i=1}^ML_{ij}P(\\omega_i|x)$ $L_{ij}$称为本应属于$\\omega_i$类的模式判别成属于$\\omega_j$类的是非代价，若$i=j$即判别正确，得分，$L_{ij}$可以取负值或零，表示不失分，若$i\\neq j$即判别错误，应该取正值。 分类器对每一个模式$x$有$M$种可能的类别可供选择，将$x$指定为具有最小风险值的那一类，则这种分类器称为最小平均条件风险分类器 按照贝叶斯公式，平均条件风险可写成：$r_j(x)=\\frac{1}{p(x)}\\sum_{i=1}^{M}L_{ij}P(x|\\omega_i)P(\\omega_i)$，舍去$\\frac{1}{P(x)}$这个公共项简化为$r_j(x)=\\sum_{i=1}^{M}L_{ij}P(x|\\omega_i)P(\\omega_i)$，这也是贝叶斯分类器，但是这个不是按错误概率最小作为标准，而是按平均条件风险作为标准。两类的情况$M=2$即全部的模式样本只有$\\omega_1$和$\\omega_2$两类，则平均风险可以写成：当分类器将$x$判别为$\\omega_1$时：$$r_1(x)=L_{11}P(x|\\omega_1)P(\\omega_1)+L_{21}P(x|\\omega_2)P(\\omega_2)$$当分类器将$x$判别为$\\omega_2$时：$$r_2(x)=L_{12}P(x|\\omega_1)P(\\omega_1)+L_{22}P(x|\\omega_2)P(\\omega_2)$$若$r_1(x)\\lt r_2(x)$，则$x$被判定为属于$\\omega_1$，此时：$$L_{11}P(x|\\omega_1)P(\\omega_1)+L_{21}P(x|\\omega_2)P(\\omega_2) \\lt L_{12}P(x|\\omega_1)P(\\omega_1)+L_{22}P(x|\\omega_2)P(\\omega_2)$$即：$$(L_{12}-L_{11})P(x|\\omega_1)P(\\omega_1) \\gt (L_{21}-L_{22})P(x|\\omega_2)P(\\omega_2)$$当$\\frac{P(x|\\omega_1)}{P(x|\\omega_2)} \\gt \\frac{P(\\omega_2)}{P(\\omega_1)} . \\frac{L_{21}-L_{22}}{L_{12}-L_{11}}$该式左边为似然比：$l_{12}=\\frac{P(x|\\omega_1)}{P(x|\\omega_2)}$右边为阈值：$\\theta_21=\\frac{P(\\omega_2)}{P(\\omega_1)} . \\frac{L_{21}-L_{22}}{L_{12}-L_{11}}$ 若$l_{12}(x) \\gt \\theta_{21}$，则$x \\in \\omega_1$ 若$l_{12}(x) \\lt \\theta_{21}$，则$x \\in \\omega_2$ 通常，当判别正确时，不失分，可选常数$L_{11}=L_{22}=0$；判别错误时，可选$L_{12}=L_{21}=1$，此时$\\theta_{21}=\\frac{P(\\omega_2)}{P(\\omega_1)}$ 例：一信号通过一受噪声干扰的信道，信道输入信号为0或1，噪声为高斯型，其均值为$\\mu=0$，方差为$\\sigma^2$，信道输出为$x$，试求最优的判别规则，从观察值$x$的基础上判别它是0还是1，直观上可以看出，若$x \\lt 0.5$应该判为0，$x \\gt 0.5$应该判为1。用贝叶斯判别条件分析，设信号送0的先验概率为$P(0)$，送1的先验概率为$P(1)$。当输入信号为0时，受噪声为正态分布$N(0,\\sigma^2)$的干扰，其幅值大小的概率密度为：$$P(x|\\omega_1)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{x^2}{2\\sigma^2}}$$当输入信号为0时，其幅值大小的概率密度为：$$P(x|\\omega_2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-1)^2}{2\\sigma^2}}$$则似然比为：$l_{12}=\\frac{P(x|\\omega_1)}{P(x|\\omega_2)} = e^{\\frac{1-2x}{2\\sigma^2}}$若$l_{12} \\gt \\theta_{21}$，即$e^{\\frac{1-2x}{2\\sigma^2}} \\gt \\theta_{21}$，则$x \\lt \\frac{1}{2} - \\sigma^2 ln\\theta_{21}$，则$x \\in \\omega_1$，此时信号是0，即：$$x \\lt \\frac{1}{2}-\\sigma^2ln\\left(\\frac{L_{21}}{L_{12}}.\\frac{P(1)}{P(0)}\\right)$$若取$L_{21}=L_{21}=1, P(1)=P(0)$，则$x \\lt \\frac{1}{2}$判为0若无噪声干扰，即$\\sigma^2=0$，则$x \\lt \\frac{1}{2}$判为0一般多类的情况对于$M$类情况，若$r_i(x) \\lt r_j(x), j = 1,2,\\cdots,M,j\\neq i$，则$x \\in \\omega_1$$L$可如下取值，判对失分为0，判错失分为1记：$$L_{ij}=\\begin{cases}0 &amp; when\\ i = j \\1 &amp; when\\ i \\neq j\\end{cases}$$则条件平均风险可写成：$$\\begin{aligned}r_j(x)&amp;= \\sum_{i=1}^ML_{ij}P(x|\\omega_i)P(\\omega_i) \\&amp;= L_{1j}P(x|\\omega_1)P(\\omega_1) + \\cdots + L_{jj}P(x|\\omega_j)P(\\omega_j) + \\cdots + L_{Mj}P(x|\\omega_M)P(\\omega_M) \\&amp;= \\sum_{i=1}^MP(x|\\omega_i)P(\\omega_i) - P(x|\\omega_j)P(\\omega_j) \\&amp;= P(x) - P(x|\\omega_j)P(\\omega_j)\\end{aligned}$$由$r_i(x) \\lt r_j(x)$，有当$P(x|\\omega_i)P(\\omega_i) \\gt P(x|\\omega_j)P(\\omega_j)$时，$x \\in \\omega_i$，对应于判别函数为：取$d_i(x)=P(x|\\omega_i)P(\\omega_i),i=1,2,\\cdots,M$，则对于全部$j \\neq i$的值，若$d_i(x) \\gt d_j(x)$，则$x \\in \\omega_i$","link":"/blog/2022/09/05/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture2-%E7%BB%9F%E8%AE%A1%E5%88%A4%E5%88%AB1/"},{"title":"模式识别学习笔记-lecture2-统计判别2","text":"正态分布模式的贝叶斯分类器当已知或有理由设想类概率密度函数$P(x|\\omega_i)$是多变量的正态分布时，贝叶斯分类器可以导出一些简单的判别函数 $M$种模式类别的多变量正态类密度函数具有$M$种模式类别的多变量正态类密度函数为：$$P(x|\\omega_i)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|C_i|^{\\frac{1}{2}}}exp\\left{-\\frac{1}{2}(x - m_i)^TC_i^{-1}(x - m_i)\\right}\\ i = 1,2,\\cdots,M$$其中每一类模式的分布密度都完全被其均值向量m_i和协方差矩阵$C_i$所规定，其定义为：$$\\begin{aligned}m_i &amp;= E_i{x} \\C_i &amp;= E_i{(x - m_i)(x - m_i)^T}\\end{aligned}$$$E_i{x}$表示对类别属于$\\omega_i$的模型的数学期望。在上述公式中，$n$为模式向量的维数，$|C_i|$为矩阵$C_i$的行列式，协方差矩阵$C_i$是对称的正定矩阵，其对角线上的元素$C_{kk}$是模式向量第$k$个元素的方差，非对角线上的元素$C_{jk}$是$x$的第$j$个分量$x_j$和第$k$个分量$x_k$的协方差。当$x_j$和$x_k$统计独立时，$C_{jk}=0$。当协方差矩阵的全部非对角线上的元素都为0时，多变量正态类密度函数可简化为$n$个单变量正态类密度函数的乘积。已知类别$\\omega_i$的判别函数可写成如下形式：$$d_i(x)=P(x|\\omega_i)P(\\omega_i), \\ i=1,2,\\cdots,M$$对于正态密度函数，可取自然对数的形式以方便计算(因为自然对数是单调递增的，取对数后不影响相应的分类性能)，则有：$$d_i(x)=ln[P(x|\\omega_i)] + lnP(\\omega_i), \\ i=1,2,\\cdots,M$$代入正态类密度函数，有：$$d_i(x) = lnP(\\omega_i)-\\frac{n}{2}ln(2\\pi)-\\frac{1}{2}ln|C_i|-\\frac{1}{2}(x - m_i)^TC_i^{-1}(x - m_i),\\ i=1,2,\\cdots,M$$去掉和$i$无关的项(并不影响分类结果)，有：$$d_i(x)=lnP(\\omega_i)-\\frac{1}{2}ln|C_i|-\\frac{1}{2}(x - m_i)^TC_i^{-1}(x - m_i),\\ i=1,2,\\cdots,M$$即为正态分布模式的贝叶斯判别函数，判别函数是一个超二次曲面 两类问题且其类模式都是正态分布的特殊情况当$C_1 \\neq C_2$时，两类模式的正态分布为：$P(x|\\omega_1)$表示为$N(m_1,C_1)$，$P(x|\\omega_2)$表示为$N(m_2,C_2)$，$\\omega_1, \\omega_2$两类的判别函数对应为：$$d_1(x) = lnP(\\omega_1)-\\frac{1}{2}ln|C_1|-\\frac{1}{2}(x - m_1)^TC_1^{-1}(x - m_1) \\d_2(x) = lnP(\\omega_2)-\\frac{1}{2}ln|C_2|-\\frac{1}{2}(x - m_2)^TC_2^{-1}(x - m_2) \\d_1(x)-d_2(x)=\\begin{cases}\\gt 0 &amp; x \\in \\omega_1 \\\\lt 0 &amp; x \\in \\omega_1\\end{cases}$$判别界面是$x$的二次型方程，当$x$是二维模式时，判别界面为二次曲线，如椭圆、圆、抛物线或双曲线等当$C_1 = C_2 = C$时，有：$$d_i(x)=lnP(\\omega_i)-\\frac{1}{2}ln|C|-\\frac{1}{2}x^TC^{-1}x+\\frac{1}{2}x^TC^{-1}m_i + \\frac{1}{2}m_i^TC^{-1}x - \\frac{1}{2}m_i^TC^{-1}m_i,\\ i = 1,2$$因$C$为对称矩阵，上式可简化为：$$d_i(x)=lnP(\\omega_i)-\\frac{1}{2}ln|C|-\\frac{1}{2}x^TC^{-1}x + m_i^TC^{-1}x - \\frac{1}{2}m_i^TC^{-1}m_i,\\ i = 1,2$$由此可导出类别$\\omega_1$和$\\omega_2$间的判别界面为：$$d_1(x)-d_2(x) = lnP(\\omega_1)-lnP(\\omega_2)+(m_1-m_2)^TC^{-1}x- \\frac{1}{2}m_1^TC^{-1}m_1 + \\frac{1}{2}m_2^TC^{-1}m_2 = 0$$判别界面是$x$的线性函数，为一超平面，当$x$是二维时，判别界面为一直线 例题$P(\\omega_1) = P(\\omega_2) = \\frac{1}{2}$，求其判别界面模式的均值向量$m_i$和协方差矩阵$C_i$可用下式估计：$$\\begin{aligned}m_1 &amp;= \\frac{1}{N_i}\\sum_{j = 1}^{N_i}x_{ij} \\ i = 1,2 \\C_i &amp;= \\frac{1}{N_i}\\sum_{j = 1}^{N_i}(x_{ij}-m_i)(x_{ij}-m_i)^T \\ i = 1,2\\end{aligned}$$其中$N_i$为类别$\\omega_i$中模式的数目，$x_{ij}$代表在第$i$个类别中的第$j$个模式，由上式可求出：$$m_1 = \\frac{1}{4}(3\\ 1\\ 1)T \\m_2 = \\frac{1}{4}(1\\ 3\\ 3)^T \\C_1 = C_2 = C = \\frac{1}{16}\\left(\\begin{matrix}3 &amp; 1 &amp; 1 \\1 &amp; 3 &amp; -1 \\1 &amp; -1 &amp; 3\\end{matrix}\\right),\\C^{-1} = 4\\left(\\begin{matrix}2 &amp; -1 &amp; -1 \\-1 &amp; 2 &amp; 1 \\-1 &amp; 1 &amp; 2\\end{matrix}\\right)$$设$P(\\omega_1)=P(\\omega_2)=\\frac{1}{2}$，因$C_1=C_2$，则判别界面为：$$\\begin{aligned}d_1(x)-d_2(x)&amp;=(m_1-m_2)^TC^{-1}x-\\frac{1}{2}m_1^TC^{-1}m_1+\\frac{1}{2}m_2^TC^{-1}m_2 \\&amp;= 8x_1-8x_2-8x_3 + 4 = 0\\end{aligned}$$ 均值向量和协方差矩阵的参数估计在贝叶斯分类器中，构造分类器需要知道类概率密度函数$P(x|\\omega_i)$，如果按照先验知识已经知道其分布，则只需要知道分布的参数即可 将参数作为非随机变量均值和协方差矩阵的估计量定义设模式的类概率密度函数为$p(x)$，则其均值向量定义为：$$m = E(x) = \\int_xxP(x)dx$$其中，$x = (x_1,x_2,\\cdots,x_n)^T,m = (m_1,m_2,\\cdots,m_n)^T$。若以样本的平均值作为均值向量的近似值，则均值估计量$\\hat{m}$为：$$\\hat{m} = \\frac{1}{N}\\sum_{j=1}^Nx_j$$其中$N$为样本的数目。协方差矩阵为：$$C = \\left(\\begin{matrix}c_{11} &amp; c_{12} &amp; \\cdots &amp; c_{1n} \\c_{21} &amp; c_{22} &amp; \\cdots &amp; c_{2n} \\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\c_{n1} &amp; c_{n2} &amp; \\cdots &amp; c_{nn}\\end{matrix}\\right)$$其每个元素$c_{lk}$定义为：$$\\begin{aligned}c_{lk} &amp;= E{(x_l-m_l)(x_k-m_k)} \\&amp;= \\int^{\\infty}{-\\infty}\\int^{\\infty}{-\\infty}(x_l-m_l)(x_k-m_k)P(x_l，x_k)dx_ldx_k\\end{aligned}$$其中，$x_l,x_k,m_l,m_k$分别是$x,m$的第$l,k$个分量。协方差矩阵写成向量形式为：$$C = E{(x - m)(x - m)^T} = E{xx^T}-mm^T$$协方差矩阵的估计量(当$N \\gg 1$为：$$\\hat{C} \\approx \\frac{1}{N}\\sum^N_{k=1}(x_k-\\hat{m})(x_k-\\hat{m})^T$$这里样本模式总体为${x_1,x_2,\\cdots,x_k,\\cdots,x_N}$，为因为计算估计量时没有真实的均值向量$m$可用，只能用均值向量的估计量来代替，会存在偏差。均值和协方差矩阵估计量的迭代运算形式假设已经计算了$N$个样本的均值估计量，若再加上一个样本，其新的估计量$\\hat{m}(N+1)$为：$$\\hat{m}(N+1) = \\frac{1}{N+1}\\sum_{j=1}^{N+1}x_j = \\frac{1}{N+1}\\left[\\sum_{j=1}^Nx_j+x_{N+1}\\right] = \\frac{1}{N+1}\\left[N\\hat{m}(N) + x_{N+1}\\right]$$其中$\\hat{m}(N)$为从$N$个样本计算得到的估计量，迭代的第一步应取$\\hat{m}(1)=x_1$。协方差矩阵的估计量的迭代运算与上述相似，取$\\hat{C}(N)$表示$N$个样本时的估计量为：$$\\hat{C}(N) = \\frac{1}{N}\\sum_{j=1}^Nx_jx_j^T - \\hat{m}(N)\\hat{m}^T(N)$$加入一个样本，则：$$\\begin{aligned}\\hat{C}(N+1)&amp;= \\frac{1}{N+1}\\sum_{j=1}^{N+1}x_jx_j^T - \\hat{m}(N+1)\\hat{m}^T(N+1) \\&amp;= \\frac{1}{N+1}\\left[\\sum_{j=1}^{N}x_jx_j^T + x_{N+1}x_{N+1}^T\\right] - \\hat{m}(N+1)\\hat{m}^T(N+1) \\&amp;= \\frac{1}{N+1}\\left[N\\hat{C}(N)+N\\hat{m}(N)\\hat{m}^T(N) + x_{N+1}x_{N+1}^T\\right] - \\frac{1}{(N+1)^2}\\left[N\\hat{m}(N) + x_{N+1}\\right]\\left[N\\hat{m}(N) + x_{N+1}\\right]^T\\end{aligned}$$其中$\\hat{C}(1) = x_1x_1^T - \\hat{m}(1)\\hat{m}^T(1)=0$是零矩阵 将参数看做随机变量设${x_1,x_2,\\cdots,x_N}$为$N$个用于估计一未知参数$\\theta$的密度函数的样本，$x_i$被一个接一个的逐次给出，于是用贝叶斯定理，可以得到在给定了$x_1,x_2,\\cdots,x_N$之后，$\\theta$的后延概率密度的迭代表示式为：$$P(\\theta|x_1,\\cdots,x_N)=\\frac{P(x_N|\\theta,x_1,\\cdots,x_{N-1})P(\\theta|x_1,\\cdots,x_{N-1})}{P(x_N|x_1,\\cdots,x_{N-1})}$$其中对于$P(\\theta|x_1,\\cdots,x_N)$而言，$P(\\theta|x_1,\\cdots,x_{N-1})$是它的先验概率，当加入了新的样本$x_N$后，得到修正之后的新的概率密度$P(\\theta|x_1,\\cdots,x_N)$。如此一步步向前推，则$P(\\theta)$是最初的先验概率密度，当读入第一个样本$x_1$时，经过贝叶斯定理计算，可得到后验概率密度$P(\\theta|x_1)$。以此为新的一步，将$P(\\theta|x_1)$作为第二部计算的先验概率密度，读入样本$x_2$，又得到第二步的后验概率密度$P(\\theta|x_1,x_2)$，……，以此可以算出最终的后延概率密度$P(\\theta|x_1,\\cdots,x_N)$，从而得到最终的结果。这里需要知道最初始的概率密度$P(\\theta)$和全概率$P(x_N|x_1,\\cdots,x_{N-1})$，全概率可以通过下式算出：$$P(x_N|x_1,\\cdots,x_{N-1}) = \\int_xP(x_N|\\theta,x_1,\\cdots,x_{N-1})P(\\theta|x_1,\\cdots,x_{N-1})d\\theta$$这一个值和未知量$\\theta$无关，可以认为是一个定值。","link":"/blog/2022/09/10/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture2-%E7%BB%9F%E8%AE%A1%E5%88%A4%E5%88%AB2/"},{"title":"离散数学学习笔记-02-命题逻辑","text":"命题proposition非真既假的普通陈述句，真值true/false唯一确定，（本命题是假的）和（本命题是真的）不是命题 命题变元或命题变项proposition variables小写英文字母表示 原子命题：atom proposition/简单命题 simple proposition 复合命题：compound proposition 命题联结词proposition connective或命题运算符proposition operator 否定词：$\\sim p，非p$ 合取词：$conjunction,p \\bigwedge q,p且q$ 析取词：$disjunction,p \\bigvee q, p或q$ 异或词：$exclusive\\ or,p \\bigoplus q,p和q真值相同时为假$ 蕴涵词：$implication，p \\Rightarrow q，p真q假时p \\Rightarrow q为假$$p$：前提(premise)，$q$：结论(conclusion)$p$是$q$的充分条件，$q$是$p$的必要条件对于命题$p \\Rightarrow q$，$q \\Rightarrow p$为其逆命题，$-p \\Rightarrow -q$为其否命题，$-q \\Rightarrow -p$为其逆否命题 等价词：$equivalence，p \\Leftrightarrow q，p和q真值相同时为真，双条件，p是q的充分必要条件，p和q等价$ 命题公式及其分类命题公式(well formed formula, wff)合式公式，简称公式 单个命题变项变项$p,q,r,\\cdots$是命题公式 如果$A$是命题公式，那么$\\sim A$也是命题公式 如果$A$和$B$是命题公式，那么由逻辑联结词连接的符号串也是命题公式 有限次应用上面三条构成的符号串才是命题公式，只有用命题公式表示的符号串才是命题，且该公式的每一命题变项真值确定 ${\\sim} \\gt {\\bigwedge,\\bigvee} \\gt {\\Rightarrow,\\Leftrightarrow}$ n元命题公式n元命题公式：含有n个命题变项的命题公式$(p_1,p_2,p_3,\\cdots,p_n)$ 对变项组$(p_1,p_2,p_3,\\cdots,p_n)$指定的一组确定真值称为该公式的一个真值指派或赋值，若使之真值为真，则称这组值为成真指派或成真赋值，若使之真值为假，则称这组值为成假指派或成假赋值 真值表truth table：$2^n$行对应$2^n$个真值指派 若所有$2^n$个赋值都是成真赋值，则称A为永真式或重言式tautology 若所有$2^n$个赋值都是成假赋值，则称A为永假式或矛盾式contradiction 若至少存在一个成真指派，则称A为可满足式，satisfiable formula 若至少存在一个成真指派和一个成假指派，则称A为非重言的可满足式 两个重言式或两个矛盾式的析取或合取任然是重言式、矛盾式 结果可能为真可能为假为不定式contingency 如果一个命题公式是重言式，则一定是不定式 命题逻辑的等值演算等值equivalent，逻辑等值logically equivalent$A \\Leftrightarrow B$是一个重言式，记作$A \\equiv B$，称$A \\equiv B$为等值式，表示对于任意的真值指派，A、B的真值均相同证明两公式等值：真值表法、等值演算法证明两公式不等值：找出一个真值指派使一个真值为真一个为假 基本等值式假设$p,q,r$为任意命题 双重否定率：$p \\equiv \\sim(\\sim p)$ 幂等律：$p \\bigvee p \\equiv p,p \\bigwedge p \\equiv p$ 交换律：$p \\bigvee q \\equiv q \\bigvee p, p \\bigwedge q \\equiv q \\bigwedge p$ 结合律：$(p \\bigvee q) \\bigvee r \\equiv p \\bigvee (q \\bigvee r),(p \\bigwedge q) \\bigwedge r \\equiv p \\bigwedge (q \\bigwedge r)$ 分配率：$p \\bigvee (q \\bigwedge r) \\equiv (p \\bigvee q) \\bigwedge (p \\bigvee r),p \\bigwedge (q \\bigvee r) \\equiv (p \\bigwedge q) \\bigvee (p \\bigwedge r)$ 德摩根律：$\\sim(p \\bigvee q) \\equiv (\\sim p) \\bigwedge (\\sim q),\\sim(p \\bigwedge q) \\equiv (\\sim p) \\bigvee (\\sim q)$ 吸收律：$p \\bigvee (p \\bigwedge q) \\equiv p,p \\bigwedge (p \\bigvee q) \\equiv p$ 零律：$p \\bigvee T \\equiv T,p \\bigwedge F \\equiv F$ 同一律：$p \\bigvee F \\equiv p,p \\bigwedge T \\equiv p$ 排中律：$p \\bigvee \\sim p \\equiv T$ 矛盾律：$p \\bigwedge \\sim p \\equiv F$ 蕴含等值式：$p \\Rightarrow q \\equiv \\sim p \\bigvee q$ 等价等值式：$p \\Leftrightarrow q \\equiv (p \\Rightarrow q) \\bigwedge (q \\Rightarrow p)$ 假言易位：$p \\Rightarrow q \\equiv (\\sim q) \\Rightarrow (\\sim p)$ 等价否定等值式：$p \\Leftrightarrow q \\equiv \\sim p \\Leftrightarrow \\sim q$ 归谬论：$(p \\Rightarrow q) \\bigwedge (p \\Rightarrow \\sim q) \\equiv \\sim p$ 代入规则：假设A是一个重言式，那么A里的命题变项每一项替换为别的，不论真值如何，替换以后任然为重言式置换规则：若$A \\equiv B$，则用命题公式B替换命题公式$\\Phi(A)$中的A后形成的$\\Phi(B)$(不一定替换每一处），$\\Phi(A) \\equiv \\Phi(B)$","link":"/blog/2022/09/10/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-02-%E5%91%BD%E9%A2%98%E9%80%BB%E8%BE%91/"},{"title":"离散数学学习笔记-01-基础知识","text":"集合与序列集合的基本概念集合set：大写字母表示S 集合的元素：小写字母表示a，$a \\in S$集合的特点： 能够明确的判断一个元素是或不是属于某集合 集合的元素没有顺序 集合的元素之间不一定存在什么关系 规定：对任意集合A都有$A \\notin A$ 特殊集合：自然数：N，整数：Z，正整数：$Z^+$，非零整数集$Z^*$，有理数集$Q$，非零有理数集$Q^*$，实数$R$，非零实数集$R^*$，复数$C$ 集合的表示： 外延表示法（列举法）${\\cdots}$ 内涵表示法（描述法) ${x|P(x)}$ 子集和超集：$A \\subseteq B$$A = B:A \\subseteq B\\ and\\ B \\subseteq A$全集：$U$空集：$\\emptyset$ 基数、势cardinality：集合中的元素数$|A|, #A, card(A)$幂集（power set）：A的所有子集所组成的集合，$\\oint(A) = {x|x \\subset A}$其中有空集和A本身 证明$X \\subseteq Y$的基本方法是：对于任意的$x \\in X$，有$x \\in Y$证明两个集合相等的方法是分别证明$X \\subseteq Y \\ and\\ Y \\subseteq X$ 集合的运算 交集 $A \\bigcap B, intersection$ 并集 $A \\bigcup B, union$ B关于A的相对补(complement of B with respect to A)或A与B的差集(difference):$A - B=A\\overline{B}$ 补集$\\overline{A},\\sim A, complement$，A关于U的相对补 对称差$A\\bigoplus B = {x|x \\in A\\ or\\ x \\in B\\ and\\ x \\notin A\\bigcap B = (A - B) \\bigcup (B - A), symmetric\\ difference$ 集合运算的性质 交换律：$$A \\bigcup B = B \\bigcup A \\A \\bigcap B = B \\bigcap A \\A \\bigoplus B = B \\bigoplus A$$ 结合律：$$(A \\bigcup B) \\bigcup C = A \\bigcup (B \\bigcup C) \\(A \\bigcap B) \\bigcap C = A \\bigcap (B \\bigcap C) \\(A \\bigoplus B) \\bigoplus C = A \\bigoplus (B \\bigoplus C)$$ 分配率：$$A \\bigcup (B \\bigcap C) = (A \\bigcup B) \\bigcap ( A \\bigcup C) \\A \\bigcap (B \\bigcup C) = (A \\bigcap B) \\bigcup ( A \\bigcap C)$$ 吸收率：$$A \\bigcup (A \\bigcap B) = A \\A \\bigcap ( A \\bigcup B) = A$$ 德摩根律：绝对形式：$$\\overline{A \\bigcup B} = \\overline{A} \\bigcap \\overline{B} \\\\overline{A \\bigcap B} = \\overline{A} \\bigcup \\overline{B}$$相对形式：$$A - (B \\bigcup C) = (A - B) \\bigcap (A - C) \\A - (B \\bigcap C) = (A - B) \\bigcup (A - C)$$ 幂等律：$$A \\bigcap A = A \\A \\bigcup A = A$$ 零律：$$A \\bigcup U = U\\A \\bigcap \\emptyset = \\emptyset$$ 同一律：$$A \\bigcap U = A \\A \\bigcup \\emptyset = A$$ 排中律：$$A \\bigcup \\overline{A} = U$$ 矛盾律：$$A \\bigcap \\overline{A} = \\emptyset$$ 序列的基本概念 sequence：排成一列的对象，有顺序，里面的对象为项item； 对于给定的集合A，$A^*$为所有由A种元素生成的有限长度序列全体，$A^*$中的元素称为A上的词word或串string； 假设$A = {a,b,c,\\cdots,z}$，则$A^*$中包括的为若干单词，$A^*$中的空序列称作空串empty string，记作$\\lambda,\\varepsilon$； 假设A是集合，$w_1 = s_1s_2\\cdots s_n,w_2 = t_1t_2\\cdots t_n$都是$A^*$中的元素，则$w_1,w_2$的连接catenation为$s_1s_2\\cdots s_nt_1t_2\\cdots t_n$记作$w_1 \\circ w_2$ 布尔矩阵布尔矩阵boolean matrix，位矩阵bit matrix $A = [a_{ij}]$是一个$m \\times n$的布尔矩阵，则定义其补complement为$\\overline{A} = [\\overline{a_{ij}}] = [1- a_{ij}]$ $A = [a_{ij}]$和$B = [b_{ij}]$都是$m \\times n$的布尔矩阵$$A \\bigcap B \\A \\bigcup B$$ $A = [a_{ij}]$是$m \\times n$矩阵，$B = [b_{ij}]$是$n \\times r$矩阵，布尔积boolean product，$A \\odot B = C = [c_{ij}]$$$c_{ij} =\\begin{cases}1 &amp; 若存在k，1 \\leq k \\leq n 使得a_{ik} = 1且b_{kj} = 1\\0 &amp; otherwise\\end{cases}$$ 定理：$$A \\bigcup B = C = [c_{ij}] \\ c_{ij} = a_{ij} + b_{ij} - a_{ij}b_{ij} \\A \\bigcap B = D = [d_{ij}] \\ d_{ij} = a_{ij}b_{ij}$$ 交换律：$$A \\bigcap B = B \\bigcap A \\A \\bigcup B = B \\bigcup A$$ 结合律：$$(A \\bigcup B) \\bigcup C = A \\bigcup (B \\bigcup C) \\(A \\bigcap B) \\bigcap C = A \\bigcap (B \\bigcap C) \\(A \\odot B) \\odot C = A \\odot (B \\odot C)$$ 分配率：$$A \\bigcap (B \\bigcup C) = (A \\bigcap B) \\bigcup (A \\bigcap C) \\A \\bigcup (B \\bigcap C) = (A \\bigcup B) \\bigcap (A \\bigcup C) \\(A \\bigcup B)^T = A^T \\bigcup B^T \\(A \\bigcap B)^T = A^T \\bigcap B^T \\(A \\odot B)^T = B^T \\odot A^T \\$$","link":"/blog/2022/09/10/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-01-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"title":"离散数学学习笔记-02-对偶和范式","text":"对偶式dual，假设A为仅含有$\\sim,\\bigvee,\\bigwedge$的命题公式，若将$A$中的$\\bigwedge$换成$\\bigvee$，$\\bigvee$换成$\\bigwedge$，若包含$F$和$T$则相互取代，即得$A^*$，$A$的对偶式。$(A^*)^* = A$ 定理1：设$A$为一个仅含有$\\sim,\\bigvee,\\bigwedge$的命题公式，$p_1,p_2,\\cdots,p_n$为其命题变项，则：$\\sim A(p_1,p_2,\\cdots,p_n) \\equiv A^*(\\sim p_1, \\sim p_2,\\cdots,\\sim p_n)$若$A$为重言式，则$A^*$必为矛盾式 定理2：设$A$和$B$为仅含联结词$\\sim,\\bigvee,\\bigwedge$的命题公式，若$A \\equiv B$，则$A^* \\equiv B^*$ 析取范式与合取范式基础概念 文字：literal，$p,\\sim p$且$p$和$\\sim p$称为互补对 析取式：fundamental disjunction有限个文字的析取组成的公式 合取式：fundamental conjunction有限个文字的合取组成的公式 析取范式：disjunction normal form，$A_1 \\bigvee A_2 \\bigvee \\cdots \\bigvee A_n$，$A_1,A_2,\\cdots,A_n$为合取式，n个合取式的析取称为析取范式 合取范式：conjunction normal form，$A_1 \\bigwedge A_2 \\bigwedge \\cdots \\bigwedge A_n$，$A_1,A_2,\\cdots,A_n$为析取式，n个析取式的合取称为合取范式 定理：任何析取范式的对偶式为合取范式，任何合取范式的对偶式为析取范式，设$B$为$A^*$的析取范式，则$B^*$为$A$的合取范式 求范式的步骤： 将$\\Rightarrow,\\Leftrightarrow$联结词转换为$\\sim,\\bigvee,\\bigwedge$$$p \\Rightarrow q \\equiv \\sim{p} \\bigvee q \\p \\Leftrightarrow q \\equiv (\\sim p \\bigvee q) \\bigwedge (\\sim q \\bigvee p)求合取范式 \\p \\Leftrightarrow q \\equiv (p \\bigwedge q) \\bigvee (\\sim p \\bigwedge \\sim q)求析取范式$$ 简化$\\sim$，使$\\sim$仅作用于命题变式 利用分配率，使其最终变为合取范式或析取范式范式存在定理任一命题公式都存在着与之等值的析取范式和合取范式极小项minterm合取式中的命题变项中，$p$和$\\sim p$只存在一个，$n$个命题变项构成$2^n$个极小项，这些极小项仅在一种情况下为真例：3个命题变项，8个极小项$$\\begin{matrix}\\sim p \\bigwedge \\sim q \\bigwedge \\sim r &amp; 000-0 &amp; m_0 \\\\sim p \\bigwedge \\sim q \\bigwedge r &amp; 001-0 &amp; m_1 \\\\vdots &amp;\\ &amp; \\vdots \\p \\bigwedge q \\bigwedge r &amp; 111-7 &amp; m_7\\end{matrix}$$主析取范式full disjunctive normal form，用$\\sum$表示，由极小项的析取构成的析取范式求主析取范式的步骤： 求$A$的一个析取范式$A’$ 若$A’$的某合取式$B$不含命题变项$p_i$或$\\sim p_i$，则将$B$展开成$$B \\equiv B \\bigwedge (p_i \\bigvee \\sim p_i) \\equiv (B \\bigwedge p_i) \\bigvee (B \\bigwedge \\sim p_i)$$ 化简 将极小项由小到大排列$m_1 \\bigvee m_2 \\bigvee m_5$用$\\sum(1,2,5)$表示 极大项maxterm析取式中的命题变项中，$p$和$\\sim p$只存在一个，$n$个命题变项构成$2^n$个极小项，这些极小项仅在一种情况下为假例：3个命题变项，8个极大项$$\\begin{matrix}\\sim p \\bigvee \\sim q \\bigvee \\sim r &amp; 111-7 &amp; M_7 \\\\sim p \\bigvee \\sim q \\bigvee r &amp; 110-6 &amp; M_6 \\\\vdots &amp;\\ &amp; \\vdots \\p \\bigvee q \\bigvee r &amp; 000-0 &amp; M_0\\end{matrix}$$ 主合取范式full conjunctive normal form，用$\\prod$表示，由极大项的合取构成的合取范式求主合取范式的步骤： 求$A$的一个合取范式$A’$ 若$A’$的某析取式$B$不含命题变项$p_i$或$\\sim p_i$，则将$B$展开成$$B \\equiv B \\bigvee (p_i \\bigwedge \\sim p_i) \\equiv (B \\bigvee p_i) \\bigwedge (B \\bigvee \\sim p_i)$$ 化简 将极大项由小到大排列$M_1 \\bigwedge M_2 \\bigwedge M_5$用$\\prod(1,2,5)$表示 定理： 主析取范式和主合取范式是唯一的 得知主析取范式和主合取范式中的任意一个都可以很快的得到另一个。$\\sum(2,4,5,6,7) \\equiv \\prod(0,1,3)$ 恰由$2^n$个极小项构成的公式必为重言式 恰由$2^n$个极大项构成的公式必为矛盾式 命题逻辑的推理推理：从前提推出结论的思维过程，前提premise或称假设hypothesis是指已知的命题公式$A_1,A_2,\\cdots,A_n$，结论conclusion是从前提出发应用推理规则推出的命题公式$B$ 基本推理公式 附加率：$A \\Rightarrow (A \\bigvee B)$ 化简律：$(A \\bigwedge B) \\Rightarrow A$ 前后件附加：$$(A \\Rightarrow B) \\Rightarrow ((A \\bigvee C) \\Rightarrow (B \\bigvee C)) \\(A \\Rightarrow B) \\Rightarrow ((A \\bigwedge C) \\Rightarrow (B \\bigwedge C)) \\(A \\Rightarrow B) \\Rightarrow ((C \\Rightarrow A) \\Rightarrow (C \\Rightarrow B)) \\$$ 对偶：$(A \\Rightarrow B) \\Rightarrow (B^* \\Rightarrow A^*)$ 假言推理、分离式：$((A \\Rightarrow B) \\bigwedge A) \\Rightarrow B$ 拒取式：$(A \\Rightarrow B) \\bigwedge \\sim B \\Rightarrow \\sim A$ 析取三段论：$(A \\bigvee B) \\bigwedge \\sim B \\Rightarrow A$ 假言三段论：$(A \\Rightarrow B) \\bigwedge (B \\Rightarrow C) \\Rightarrow (A \\Rightarrow C)$ 等价三段论：$(A \\Leftrightarrow B) \\bigwedge (B \\Leftrightarrow C) \\Rightarrow (A \\Leftrightarrow C)$ 构造性二难：$(A \\Rightarrow B) \\bigwedge (C \\Rightarrow D) \\bigwedge (A \\bigvee C) \\Rightarrow (B \\bigvee D)$ 构造性二难（特殊形式）：$$(A \\Rightarrow C) \\bigwedge (B \\Rightarrow C) \\bigwedge (A \\bigvee B) \\Rightarrow C \\(A \\Rightarrow B) \\bigwedge (\\sim A \\Rightarrow B) \\Rightarrow B$$ 破坏性二难：$(A \\Rightarrow B) \\bigwedge (C \\Rightarrow D) \\bigwedge (\\sim B \\bigvee \\sim D) \\Rightarrow (\\sim A \\bigvee \\sim C)$ 附加前提证明法additional premise前提A，结论$C \\Rightarrow B$可以转化为证明前提A，C，结论B 归谬法(反证法）negation of conclusion前提A，结论B可以转为证明前提$A,\\sim B$，结论得到矛盾 归结法resolution 将$A \\bigwedge \\sim B$化为合取范式$C_1 \\bigwedge C_2 \\bigwedge \\cdots \\bigwedge C_n$，各个$C_i$构成子句集$S = {C_1,C_2,\\cdots,C_n}$ 对S中的子句作归结 直至归结出矛盾式","link":"/blog/2022/09/10/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-02-%E5%AF%B9%E5%81%B6%E5%92%8C%E8%8C%83%E5%BC%8F/"},{"title":"离散数学学习笔记-03-谓词","text":"谓词和量词个体词individual相当于名词，个体常项$a,b,c$，个体变项$x,y,z$，个体变项的取值范围$D$称为个体域或论域，全总个体域是指宇宙间一切事物组成的个体域 谓词 一元谓词：表示个体词的性质，属性$P(x),Q(x)$ 多元谓词：表示个体词之间的关系$P(x,y),R(x,y,z)$ 命题是确定谓词含义的零元谓词 量词 全称量词：universal quantification $(\\forall x)P(x)$ 存在量词：existential quantification $(\\exists x)P(x)$ 谓词公式及其分类谓词公式(合式) 若A是谓词公式，且A种无$\\forall x,\\exists x$出现则$(\\forall x)A(x),(\\exists x)A(x)$也是 若A是，$\\sim A$也是 若A，B是，则由逻辑联结词联结的也是 谓词公式的分类 普遍有效的公式、逻辑有效式：在任何解释下真值为真 不可满足式：在任何解释下真值均为假 可满足式：至少存在一个解释使之为真 定理：丘奇图灵，谓词逻辑是不可判定的，即对任一谓词公式而言，没有一个可行的办法判定它是否是普遍有效的 代换实例：设命题公式$A$含有命题变项$P_1,P_2,\\cdots,P_n$，用n个谓词公式$A_1,A_2,\\cdots,A_n$分别处处代换$P_1,P_2,\\cdots,P_n$所得公式是原公式的代换实例 定理：重言式、矛盾式的代换实例仍然为重言式、矛盾式 $(\\forall x)A(x),(\\exists x)A(x)$中$\\forall,\\exists$之后的x称为量词的指导变项或作用变项，$A(x)$称为相应量词的辖域或作用域，在$A(x)$中的x的出现称为约束出现，x称为约束变项，其他变项称为自由变项，若无自由变项即为命题 自然语言的形式化 将问题分解为一些原子命题和逻辑联结符 分解出各个原子命题的个体词、谓词、量词 按照合式公式的原则翻译出自然语句 例：令谓词$P(x)$表示x是整数，$Q(x)$表示x是奇数，$R(x)$表示x是偶数，$S(x)$表示x是素数，$E(x,y)$表示$x = y$，$G(x,y)$表示$x &gt; y$ 所有的素数都是整数：$(\\forall x)(S(x) \\Rightarrow P(x))$ 谓词逻辑的等值演算等值：谓词公式$A, B$等值，$A \\equiv B$，$A,B$在任何解释下真值均相同 消去量词等值式设论域$D = {a_1,a_2,\\cdots,a_m}$$$(\\forall x)A(x) \\equiv A(a_1) \\bigwedge A(a_2) \\bigwedge \\cdots \\bigwedge A(a_m) \\(\\exists x)A(x) \\equiv A(a_1) \\bigvee A(a_2) \\bigvee \\cdots \\bigvee A(a_m)$$ 量词否定等值式、德摩根律$$\\sim (\\forall x)A(x) \\equiv (\\exists x)\\sim A(x)\\\\sim (\\exists x)A(x) \\equiv (\\forall x)\\sim A(x)$$ 量词辖域收缩与扩张等值式$$(\\forall x)(A(x) \\bigvee B) \\equiv (\\forall x)A(x) \\bigvee B \\(\\forall x)(A(x) \\bigwedge B) \\equiv (\\forall x)A(x) \\bigwedge B \\(\\exists x)(A(x) \\bigvee B) \\equiv (\\exists x)A(x) \\bigvee B \\(\\exists x)(A(x) \\bigwedge B) \\equiv (\\exists x)A(x) \\bigwedge B$$ 量词分配等值式$$(\\forall x)(A(x) \\bigwedge B(x)) \\equiv (\\forall x)A(x) \\bigwedge (\\forall x)B(x) \\(\\exists x)(A(x) \\bigvee B(x)) \\equiv (\\exists x)A(x) \\bigvee (\\forall x)B(x) \\(\\forall x)(\\forall y)A(x,y) \\equiv (\\forall y)(\\forall x)A(x,y) \\(\\exists x)(\\exists y)A(x,y) \\equiv (\\exists y)(\\exists x)A(x,y)$$ 对偶式dual，在仅含联结词$\\sim, \\bigwedge,\\bigvee$的谓词公式$A$中，将$(\\bigwedge,\\bigvee$)、($\\forall,\\exists$)、($F,T$)互换，若$A \\equiv B$则$A^* \\equiv B^*$ 置换规则设$\\Phi(A)$是含谓词公式$A$的公式，$\\Phi(B)$是用谓词公式$B$取代$\\Phi(A)$中的$A$，不一定是每一处之后得到的谓词公式，若$A \\equiv B$则$\\Phi(A) \\equiv \\Phi(B)$ 代替规则将谓词公式中某个自由出现的个体变项的所有自由出现改成改成$A$中未曾出现的符号，所得$A’ \\equiv A$ 换名规则将谓词公式A中某量词的指导变项及其辖域内的所有约束出现改成A中未出现的符号所得$A’ \\equiv A$","link":"/blog/2022/09/10/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-03-%E8%B0%93%E8%AF%8D/"},{"title":"自然语言处理学习笔记-lecture2-数学基础1-微积分","text":"微积分函数设数集$D \\subset \\mathbb{R}$，则称映射$f : D \\subset \\mathbb{R}$为定义在$D$上的函数，通常记为 $y = f(x), x ∈ D$，其中$x$称为自变量，$y$称为因变量，$D$称为定义域，记作$D_f$，即$D_f = D$。对于每个$x ∈ D$，按对应法则$f$，总有唯一的值$y$与之相对应，这个值称为函数$f$在$x$处的函数值，记作$f(x)$，即$y = f(x)$。函数值$f(x)$的全体所构成的集合称为函数f的值域，记作$R_f$或$f (D)$，即 $$R_f =f(D)={y|y=f(x),x∈D}$$例如，$f(x) = 3x + 2$是一个函数，定义域是$R$，值域是$R$，自变量和因变量之间存在一一映射。表示函数的记号可以任意选取，除了常用的$f$以 外，还可以用其他的英文字母或希腊字母，如$g$、$F$和$\\phi$。 复合函数给定两个函数$f$和$g$，复合函数定义为:$$( f \\circ g)(x) = f(g(x))$$两个函数$f$和$g$能构成复合函数$f \\circ g$的条件是:函数$g$的值域$R_g$必须是函数$f$ 的定义域$D_f$的子集，即$R_g \\subseteq D_f$。例如，$y = f(u) = 3u + 2$的定义域为$\\mathbb{R}$，而$u = g(x) = x2 − 2$的定义域为$\\mathbb{R}$。由于$g(R) \\subseteq R$，因此$f$和$g$可以构成复合函数 导数设函数$y = f(x)$在点$x_0$的某个邻域内有定义，当自变量$x$在$x_0$处有增量$\\Delta x$， 而且$x_0 + \\Delta x$也在该邻域内时，函数取得增量$\\Delta y = f(x_0 + \\Delta x) − f(x_0)$。如 果$\\Delta y$与$\\Delta x$之比当$\\Delta x → 0$时极限存在，则称函数$y = f(x)$在点$x_0$处可导， 并称这个极限为函数$y = f(x)$在点$x_0$处的导数，记作:$$f’(x_0) = \\lim_{\\Delta x \\rightarrow 0} \\frac{f(x_0 + \\Delta x) − f(x_0)}{\\Delta x}$$ 导函数如果函数$y = f(x)$在开区间内每一点都可导，则称函数$f(x)$在区间内可导。这时函数$y = f(x)$对于区间内的每一个确定的$x$值，都对应着一个确定的导数值，这就构成一个新的函数。我们将该函数称之为原来函数的导函数，记作$y′$、$f′(x)$或$df(x)/dx$，简称导数。 导数的四则运算对于可导函数$f$和$g$，导数的四则运算规则如下: 加法: $(f + g)’ = f’ + g’$ 减法: $(f − g)’ = f’ − g’$ 乘法: $(fg)′ = f’g + fg’$ 除法: $(f/g)’ = (f’g − fg’)/g^2$ 复合函数的导数对于复合函数$(f \\circ g)(x)$，通常使用链式法则计算其导数:$$( f \\circ g)’(x) = f’(g(x))g’(x)$$令$u = g(x)$，则链式法则的另一种表述方式为:$$\\frac{df(g(x))}{dx} = \\frac{df(u)}{du} \\times \\frac{du}{dx}$$ 二阶导数一般而言，函数$y = f(x)$的导数$y′ = f′(x)$仍然是$x$的函数，可以进一步求 导。二阶导数是原函数导数的导数，即对原函数进行二次求导，记作:$$y’’ = (y’)’$$二阶导数的另一种常见的表示方法为$$y’’ = \\frac{d^2y}{dx^2}$$例如，$y = x^2$的一阶导数为$y’ = 2x$，而二阶导数则是一阶导数$y’ = 2x$的导数y′′ = 2。二阶导数反映了一阶导数的变化率。我们通常使用二阶导数来判断函数的凹凸性并计算极值。类似地，在条件允许的情况下，还可以计算函数的三阶导数、四阶导数或高阶导数。 函数的单调性设函数$f(x)$的定义域为$D$，区间$I \\subset D$。如果对于区间$I$上任意两点$x_1$和$x_2$， 当$x_1 &lt; x_2$时，恒有$f(x_1) &lt; f(x_2)$，则称函数$f(x)$在区间$I$上单调递增。反之，如果对于区间$I$ 上任意两点$x_1$ 和$x_2$ ，当$x_1 &lt;x_2$ 时，恒有 $f(x_1) &gt; f(x_2)$，则称函数$f(x)$在区间$I$上单调递减。 凹函数给定函数$f : \\mathbb{R} → \\mathbb{R}$，对于任意两个点$x_1$和$x_2$，如果满足下列条件：$$f\\left(\\frac{x_1+x_2}{2} \\right) \\leq \\frac{f(x_1) + f(x_2)}{2}$$ 凸函数给定函数$f : \\mathbb{R} → \\mathbb{R}$，对于任意两个点$x_1$和$x_2$，如果满足下列条件：$$f\\left(\\frac{x_1+x_2}{2} \\right) \\geq \\frac{f(x_1) + f(x_2)}{2}$$ 函数的极值设函数$f(x)$在点$x = x_0$及其附近有定义。如果对于$x_0$附近的所有点都有 $f(x) &lt; f(x_0)$，则$f(x_0)$是函数$f(x)$的一个极大值，$x_0$是函数$f(x)$的一个极大值点。如果对于$x_0$附近的所有点都有$f(x) &gt; f(x_0)$，则$f(x_0)$是函数$f(x)$的一个 极小值，$x_0$是函数$f (x)$的一个极小值点。 函数的最值函数在整个定义域内可能有许多极大值或极小值，而且某个极大值不一 定大于某个极小值。函数f(x)在整个定义域内的最小函数值$f(x_0)$称为函数 $f(x)$的最小值，$x_0$称为最小值点。类似地，函数$f(x)$在整个定义域内的最大函数值$f (x_0)$称为函数$f (x)$的最大值，$x_0$称为最大值点。如果函数$f(x)$在闭区间$[a, b]$上连续，则$f(x)$在$[a, b]$上必有最大值和最小值。在开区间$(a, b)$上连续的函数$f(x)$不一定有最大值和最小值，如函数$f(x) = 1/x$。函数的最值点必在函数的极值点或者区间的端点处获得。函数的极值可能有多个，但是最值最多只有一个。如果函数$f(x)$在闭区间$[a, b]$上有定义，在开区间$(a, b)$内有导数，则求函数f(x)在闭区间$[a, b]$上的最大值和最小值的步骤如下: 求函数$f(x)$在开区间$(a,b)$的导数$f’(x)$; 求方程$f’(x) = 0$在$(a, b)$内的解; 求在$(a,b)$内使$f’(x)=0$的所有点的函数值和$f(x)$在闭区间端点处的函数值$f (a)$和$f (b)$; 比较上面所求的所有值，其中最大值为函数$f(x)$在闭区间$[a, b]$上的最大值，最小值为函数$f(x)$在闭区间$[a, b]$上的最小值。 例如，可以使用上述方法计算函数$f(x) = x^2 − 2x + 1$在区间$[−2,2]$上的最大值和最小值，得到函数的最小值点是1，最大值点是−2。 不定积分函数$f(x)$的不定积分是一个导数等于$f(x)$的函数$F$，即$F’(x) = f(x)$。相应地，函数$F(x)$称为$f(x)$的原函数。一个函数通常有多个原函数。例如，函数$f(x) = 2x$的原函数可以是$F(x) = x^2 + 1$，也可以是$F(x) = x^2 + 2$。因此，我们通常将原函数写成以下的形式:$$\\int f(x)dx = F(x) + C$$其中，$C$表示任意常数。常见的积分公式如下: 定积分设函数$f(x)$在区间$[a, b]$上连续，将区间$[a, b]$分成$n$个长度相等的子区间，则 函数$f(x)$在区间$[a, b]$上的定积分定义为:$$\\int_a^b f(x)dx = \\lim_{n \\rightarrow +\\infty}f(a + \\frac{i}{n}(b-a))\\frac{b-a}{n}$$其中，$a$称为积分下限，$b$称为积分上限，$[a, b]$称为积分区间，$x$称为积分变 量，$f (x)$称为被积函数。从直观上理解，定积分计算的是包围区域的面积。 多元函数设$D$是一个非空的$n$元有序数组的集合，$f$为某一确定的对应法则，如果对于每一个有限数组$(x_1, x_2, …, x_n) \\in D$， 通过对应法则$f$，都有唯一确定的实数$y$与之对应，则称对应法则$f$为定义在$D$上的多元函数，记为:$$y = f(x_1,x_2,\\cdots,x_n)$$其中$x_1, x_2, …, x_n$称为自变量，$y$称为因变量。 偏导数设函数$z = f(x, y)$在点$(x_0, y_0)$的某一邻域内有定义，当$y$固定在$y_0$而$x$在$x_0$处 有增量$\\Delta x$时，相应地函数值有增量$f(x_0 + \\Delta x, y_0) − f(x_0, y_0)$。如果极限$$\\lim_{\\Delta x \\rightarrow 0}\\frac{f(x_0 + \\Delta x,y_0)-f(x_0,y_0)}{\\Delta x}$$存在，则称此极限为函数$z = f(x, y)$在点$(x_0, y_0)$处对$x$的偏导数，记为:$$\\frac{\\partial z}{\\partial x} | {x = x_0,y=y_0} = \\lim{\\Delta x \\rightarrow 0}\\frac{f(x_0 + \\Delta x,y_0)-f(x_0,y_0)}{\\Delta x}$$另一种形式是$f_x(x_0, y_0)$。同理可以定义函数在点$(x_0, y_0)$处对y的偏导数。如果函数$z = f(x, y)$在区域$D$内任意一点$(x, y)$处对$x$的偏导数都存在，那么这个偏导数是$x$和$y$的函数，成为函数$z = f(x, y)$对自变量$x$的偏导数，记为 $\\partial z/\\partial x$。 多元函数求导设$f(x, y) = x^2 + 3xy + y − 1$，求该函数对$x$和$y$的偏导在点$(4, − 5)$处的取值。求解方法如下。首先计算函数对$x$的偏导。在计算过程中，我们可以将$y$看作常量，然后对$x$求导:$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x}(x^2 + 3xy + y − 1) = 2x + 3y$$因此，$\\partial f/\\partial x$在$(4, − 5)$处的值为$2 \\times 4 + 3 \\times (−5) = − 7$。接下来计算函数对$y$的偏导，将$x$看作常量:$$\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y}(x^2 + 3xy + y − 1) = 3x + 1$$因此，$\\partial f/\\partial y$在$(4, − 5)$处的值为$3 \\times 4 + 1 = 13$ 多元复合函数求导首先来考虑一元函数与多元函数复合的情况。若函数$u = \\phi(x)$和函数 $v = \\psi(x)$都在点$x$可导，函数$z = f(u, v)$在对应点$(u, v)$具有连续偏导数，那 么复合函数$z = f(\\phi(x), \\psi(x))$在点$x$可导，其导数为:$$\\frac{dz}{dx} = \\frac{\\partial z}{\\partial u}\\frac{du}{dx} + \\frac{\\partial z}{\\partial v}\\frac{dv}{dx}$$例如，令$z = f(u, v) = u^2 − v^2$，$u = \\phi(x) = x^2 − 1$，$v = \\psi(x) = 3x + 2$，则 复合函数$z$对$x$的导数可计算为:$$\\begin{aligned}\\frac{dz}{dx} &amp;= \\frac{\\partial z}{\\partial u}\\frac{du}{dx} + \\frac{\\partial z}{\\partial v}\\frac{dv}{dx} \\&amp;= 2u \\times 2x + (-2v) \\times 3 \\&amp;= 4x^3 - 10x -12\\end{aligned}$$然后考虑多元函数与多元函数复合的情况。如果函数$u = \\phi(x, y)$与函数 $v = \\psi(x, y)$具有对$x$和$y$的偏导数，函数$z = f(u, v)$在对应点$(u, v)$具有连续偏导数，那么复合函数$z = f(\\phi(x, y), \\psi(x, y))$在点$(x, y)$的两个偏导数存在:$$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial u}\\frac{\\partial u}{\\partial x} + \\frac{\\partial z}{\\partial v}\\frac{\\partial v}{\\partial x} \\\\frac{\\partial z}{\\partial y} = \\frac{\\partial z}{\\partial u}\\frac{\\partial u}{\\partial y} + \\frac{\\partial z}{\\partial v}\\frac{\\partial v}{\\partial y}$$例如，令$z = f(u, v) = u + v$，$u = \\phi(x, y) = xy，v = \\psi(x, y) = x + y$，则复合函数$z$对$x$和$y$的偏导数分别是:$$\\frac{\\partial z}{\\partial x} = y + 1 \\\\frac{\\partial z}{\\partial y} = x + 1$$ 梯度设二元函数$z = f(x, y)$在平面区域$D$上具有一阶连续偏导数，则对于每一 个点$(x, y)$可以定义一个向量，称为函数$z = f(x, y)$在点$(x, y)$的梯度，记作:$$\\nabla f(x,y) = \\left(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}\\right)$$例如，令$z = f(x, y) = x^2 − y^3$，则$x$和$y$的偏导函数为:$$\\frac{\\partial f}{\\partial x} = 2x,\\frac{\\partial f}{\\partial y} = 3y^2$$因此，函数$f(x, y)$在点$(2,1)$处的梯度是一个二维向量$(4,3)$。多元函数的梯度可以类似地计算。梯队对于计算多元函数的极值而言非常重要，在深度学习的参数优化中被广泛使用。 多元函数极值设函数$z = f(x,y)$在点$(x_0,y_0)$的某个邻域内有定义，对于该邻域内异于 $(x_0, y_0)$的点，如果不等式$$f(x, y) \\lt f(x_0, y_0)$$成立，则称函数$f(x, y)$在点$(x_0, y_0)$处有极大值。如果不等式$$f(x, y) \\gt f(x_0, y_0)$$成立，则称函数$f(x, y)$在点$(x_0, y_0)$处有极小值。例如，函数$z = 3x^2 + 4y^2$在点$(0,0)$处有极小值，因为除了$(0,0)$以外所有的点的函数值均为正，只有在点$(0,0)$处的函数值为0。与之相反，函数$z = − \\sqrt{x^2 + y^2}$在点$(0,0)$处有极大值，因为除了$(0,0)$以外所有的点的函数值均为负，只有在点$(0,0)$处的函数值为0。 多元函数极值条件定理1(必要条件):设函数$z = f(x,y)$在点$(x_0,y_0)$处具有偏导数，且在点 $(x_0, y_0)$处有极值，则函数在该点的偏导数必然为0:$$f_x(x_0, y_0) = 0, f_y(x_0, y_0) = 0$$定理2(充分条件):设函数$z = f(x, y)$在点$(x_0, y_0)$的某邻域内连续且有一阶及二阶连续偏导数，并且$f_x(x_0, y_0) = 0，f_y(x_0, y_0) = 0$，令$$f_{xx}(x_0, y_0) = A, f_{xy}(x_0, y_0) = B, f_{yy}(x_0, y_0) = C$$则$f(x, y)$在$(x_0, y_0)$处是否取得极值的条件如下: 当$AC − B^2 &gt; 0$时有极值，当$A &lt; 0$时有极大值，$A &gt; 0$时有极小值。 当$AC − B^2 &lt; 0$时没有极值。 当$AC − B^2 = 0$时可能有极值，也可能没有极值。 求多元函数极值求二元函数$f(x, y) = x^3 − y^3 + 3x^2 + 3y^2 − 9x$的极值。首先求解一阶导数组成的方程组:$$f_x(x, y) = 3x^2 + 6x − 9 = 0 \\f_y(x, y) = −3y^2 + 6y = 0$$得到四组解:$(1, 0)、(1, 2)、(−3, 0)$和$(−3, 2)$。它们不一定都是极值点，需要进一步考察二阶导数:$$f_{xx}(x, y) = 6x + 6 \\f_{xy}(x, y) = 0 \\f_{yy}(x, y) = −6y + 6$$对四个解分别计算A、B和C，考察定理2的条件。 $(1,0):AC−B^2=12×6&gt;0$且$A=12&gt;0$，因此$(1,0)$是函数$f(x,y)$的一个极小值点，对应的极小值是$f(1,0) = − 5$。 $(1, 2):AC − B^2 = 12 × (−6) &lt; 0$，因此$(1, 2)$不是函数$f(x, y)$的极值点。 $(−3, 0):AC − B^2 = (−12) × 6 &lt; 0$，因此$(−3, 0)$不是函数$f(x, y)$的极值点。 $(−3,2):AC−B^2=(−12)×(−6)&gt;0$且$A=−12&lt;0$，因此$(−3,2)$是函数$f(x, y)$的一个极大值点，对应的极大值是$f(−3, 2) = − 31$。 拉格朗日乘子法求函数$z = f(x, y)$在满足$g(x, y) = 0$下的条件极值，可以转化为函数$$F(x, y, \\lambda) = f(x, y) + \\lambda g(x, y)$$的无约束条件极值问题。例如，给定双曲线$xy = 3$求该曲线上距离原点最近的点。这是一个典型的带约束的求极值问题。原始问题可以转化为:$$F(x, y, λ) = x^2 + y^2 + λ(xy − 3)$$计算函数$F(x, y, λ)$的一阶偏导，得到方程组:$$F_x(x, y, λ) = 2x + λy = 0 \\F_y(x, y, λ) = 2y + λx = 0 \\F_λ(x, y, λ) = xy − 3 = 0$$求解该方程组，可以得到$λ = 2$或$λ = − 2$。当$λ = 2$时，无法求解$x$和$y$，因为势必有$−x^2 = 3$。当$λ = − 2$时，有两组解:$( 3, 3)$和$(− 3, − 3)$。","link":"/blog/2022/09/10/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture2-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%801-%E5%BE%AE%E7%A7%AF%E5%88%86/"},{"title":"自然语言处理学习笔记-lecture2-数学基础2-概率论","text":"概率论随机试验具备以下三个特点的试验称为随机试验: 可以在相同的条件下重复地运行; 每次试验的可能结果可能不止一个，并且能事先明确试验的所有可能结果; 进行一次试验之前不能确定哪一个结果会出现。 以下是一些随机试验的例子: 抛一枚硬币，观察正面$H$、反面$T$出现的情况。 抛一颗骰子，观察出现的点数。 在一批灯泡里任意抽取一只，测试它的寿命。 样本空间对于随机试验，尽管在每次试验之前不能预知试验的结果，但试验的所有可能结果组成的集合是已知的。我们将随机试验$E$的所有可能结果组成的集合称为$E$的样本空间，记为$S$。样本空间中的元素，称为样本点。例如，给定以下随机试验 $E_1$:抛一枚硬币，观察正面$H$、反面$T$出现的情况。 $E_2$:抛一颗骰子，观察出现的点数。 $E_3$:在一批灯泡里任意抽取一只，测试它的寿命。对应的样本空间是: $S_1:{H,T}$ $S_2:{1,2,3,4,5,6}$ $S_3:{t|t \\geq 0}$ 随机事件试验$E$的样本空间$S$的子集称为$E$的随机事件，简称为事件。例如，令“将一枚硬币抛掷两次，观察正面$H$、反面$T$出现的情况”是一个随机试验$E$，则其样本空间总共包含四个元素:$$S = {HH, HT, TT, TH}$$我们可以定义一个事件“第一次出现的是$H$”，即$$A1 = {HH, HT}$$还可以定义另一个事件“两次出现的是同一面”，即$$A2 = {HH, TT}$$显然，$A_1$和$A_2$都是样本空间的子集。 概率设$E$是随机试验，$S$是样本空间。对于$E$的每一个事件$A$赋予一个实数，记为$P(A)$，称为事件$A$的概率。概率必须满足以下条件: 非负性:对于每一个事件$A$，有$P(A) ≥ 0$; 规范性:对于必然发生的事件$S$，有$P(S) = 1$; 可列可加性:设$A_1 、A_2 、…$是两两互不相容的事件，即对于 $A_i \\bigcap A_j =\\emptyset(i \\neq j)$，有$P(A_1 \\bigcup A_2 \\bigcup …)=P(A_1)+P(A_2)+…$。 令$A$和$B$为任意两个事件，$AB$表示两个事件同时发生，以下公式成立:$$P(A \\bigcup B) = P(A) + P(B) − P(AB)$$对于前面抛掷两次硬币的例子，如果$A$表示“第一次是$H$”，$B$表示“两次结果都一样”，那么$AB$表示“两次都是$H$”。 等可能概型等可能概型是指符合以下两个条件的随机试验: 试验的样本空间只能包含有限个元素; 试验中每个基本事件(即每个结果)发生的可能性基本相同。 例如，一个口袋里装有6只球，其中有4只白球和2只红球。从袋中取球两次，每次随机地取一只，假设每只球都有相等概率被抽中。第一次取一球不放回袋中，第二次从剩余的球中再取一球。计算:(1)取到的两只球都是白球的概率，(2)取到的两只球至少有一只是白球的概率。首先计算两只球都是白球的概率:$(4/6) × (3/5) = 2/5$。然后，先计算两只球都是红球的概率:$(2/6) × (1/5) = 1/15$，然后可以得到取到的两只球至少有一只是白球的概率:$1 − (1/15) = 14/15$。 条件概率设A和B是两个事件，且P(A) &gt; 0，称$$P(B|A) = \\frac{P(AB)}{P(A)}$$为在事件$A$发生的条件下事件$B$发生的条件概率。 不难验证，条件概率符合概率定义中的三个条件: 非负性:对于每一个事件$B$，有$P(B|A) ≥ 0$; 规范性:对于必然发生的事件$S$，有$P(S|A) = 1$; 可列可加性:设$B_1 、B_2 、…$是两两互不相容的事件，则有：$$P(\\bigcup_{i=1}^\\infty B_i|A) = \\sum_{i=1}^\\infty P(B_i|A)$$ 例如，一个口袋里装有6只球，其中有4只白球和2只红球。从袋中取球两次，每次随机地取一只，假设每只球都有相等的概率被抽中。第一次取一球不放回袋中，第二次从剩余的球中再取一球。设事件$A$为“第一次取到白球”，事件$B$为“第二次取到白球”，计算条件概率$P(B | A)$。首先计算$P(A)$。由于开始口袋中有6只球，其中有4只白球，因此第一次取到白球的概率$P(A) = 4/6$。然后计算$P(AB)$，即事件“两次都抽到白球”的概率:$$P(AB) = \\frac{4}{6} \\times \\frac{3}{5} = \\frac{2}{5}$$因此，条件概率计算如下:$$P(B|A) = \\frac{P(AB)}{P(A)} = \\frac{2}{5} \\times \\frac{6}{4} = \\frac{3}{5}$$ 全概率公式设$S$为试验$E$的样本空间，$B_1, B_2, …, B_n$为事件$E$的一组事件，如果以下两个条件成立 $B_i \\bigcap B_j = \\emptyset,i \\neq j,i,j = 1,\\cdots, n$ $B_1 \\bigcup B_2 \\bigcup \\cdots \\bigcup B_n = S$ 则称$B_1,B_2,\\cdots,B_n$为样本空间$S$的一个划分。例如，试验$E$“掷一颗骰子观察其点数”样本空间为$S = {1, 2, 3, 4, 5, 6}$，则 $B_1 = {1, 2, 3}，B_2 = {4, 5}和B_3 = {6}$是$S$的一个划分。设$A$是试验$E$的一个事件，$B_1, B_2, …, B_n$是其样本空间的一个划分，则以下全概率公式成立:$$P(A) = \\sum_{i = 1}^nP(A|B_i)P(B_i)$$ 贝叶斯公式设$A$和$B$是随机试验$E$的任意两个事件，以下贝叶斯公式成立:$$P(B|A) = \\frac{P(A|B)P(B)}{P(A)}$$可以进一步与全概率公式结合起来。令$B_1, B_2, …, B_n$是$S$的一个划分，而且$P(B_i) &gt; 0 (i = 1, 2,…, n)$，则有:$$P(B_i|A) = \\frac{P(A|B_i)P(B_i)}{\\sum_{j = 1}^nP(A|B_j)P(B_j)}$$贝叶斯公式在人工智能中非常重要，产生了重要的贝叶斯学派。贝叶斯公式对于揭示信息认知加工过程与规律、实现有效的学习和判断决策都具有十分重要的理论意义和实践价值。 独立性设$A$和$B$是两个随机事件，如果满足等式$P(AB) = P(A)P(B)$则称事件$A$和$B$相互独立。两个事件相互独立的含义是其中一个事件已发生，不影响另一个事件发生的概率。在实际应用中，对于事件的独立性通常是根据事件的实际意义去判断。如果根据实际情况分析，两个事件之间没有关联或者关联很弱，那么就认为它们之间是相互独立的。例如，如果甲、乙两人同一天感冒，甲在中国，乙在美国，双方并未接触，则可以认为两个事件是独立的。如果甲、乙是住在同一个宿舍的舍友，那么就不能认为是相互独立的。 随机变量将一枚硬币抛掷两次，观察出现正面$H$和反面$T$的情况，样本空间是$$S = {HH, HT, TT, TH}$$以$X$表示两次投掷得到正面$H$的总数，则$X$的取值是一个随机变量: $X = 0$:当投掷结果是${TT}$时; $X = 1$:当投掷结果是${HT}$或${TH}$时; $X = 2$:当投掷结果是${HH}$时。 随机变量的取值随试验的结果而定，在试验之前不能预知取什么值，并且其取值有一定的的概率。随机变量的引入，使我们能够描述各种随机现象，并能利用数学方法对随机试验的结果进行深入分析。 离散型随机变量取值是有限个或可列举无限个的随机变量称为离散型随机变量。例如，抛掷一枚硬币，只可能取正面和反面两个取值，因此是离散型随机变量。设离散型随机变量$X$可能的取值为$x_k (k = 1, 2,…) $，$X$取各个可能值的概率，即事件${X = x_k}$的概率，为:$$P(X = x_k) = p_k,k=1,2,\\cdots$$上式称为离散型随机变量$X$的分布律。注意，根据概率的定义，$p_k$满足以下两个条件: $p_k \\geq 0,k=1,2,\\cdots$ $\\sum_{k=1}^\\infty p_k = 1$ 离散型随机变量分布以下两种离散型随机变量经常被使用。第一个是$(0 − 1)$分布。设随机变量$X$只能取0和1两个值，其分布律为$$P(X = k) = p^k(1-p)^{1-k}$$其中，$k$的取值是0或1，$0 &lt; p &lt; 1$。第二个是二项分布。设$n$是一个正整数，$k$是一个不大于$n$的非负整数，即 $0 ≤ k ≤ n$，某个随机事件$A$发生的概率为$p$，则在$n$次试验中事件$A$发生$k$ 次的概率为:$$P(X = k) = \\left( \\begin{matrix} n \\ k \\end{matrix} \\right) p^k(1-p)^{1-k}$$显然，当$n = 1$时，二项分布等价于$(0 − 1)$分布。 随机变量的分布函数对于非离散型随机变量，其取值不能一一列举，因此需要采用新的形式对离散型和非离散型随机变量进行统一描述。设$X$是一个随机变量，$x$是任意实数，函数$$F(x) = P(X \\leq x)$$称为$X$的分布函数。对于任意两个实数$x_1$和$x_2$且满足$x_1 &lt; x_2$，均有:$$\\begin{aligned}P(x_1 \\leq X \\leq x_2) &amp;= P(X \\leq x_2) - P(X \\leq x_1) \\&amp;= F(x_2) - F(x_1)\\end{aligned}$$因此，如果已知$X$的分布函数，我们就知道$X$落在任意区间$(x1, x2]$的概率。从这个意义上说，分布函数完整地描述了随机变量的统计规律性。 分布律与分布函数 x -1 2 3 $p_k$ 0.25 0.50 0.25 给定上表所示的分布律，相应的分布函数定义如下:$$F(x) =\\begin{cases}0.00 &amp; x \\lt -1 \\0.25 &amp; -1 \\leq x \\lt 2 \\0.75 &amp; 2 \\leq x \\lt 3 \\1.00 &amp; x \\geq 3\\end{cases}$$由此可见，分布函数可以全面地描述离散型随机变量。 连续型随机变量如果对于随机变量$X$的分布函数$F(x)$，存在非负函数$f(x)$，使对于任意实数$x$有$$F(x) = \\int_{-\\infty}^x f(t)dt$$则称$X$为连续型随机变量。$f (x)$称为$X$的概率密度函数，具有以下性质: $f(x) \\geq 0$ $\\int_{-\\infty}^{\\infty} f(x)dx = 1$ 对于任意实数$x_1$和$x_2(x_1 ≤ x_2)$，$P(x_1 &lt; X ≤ x_2) = F(x_2) − F(x_1)$; 若$f(x)$在点$x$处连续，则有$F’(x) = f(x)$。 均匀分布若连续型随机变量$X$具有概率密度$$f(x) =\\begin{cases}\\frac{1}{b-a} &amp; a \\lt b \\0 &amp; otherwise\\end{cases}$$则称$X$在区间$(a, b)$上服从均匀分布，记为$X ∼ U(a, b)$。 正态分布若连续型随机变量$X$具有概率密度$$f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$$其中$\\mu$和$\\sigma$实常数且$\\sigma &gt; 0$，则称$X$服从参数为$\\mu$和$\\sigma$的正态分布或高斯分布，记作$X ∼ N(\\mu, \\sigma^2)$。 二维随机变量之前只限于讨论单个随机变量的情况，实际问题中经常出现多个随机变 量的情况。例如，为了研究某一地区某一年龄段儿童的发育情况，需要 统计儿童的身高和体重。设$(X, Y )$是二维随机变量，对于任意实数$x$和$y$，二元函数$$F(x,y) = P(X \\leq x,Y \\leq y)$$称为二维随机变量$(X, Y )$的分布函数，或随机变量$X$和$Y$的联合分布函数。$$P(x_1 \\lt X \\leq x_2，y_1 \\lt Y \\leq y_2) = F(x_2,y_2) - F(x_2,y_1) - F(x_1,y_2) + F(x_1,y_1)$$ 二维离散型随机变量如果二维随机变量$(X, Y )$全部可能的取值是有限对或可列无限多对，则称 $(X,Y)$是离散型的随机变量。设$(X,Y)$所有的可能取值为$(x_i,y_j)，i, j = 1, 2,…$，则$X$和$Y$的联合分布律定义为$$P(X = x,Y = y) = p_{ij}$$联合分布律通常使用表格的方式来表示: $x_1$ $x_2$ $\\cdots$ $x_i$ $\\cdots$ $y_1$ $p_{11}$ $p_{21}$ $\\cdots$ $p_{i1}$ $\\cdots$ $y_2$ $p_{12}$ $p_{22}$ $\\cdots$ $p_{i2}$ $\\cdots$ $\\vdots$ $\\vdots$ $\\vdots$ $\\ddots$ $\\vdots$ $\\cdots$ $y_j$ $p_{1j}$ $p_{2j}$ $\\cdots$ $p_{ij}$ $\\cdots$ $\\vdots$ $\\vdots$ $vdots$ $\\cdots$ $\\vdots$ $\\cdots$ 二维连续型随机变量对于二维随机变量$(X,Y)$的分布函数$F(x,y)$，如果存在非负的函数$f(x,y)$使 得对于任意$x$和$y$都有:$$F(x,y) = \\int_{-\\infty}^{y}\\int_{-\\infty}^{x}f(u,v)dudv$$则称$(X,Y)$是连续型的二维随机变量，函数$f(x,y)$称为二维随机变量$(X,Y)$的概率密度，或成为随机变量X和Y的联合概率密度。例如，给定概率密度:$$f(x,y) =\\begin{cases}2e^{-(2x+y)} &amp; x \\gt 0,y \\gt 0 \\0 &amp; otherwise\\end{cases}$$可计算分布函数为$F(x, y) = (1 − e^{−2x})(1 − e^{−y})$，当$x &gt; 0$且$y &gt; 0$时。 边缘分布律二维随机变量$(X, Y )$作为一个整体，具有分布函数$F(x, y)$，而$X$和$Y$都是随机变量，各自也有分布函数，分别记为$F_X(x)$和$F_Y(y)$，分别称为二维随机变量 $(X, Y )$关于X和关于Y的边缘分布函数，定义如下:$$F_X(x) = P(X \\leq x,Y \\lt \\infty) = F(x,\\infty) \\F_Y(y) = P(X \\lt \\infty,Y \\leq y) = F(\\infty,y)$$随机变量$X$和$Y$的分布律分别定义为:$$P(X = x_i) = \\sum_{j = 1}^\\infty P_{ij} \\P(Y = y_j) = \\sum_{i = 1}^\\infty P_{ij}$$上述式子也称为二维离散型随机变量$(X, Y )$关于$X$和$Y$的边缘分布律。 边缘概率密度对于连续型随机变量$(X,Y)$，设其概率密度为$f(x,y)$，由于$$F_X(x) = F(x,\\infty) = \\int_{-\\infty}^x\\left(\\int_{-\\infty}^\\infty f(x,y)dy\\right)dx$$由此可知$X$是一个连续型随机变量，而且其概率密度函数为:$$f_X(x) = \\int_{-\\infty}^\\infty f(x,y)dy$$同样，$Y$也是一个连续型随机变量，而且其概率密度函数为:$$f_Y(y) = \\int_{-\\infty}^\\infty f(x,y)dx$$$f_X(x)$和$f_Y(y)$分别是关于$X$和关于$Y$的边缘概率密度。 条件分布律下面来考虑事件${Y = y_j}$在已发生的条件下事件${X = x_i}$发生的概率，也就是求事件${X = x_i | Y = y_j}$的概率。设$(X, Y )$是二维离散型随机变量，对于固定的$j$，若$P(Y = y_j) &gt; 0$，则称:$$P(X = x_i|Y = y_j) = \\frac{P(X = x_i,Y = y_i)}{P(Y = y_j)}$$为在$Y = y_j$条件下随机变量$X$的条件分布律。类似地，对于固定的$i$，若$P(X = x_i) &gt; 0$，则称:$$P(Y = y_j|X = x_i) = \\frac{P(X = x_i,Y = y_i)}{P(X = x_i)}$$为在$X = x_i$条件下随机变量$Y$的条件分布律。 条件概率密度设二维随机变量$(X,Y)$的概率密度为$f(x,y)$，$(X,Y)$关于$Y$的边缘概率密度为$f_Y(y)$。若对于固定的y，fY(y) &gt; 0，则在Y = y条件下X的条件概率密度定义为:$$f_{X|Y}(x|y) = \\frac{f(x,y)}{f_Y(y)}$$与之对应地，在$Y = y$条件下$X$的条件分布函数定义为:$$F_{X|Y}(x|y) = \\int_{-\\infty}^x \\frac{f(x,y)}{f_Y(y)}dx$$类似地，我们也可以定义在X = x条件下Y的条件概率密度和条件分布函数。 相互独立的随机变量设$F(x,y)$、$F_X(x)$和$F_Y(y)$分别是二维随机变量$(X,Y)$的分布函数及边缘概率分布，如果对于所有的$x$和$y$有:$$P(X \\leq x,Y \\leq y) = P(X \\leq x)P(Y \\leq y) \\F(x, y) = F_X(x)F_Y(y)$$则称随机变量$X$和$Y$相互独立当$X$和$Y$是离散型随机变量时，$X$和$Y$相互独立的条件是:$$P(X = x_i, Y = y_j) = P(X = x_i)P(Y = y_j)$$当$X$和$Y$是连续型随机变量时，$X$和$Y$相互独立的条件是:$$f(x, y) = f_X(x)f_Y(y)$$ 数学期望设离散型随机变量$X$的分布律为$P(X = x_k) = p_k(k ≥ 1)$，其数学期望定义为:$$\\mathbb{E}(X) = \\sum_{k = 1}^\\infty x_kp_k$$类似地，设连续型变量$X$的概率密度为$f (x)$，其数学期望定义为:$$\\mathbb{E}(X) = \\int_{-\\infty}^\\infty xf(x)dx$$例如，假定$P(X = 0) = 0.3，P(X = 1) = 0.5，P(X = 2) = 0.2$，则$X$的数学期望计算如下:$$\\mathbb{E}(X) = 0 × 0.3 + 1 × 0.5 + 2 × 0.2 = 0.9$$ 随机变量函数的数学期望设$Y$是随机变量$X$的连续函数，即$Y = g(X)$。如果$X$是离散型随机变量，其分布律为$P(X = x_k) = p_k(k ≥ 1)$，则$Y$的数学期望定义为:$$\\mathbb{E}(Y) = \\mathbb{E}(g(X))= \\sum_{k = 1}^\\infty g(x_k)p_k$$如果$X$是连续型随机变量，其概率密度为$f (x)$，则$Y$的数学期望定义为:$$\\mathbb{E}(Y) = \\mathbb{E}(g(X))= \\int_{-\\infty}^\\infty g(x)f(x)dx$$ 数学期望的性质 设$C$为实常数，则有$\\mathbb{E}(C) = C$。 设$X$是一个随机变量，$C$是常数，则有$\\mathbb{E}(CX) = C\\mathbb{E}(X)$。 设$X$和$Y$是两个随机变量，则有$\\mathbb{E}(X + Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)$。这一性质可以推广到任意有限个随机变量之和的情况。 设$X$和$Y$是两个相互独立的随机变量，则有$\\mathbb{E}(XY) = \\mathbb{E}(X)\\mathbb{E}(Y)$。这一 性质可以推广到任意有限个相互独立的随机变量之积的情况。 方差方差用于度量随机变量与其均值的偏离程度。设$X$是一个随机变量，$X$的方差定义为:$$D(X) = Var(X) = \\mathbb{E}((X - \\mathbb{E}(X))^2)$$我们通常将 $\\sqrt{D(X)}$记为$\\sigma(X)$，称为标准差或者均方差。对于离散型随机变量，方差计算公式为$$D(X) = \\sum_{k = 1}^\\infty(x_k - \\mathbb{E}(X))^2p_k$$对于连续型随机变量，方差计算公式为:$$D(X) = \\int_{-\\infty}^{\\infty}(x - \\mathbb{E}(X))^2f(x)dx$$","link":"/blog/2022/09/10/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture2-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%802-%E6%A6%82%E7%8E%87%E8%AE%BA/"},{"title":"自然语言处理学习笔记-lecture2-数学基础3-线性代数","text":"线性代数向量$n$个有次序的数$a_1, a_2, …, a_n$所组成的数组称为$n$维向量。这$n$个数称为该向量的$n$个分量，第$i$个数$a_i$称为第$i$个分量。向量通常表示为: $$a = (a_1,a_2,\\cdots,a_n)$$向量的模也称为向量的大小，定义如下:$$||a|| = \\sqrt{a_1^2+\\cdots+a_n^2}$$给定两个$n$维向量$a = (a_1, …, a_n)$和$b = (b_1, …, b_n)$，主要运算公式如下: 加法:$a+b=(a_1+b_1,…,a_n+b_n)$ 与数的乘法:设$\\lambda$是一个实数，则$\\lambda a = (\\lambda a_1, …, \\lambda a_n)$ 内积:$a\\cdot b=\\sum_{i = 1}^N a_ib_i$ 元素级乘法:$a\\circ b=(a_1b_1,\\cdots,a_nb_n)$ 矩阵由$m × n$个数$a_{ij}(i = 1,…, m; j = 1,…, n)$排成的$m$行$n$列的数表称为$m × n$ 矩阵，记作$$A = \\left(\\begin{matrix}a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\\\end{matrix}\\right)$$这$m × n$个数称为矩阵$A$的元素。行数和列数都等于$n$的矩阵称为$n$阶方阵。只有一行的矩阵称为行向量:$$A = (a_1,a_2,\\cdots,a_n)$$只有一列的矩阵称为列向量:$$A = \\left(\\begin{matrix}a_{1}\\a_{2}\\\\vdots\\a_{m}\\\\end{matrix}\\right)$$ 矩阵的加法设有两个$m × n$矩阵$A = (a_{ij})$和$B = (b_{ij})$，那么矩阵$A$和$B$的和记为:$$A + B = \\left(\\begin{matrix}a_{11} + b_{11} &amp; a_{12} + b_{12} &amp; \\cdots &amp; a_{1n} + b_{1n} \\a_{21} + b_{21} &amp; a_{22} + b_{22} &amp; \\cdots &amp; a_{2n} + b_{2n} \\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\a_{m1} + b_{m1} &amp; a_{m2} + b_{m2} &amp; \\cdots &amp; a_{mn} + b_{mn} \\\\end{matrix}\\right)$$需要注意，只有两个矩阵的行数和列数相同时，才可以进行加法运算。设$A、B$和$C$都是$m × n$矩阵，则矩阵加法满足以下运算律: 交换律:$A+B=B+A$。 结合律:$(A+B)+C=A+(B+C)$ 数与矩阵的乘法实数$\\lambda$与矩阵$A$的乘积记作$\\lambda A$或$A\\lambda$，计算如下:$$\\lambda A = A\\lambda = \\left(\\begin{matrix}\\lambda a_{11} &amp; \\lambda a_{12} &amp; \\cdots &amp; \\lambda a_{1n} \\\\lambda a_{21} &amp; \\lambda a_{22} &amp; \\cdots &amp; \\lambda a_{2n} \\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\lambda a_{m1} &amp; \\lambda a_{m2} &amp; \\cdots &amp; \\lambda a_{mn} \\\\end{matrix}\\right)$$设$A$和$B$为$m × n$矩阵，$\\lambda$和$\\mu$为实数，则数与矩阵的乘法满足以下规律: $(\\lambda \\mu)A = \\lambda(\\mu A)$ $(\\lambda + \\mu)A = \\lambda A + \\mu A$ $\\lambda(A + B) = \\lambda A + \\lambda B$ 矩阵与矩阵相乘设$A$是一个$m × s$矩阵，$B$是一个$s × n$矩阵，那么矩阵$A$与矩阵$B$的乘积是 一个$m × n$矩阵$C = AB$，其中:$$c_{ij} = \\sum_{k = 1}^s a_{ik}b_{kj}$$其中，$a_{ik}$是矩阵$A$的元素，$b_{kj}$是矩阵$B$的元素，$c_{ij}$是矩阵$C$的元素。注意，当且仅当左矩阵的列数等于右矩阵的行数时，两个矩阵才能相乘。 矩阵的转置把矩阵$A$的行换成同序数的列得到一个新矩阵，称为$A$的转置矩阵，记作 $A^T$。矩阵的转置满足下述运算规律: $(A^T)^T = A$ $(A + B)^T = A^T + B^T$ $(\\lambda A)^T = \\lambda A^T$ $(AB)^T = B^TA^T$ 方阵的行列式由$n$阶方阵$A$的元素所构成的行列式，称为方阵$A$的行列式，记作 $| A |$ 或 $detA$。给定一个两行两列的方阵，其行列式计算公式为:$$A =\\left(\\begin{matrix}a_{11} &amp; a_{12} \\a_{21} &amp; a_{22}\\end{matrix}\\right) \\|A| = detA =\\left|\\begin{matrix}a_{11} &amp; a_{12} \\a_{21} &amp; a_{22}\\end{matrix}\\right| = a_{11}a_{22} - a_{12}a_{21}$$ 三行行列式三行三列的方阵的行列式的计算更复杂一些，基本规律是先按照正向 (即从上方往右下方)对角线求和，再按照反向(即从上方往左下方) 对角线求和，最后计算两者之差。$$\\left|\\begin{matrix}a_{11} &amp; a_{12} &amp; a_{13} \\a_{21} &amp; a_{22} &amp; a_{23} \\a_{31} &amp; a_{32} &amp; a_{33}\\end{matrix}\\right| \\\\begin{aligned}= &amp;+ a_{11}a_{22}a_{33} - a_{11}a_{23}a_{32} \\&amp;-a_{12}a_{21}a_{33} + a_{12}a_{23}a_{31} \\&amp;+a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31}\\end{aligned}$$ 逆矩阵对于$n$阶矩阵$A$，如果有一个$n$阶矩阵$B$$$AB = BA = E$$则说矩阵$A$是可逆的，并把矩阵$B$称为$A$的逆矩阵。$A$的逆矩阵通常记为 $A^{−1}$。对于可逆矩阵，有以下性质: 如果矩阵$A$是可逆的，那么$A$的逆矩阵是唯一的。 如果矩阵 $A$可逆，则$|A| \\neq 0$。 如果$AB = E$或$BA = E$，则$B = A^{−1}$。 如果$A$可逆，则$A^{−1}$亦可逆，且$(A^{−1})^{−1} = A$。 如果$A$和$B$为同阶矩阵且均可逆，则$AB$亦可逆，且$(AB)^{−1} = B^{−1}A^{−1}$。 矩阵的初等变换给定一个矩阵，以下三种变换称为初等行变换: 对调第$i$行和第$j$行，记作$r_i \\leftrightarrow r_j$。 第$i$行的所有元素乘以实数$k$，记作$kr_i$。 把第$j$行所有元素的$k$倍加到第$i$行对应的元素上，记作$r_i + kr_j$。 同理，可以定义矩阵的初等列变换: 对调第$i$列和第$j$列，记作$c_i \\leftrightarrow c_j$。 第$i$列的所有元素乘以实数$k$，记作$kc_i$。 把第$j$列所有元素的$k$倍加到第$i$列对应的元素上，记作$c_i + kc_j$。 标准型与矩阵的秩对于$m × n$矩阵$A$，总可以经过初等行变换和列变换将其化简为以下形式:$$F = \\left(\\begin{matrix}E_r &amp; O \\O &amp; O\\end{matrix}\\right)$$其中，$E_r$表示维度为$r$的单元方阵，$O$表示元素全为$0$的矩阵。$F$称为标准形，$r$称为矩阵的秩。 方阵的特征值和特征向量设$A$是$n$阶矩阵，如果存在实数$\\lambda$和$n$维非零列向量$x$使得以下等式成立:$$Ax = \\lambda x$$则称$\\lambda$是矩阵$A$的特征值，非零向量$x$为$A$的对应于特征$\\lambda$的特征向量。","link":"/blog/2022/09/10/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture2-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%803-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"title":"自然语言处理学习笔记-lecture2-数学基础4-信息论","text":"信息论信息量什么是信息量?假设我们听到了两件事，分别如下: 事件A:巴西队获得了2022年FIFA世界杯冠军。 事件B:中国队获得了2022年FIFA世界杯冠军。仅凭直觉来说，显而易见事件B的信息量比事件A的信息量要大(也就是“大新闻”)。究其原因，是因为事件A发生的概率很大，事件B发生的概率很小。所以当越不可能的事件发生了，我们获得的信息量就越大，而越可能发生的事件发生了，我们获得的信息量就越小。因此，信息量应该和事件发生的概率有关。熵如果$X$是一个离散型随机变量，其概率分布为$P(X = x) = p(x)，x ∈ \\mathscr{X}$。 其中，$\\mathscr{X}$表示随机变量所有取值的集合，则该随机变量的熵为:$$H(X) = -\\sum_{x \\in \\mathscr{X}}p(x)log_2p(x)$$我们约定$0 log_2 0 = 0$。熵表示信源每发出一个符号所提供的平均信息量。一个随机变量的熵越 大，其不确定性越大，相应地正确估计其值的可能性就越小。越不确定 的随机变量需要越大的信息量来确定其值。相对熵两个概率分布$p(x)$和$q(x)$的相对熵也称为KL散度(英文全称:Kullback- Leibler divergence)，定义如下:$$KL(p||q) = \\sum_{x \\in \\mathscr{X}}p(x)log\\frac{p(x)}{q(x)}$$约定$0 log(0/q) = 0，p log(p/0) = \\infty$。相对熵通常用于衡量两个概率分布的差距。当两个随机分布相同时，其相对熵为0。当两个随机分布的差别增加时，其相对熵也增加。交叉熵相对熵的公式可以表述为$$\\begin{aligned}KL(p||q)&amp;= \\sum_xp(x)\\log p(x) - \\sum_xp(x) \\log q(x) \\&amp;= -H(p(x)) - \\sum_xp(x) \\log q(x)\\end{aligned}$$等式的前一部分是$p$的熵，而后一部分则是交叉熵:$$H(p,q) = -\\sum_xp(x) \\log q(x)$$在人工智能中，往往需要评估模型分布和真实分布之间的差距，使用$KL$ 散度非常合适。但由于$KL$散度的前一部分跟真实分布相关，在优化过程中不变化，因此一般使用交叉熵作为损失函数并评估模型。","link":"/blog/2022/09/10/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture2-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%804-%E4%BF%A1%E6%81%AF%E8%AE%BA/"},{"title":"高级人工智能-lecture2-搜索问题","text":"本文主要讨论基于目标的Agent中的一种，称为问题求解Agent，要进行问题求解，首先要讨论的是对问题及其解的精确定义，将通过一些实例来说明如何描述一个问题及其解，接着介绍一些求解此类问题的通用的搜索算法，包括无信息搜索算法和有信息的搜索算法。 搜索问题目标是世界的一个状态集合，是目标被满足的那些状态的集合，任务是找到现在和未来如何行动，以使达到一个目标状态。 搜索问题构成： 输入：问题 输出：问题的解 构成（五个部分）问题的输入 Agent的初始状态 描述Agent的可能行动：在给定的状态$s$下，Agent可以采取的动作$ACTIONS(s)$ 转移模型：对每个模型的描述，在状态$s$下采取动作$a$后达到的状态$RESULT(s,a)$，也称为后继状态 状态空间：初始状态、行动和转移模型无疑就定义了问题的状态空间，即从初始状态可以达到的所有的状态的集合。状态空间形成一个有向网络或图，其中结点表示状态，结点之间的弧表示行动 目标测试：确定给定的状态是不是目标状态 路径耗散函数为每条路径赋一个耗散值，即边加权。 输出：解是一个行动序列，将初始状态转换成目标状态，解的质量由路径耗散函数度量，所有解里路径耗散值最小的解即为最优解。 搜索树 根节点对应了初始状态 子节点对应了父节点的后继 节点显示状态，但对应的是到达这些状态的行动 一般的树搜索搜索算法特性： 完备性，当问题有解时，保证能找到一个解 最优性：保证能找到最优解 时间复杂度 空间复杂度 以下面三个值来计算 $b$：分支因子，任何节点的最多后继数 $d$：目标节点所在的最浅的深度，如从根节点到目标状态的步数 $m$：状态空间中任何路径的最大长度 节点的类别： 扩展的节点：扩展之后就是访问过的节点 生成的节点：入队列的节点，等待被扩展 图搜索主要思想：不要扩展一个状态两次执行： 树搜索+扩展过的状态集closed set 将节点扩展成搜索树 扩展节点之前，检查确保它的状态在之前没有被扩展 如果不是新的状态，忽略；如果是新的，加入到closed set 无信息的搜索算法uninformed无信息搜索指的是除了问题定义中提供的状态信息外没有任何附加信息。 广度优先搜索 完备性：可以保证 最优性：只有当每个路线的代价都是一样的时候 时间复杂度：$b+b^2 + b^3 + \\cdots + b^d = O(b^d)$ 空间复杂度：对任何类型的图搜索，每个已扩展的结点都保存在探索集中，空间复杂度总是在时间复杂度的 $b$ 分之一内。特别对于宽度优先图搜索，每个生成的结点都在内存中。那么将有$O(b^{d - 1})$个结点在探索集中，$O(b^d)$个结点在边缘结点集中。所以空间复杂度为 $O(b^d)$ 可以在节点扩展时或者生成时进行目标测试 代价一致搜索uniform cost search深度优先搜索知识找到了行动次数最少得路线，但是没有考虑到代价这一个因素-也就是最小生成树 完备性：可以满足 最优性：可以满足 一致代价搜索由路径代价而不是深度来引导，所以算法复杂度不能简单地用$b$和$d$来表示。引入$C^*$表示最优解的代价，假设每个行动的代价至少为$\\varepsilon$，那么最坏情况下，算法的时间和空间复杂度为$O(b^{1+\\lfloor C^* / \\varepsilon \\rfloor})$要比$O(b^d)$大得多。这是因为一致代价搜索在探索包含代价大的行动之前，经常会先探索代价小的行动步骤所在的很大的搜索树。当所有的单步耗散都相等的时候，$O(b^{1+\\lfloor C^* / \\varepsilon \\rfloor})$就是$O(b^d)$。此时，一致代价搜索与宽度优先搜索类似，除了算法终止条件，宽度优先搜索在找到解时终止，而一致代价搜索则会检査目标深度的所有结点看谁的代价最小;这样，在这种情况下一致代价搜索在深度无意义地做了更多的工作。 在节点被扩展时进行目标测试 深度优先搜索 完备性：深度可能是无限的，需要避免环的出现 最优性：没有，总是寻找最左边的路线，没有考虑深度和代价 时间复杂度：深度优先搜索的时间复杂度受限于状态空间的规模(当然，也可能是无限的 )另一方面，深度优先的树搜索，可能在搜索树上生成所有 $O(b^m)$个结点，其中 $m$ 指的是任一结点的最大深度;这可能比状态空间大很多。要注意的是 $m$ 可能比 $d$ (最浅解的深度)大很多， 并且如果树是无界限的，$m$ 可能是无限的。 空间复杂度：深度优先搜索只需要存储一条从根结点到叶结点的路径，以及该路径上每个结点的所有未被扩展的兄弟结点即可。一旦一 个结点被扩展，当它的所有后代都被探索过后该结点就从内存中删除。考虑状态空间分支因子为$b$最大深度为$m$ 深度优先搜索只需要存储$O(bm)$个结点. 深度受限搜索在无限状态空间深度优先搜索会令人尴尬地失败，而这个问题可以通过对深度优先搜索设置界限 $l$来避免。就是说，深度为$l$的结点被当作没有后继对待。这种方法称为深度受限搜索 (depth-limited search)。 完备性：如果我们选 择了$l \\lt d$，即是说，最浅的目标结点的深度超过了深度限制，那么这种搜索算法是不完备的。 最优性：如果选择的$l \\gt d$，深度受限搜索同样也不是最优的。 时间复杂度：$O(b^l)$ 空间复杂度：$O(bl)$ 深度优先搜索可以看作是特殊的深度受限搜索，其深度$l = \\infty$ 迭代深入搜索iterative deepening结合DFS的空间优势和BFS的时间优势 首先限制深度为1，进行深度优先搜索 然后限制深度为2，进行深度优先搜索 然后限制深度为3，进行深度优先搜索 …… 当深度界限达到$d$，即最浅的目标结点所在深度时，就能找到目标结点。 浪费冗余：通常绝大多数的节点都在底层，所以上层节点生成多次影响不是很大 完备性：和宽度优先搜索一样，当分支因子有限时是该搜索算法是完备的 最优性：当路径代价是结点深度的非递减函数时该算法是最优的。 时间复杂度：$O(b^d)$ 空间复杂度：$O(bd)$ 双向搜索同时运行两个搜索，一个从初始状态向前搜索同时另一个从目标状态向后搜索，希望它们在中间某点相遇，此时搜索终止。理由是$b^{d/2} + b^{d/2}$要比$b^d$小很多。 时间复杂度：$O(b^{\\frac{d}{2}})$ 空间复杂度：$O(b^{\\frac{d}{2}})$ 一般来讲，当搜索空间较大并且不知道解所在深度时，迭代加深的深度优先搜索是首选的无信息搜索方法。 搜索算法总结b表示树的宽度，是分支因子，m表示最大深度，d表示最浅目标节点的深度 有信息(启发式)的搜索策略informed search使用问题本身的定义之外的特定知识，比无信息的搜索策略更有效地进行问题求解。结点是基于评价函数$f(n)$值被选择扩展的。评估函数被看作是代价估计，因此评估值最低的结点被选择首先进行扩展。最佳优先图搜索的实现与一致代价搜索类似, 不过最佳优先是根据$f$值而不是$g$值对优先级队列排队。对$f$的选择决定了搜索策略，大多数的最佳优先搜索算法$f$由启发函数(heuristic function) 构成:$$h(n) = 结点n到目标结点的最小代价路径的代价估计值$$ 估计一个状态到目标距离的函数 问题给予算法的额外信息，为特定搜索问题而设计 贪婪最佳优先搜索 策略：扩展最接近目标状态的节点，理由是这样可能可以很快找到解。因此，它只用启发式信息，即$f(n) = h(n)$，例如在罗马尼亚问题中是到目的地的距离。 通常情况下可以很快到达目标 最坏情况下类似于深度优先搜索 完备性：和深度优先搜索类似，不完备 最优性：不具备 时间复杂度：最坏情况下$O(b^m)$ 空间复杂度：最坏情况下$O(b^m)$ 复杂度决定于启发式函数的质量$A^*$搜索 结合使用代价一致搜索(代价$g(n)$)和贪心搜索(代价$h(n)$)，$g(n)$是从开始结点到结点$n$的路径代价，$h(n)$是从结点 $n$ 到目标结点的最小代价路径的估计值$$f(n) = d(n) + h(n)$$ 算法和一致代价类似，只是使用的代价变为$f$ 保证最优性的条件：可采纳性和一致性 一致的启发式都是可采纳的 如果$h(n)$是可采纳的，那么$A^*$的树搜索版本是最优的 如果$h(n)$是一致的，你们图搜索的$A^*$算法是最优的算法的结束条件：当目标入列时不停止，只有当目标出列时才停止可采纳启发式保障最优性的第一个条件是$h(n)$是一个可采纳启发式。可采纳启发式是指它从不会过高估计到达目标的代价。因为$g(n)$是当前路径到达结点$n$的实际代价，而$f(n) = g(n) + h(n)$，我们可以得到直接结论: $f(n)$永远不会超过经过结点$n$的解的实际代价。 启发函数h是可采纳的，那么：$$0 \\leq h(n) \\leq h^*(n)$$其中$h^*(n)$是最接近目标的真实耗散，例如到目标节点的直线距离肯定是最短的，那么当$h(n)$取直线距离的时候就是可采纳的 想出可采纳的启发函数是$A^*$算法实际使用中的重点 一致性启发式对于每个节点$n$和通过任一行动$a$生成的$n$的每个后继结点$n’$，从结点$n$到达目标的估计代价不大于从$n$到$n’$的单步代价与从$n’$到达目标的估计代价之和$$h(n) \\leq c(n,a,n’) + h(n’)$$ $A^*$算法的最优性 首先证明如果$h(n)$是一致的，那么沿着任何路径的$f(n)$值是非递减的，假设有$n’$是$n$的后继结点，那么有：$$f(n’) = g(n’) + h(h’) = g(n) + c(n,a,n’) + h(n’) \\geq g(n) + h(n) = f(n)$$ 下一步则需要证明:若$A^*$选择扩展结点$n$时，就已经找到到达结点$n$的最优路径。否则，在到达结点$n$的最优路径上就会存在另一边缘结点$n’$，因为$f$在任何路径上都是非递减的，$n’$的$f$代价比$n$小，会先被选择。 所以算法以$f(n)$的非递减序扩展接点，所以第一个被选择扩展的目标结点一定是最优解，之后扩展的目标结点代价都不会低于它 算法是效率最优的，也就是说没有其他的最优算法可以保证扩展的结点数少于$A^*$算法 $A^*$算法的完备性假设$C^*$是最优解路径的代价值，那么可以得到： $A^*$算法扩展所有$f(n) \\lt C^*$的结点； 算法在扩展目标结点之前可能会扩展一些正好处于等值线$f(n) = C^*$上的结点，这要这样的结点的数目是有穷的，算法就是完备的 $A^*$算法的目标损耗假设绝对误差定义为$\\Delta \\equiv h^* - h$，$h^*$表示从根结点到目标结点的实际代价，相对误差定义为$\\varepsilon = (h^* - h) / h^*$，当模型是一个只有一个目标状态的状态空间时，时间复杂度为$O(b^\\Delta)$，考虑每步骤代价均为常量，我们可以把这记为$O(b^{\\varepsilon d})$ 存储受限的启发式搜索$A^*$算法减少内存需求的简单办法就是将迭代加深的思想用在启发式搜索上，即迭代加深$A^*(IDA^*)$算法 判断启发式函数的好坏 有效分支因子$b^*$，对于某一问题，如果$A^*$算法生成的总结点数为$N$，解的深度为$d$，那么$b^*$就是深度为$d$的标准搜索树为了能够包括$N + 1$个结点所必需的分支因子。即：$$N + 1 = 1 + b^* + (b^*)^2 + \\cdots + (b^*)^d$$有效分支因子越小，启发式函数越好，设计良好的启发式会使$b^*$的值接近于1 对于两个启发式函数，如果对于任一结点$n$，有$h_2(n) \\geq h_1(n)$，那么$h_2$启发式函数更好， 因为使用$h_2$的$A^*$算法永远不会比使用$h_1$的$A^*$算法扩展更多的结点 从松弛问题出发设计可采纳的启发式 减少了行动限制的问题称为松弛问题。松弛问题的状态空间图是原有状态空间的超图，原因是减少限制导致图中边的增加。 由于松弛问题增加了状态空间的边，原有问题中的任一最优解同样是松弛问题的最优解;但是松弛问题可能存在更好的解，理由是增加的边可能导致捷径。所以，一个松弛问题的最优解代价是原问题的可采纳的启发式。","link":"/blog/2022/09/11/%E9%AB%98%E7%BA%A7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-lecture2-%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98/"},{"title":"高级人工智能-lecture2-局部搜索问题","text":"适用于那些关注解状态而不是路径代价的问题，如果到目标的路径是无关紧要的，我们可能考虑不同的算法，这类算法不关心路径。 局部搜索算法从单个当前结点(而不是多条路径)出发，通常只移动到它的邻近状态。一般情况下不保留搜索路径。局部搜索算法家族包括由统计物理学带来的模拟退火法(simulated annealing) 和进化生物学带来的遗传算法 (genetic algorithms)。除了找到目标，局部搜索算法对于解决纯粹的最优化问题十分有用，其目标是根据目标函数找到最佳状态。 爬山法(贪婪局部搜索)是简单的循环过程，不断向值增加的方向持续移动—— 即，登高。算法在到达一个“峰顶”时终止，邻接状态中没有比它值更高的。 爬山法经常会陷入困境： 局部极大值:局部极大值是一个比它的每个邻接结点都高的峰顶，但是比全局最大值要小。爬山法算法到达局部极大值附近就会被拉向峰顶，然后就卡在局部极大值处无处可走。 山脊:下图显示了山脊的情况。山脊造成一系列的局部极大值，贪婪算法很难处理这种情况。图中的状态(黑色圆点)叠加在从左到右上升的山脊上，创造了一个不直接相连的局部极大值序列。从每个局部极大点出发，可能的行动都是指向下山方向的 高原:高原是在状态空间地形图上的一块平原区域。它可能是一块平的局部极大值，不存在上山的出口，或者是山肩，从山肩还有可能取得进展。爬山法在高原可能会迷路。解决这个问题的一个方法是在最佳后继值和当前状态值相等的时候侧向移动，此时可能陷入循环，所以可以同时限制侧向移动的次数。 爬山法的变形 随机爬山法在上山移动中随机地选择下一步，随机地生成后继结点直到生成一个优于当前结点的后继;被选中的概率可能随着上山移动的陡峭程度不同而不同。这种算法通常比最陡上升算法的收敛速度慢不少，但是在某些状态空间地形图上它能找到更好的解。 随机重启爬山法(random restart hill climbing) ：原爬山法是不完备的，该算法通过随机生成初始状态来导引爬山法搜索，直到找到目标。这个算法完备的概率接近于 1, 理由是它最终会生成一个目标状态作为初始状态。 模拟退火搜索模拟退火算法的内层循环与爬山法类似。只是它没有选择最佳移动，选择的是随机移动。如果该移动使情况改善，该移动则被接受。否则，算法以某个小于 1 的概率接受该移动。如果移动导致状态“变坏”，概率则成指数级下降一评估值$\\Delta E$变坏。这个概率也随“温度’’$T$降低而下降:开始$T$高的时候可能允许“坏的”移动，$T$越低则越不可能发生。如果调度让$T$下降得足够慢，算法找到全局最优解的概率逼近于 1。 局部束搜索 局部束搜索(local beam search) 算法记录$k$个状态而不是只记录一个。它从$k$个随机生成的状态开始。每一步全部$k$个状态的所有后继状态全部被生成。如果其中有一个是目标状态，则算法停止。否则， 它从整个后继列表中选择$k$个最佳的后继，重复这个过程。 如果是最简单形式的局部束搜索，那么由于这$k$个状态缺乏多样性，它们很快会聚集到状态空间中的一小块区域内，使得搜索代价比高昂的爬山法版本还要多。随机束搜索(stochastic beam search) 为解决此问题的一种变形，它与随机爬山法相类似。随机束搜索并不是从候选后继集合中选择最好的$k$个后继状态，而是随机选择$k$个后继状态，其中选择给定后继状态的概率是状态值的递增函数。 遗传算法遗传算法(genetic algorithm, 或 GA) 是随机束搜索的一个变形，它通过把两个父状态结合来生成后继，而不是通过修改单一状态进行。像束搜索一样，遗传算法也是从$k$个随机生成的状态开始，我们称之为种群。每个状态，或称个体，用一个有限长度的字符串表示，通常是 0、1 串。例如，八皇后问题的状态必须指明 8 个皇后的位置，每列有 8 个方格，所以需要 $8 \\times \\log_28 = 24$比特来表示。","link":"/blog/2022/09/11/%E9%AB%98%E7%BA%A7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-lecture2-%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98/"},{"title":"卜算法学习笔记-02-分而治之算法01","text":"给定一个问题，如何求解？首先查看最简单的实例能否求解，假如可以求解的话，下一步就是思考能否将大的实例分解成小的实例，以及能否将小的实例组合成为大的实例，如果都可以的话就称实例能归约，这个问题具有递归结构，可以设计递归算法进行求解 排序问题：对数组的归约排序问题： 输入：一个包含 $n$ 个元素的数组 $A[0..n − 1]$，其中每个元素都是整数; 调整元素顺序后的数组 $A$，使得对任意的两个下标 $0 ≤ i &lt; j ≤ n − 1$，有 $A[i] ≤ A[j]$。 依据元素下标拆分数组：插入排序与归并排序第一种拆分方案及插入排序算法 算法分析我们只需执行一个简单操作即可将数组 $A[0..n − 1]$ 分解成两部分:前 $n − 1$ 个元素 $A[0..n − 2]$，以及单独一个元素 $A[n − 1]$。前 $n − 1$ 个元素组成一个小 的数组，是原给定实例的子实例。在将原给定实例拆分成子实例之后，我们假定子实例已被求解，对数组来说，所谓子实例的解就是已经排好序的小的数组 $A[0..n − 2]$。要想完成对整个数组 $A[0..n−1]$ 的排序，我们只需将最后一个元素 $A[n−1]$和小数组 $A[0..n−2]$ 中的元素逐个比较，然后将 $A[n − 1]$ 插入到合适的位置即可。连续应用递归调用，最终会到达基始情形:当 $n = 1$ 时，数组 $A$ 只有一个元素，此时无需排序，直接返回即可。 时间复杂度：$$T(n) =\\begin{cases}1 &amp; n = 1 \\T(n - 1) + O(n) &amp; otherwise\\end{cases}$$将上述递归式展开：$$\\begin{aligned}T(n) &amp;\\leq T(n - 1) + cn \\&amp;\\leq T(n - 2) + c(n - 1) + cn \\&amp;\\cdots \\&amp;\\leq c + \\cdots + c(n - 1) + cn \\&amp;= O(n^2)\\end{aligned}$$算法低效的原因是:在运行过程中，问题规模是呈线性下降的，即每次递归操作都是将规模为 $n$ 的问题分解成一个规模为 $n − 1$ 的子问题。第二种拆分方案及归并排序算法 算法分析：将大的数组 $A[0..n − 1]$按下标拆分成规模相同的两半，即 $A[0..⌈ \\frac{n}{2} ⌉ − 1]$和 $A[⌈ \\frac{n}{2} ⌉..n − 1]$;每一半依然是数组，形式相同，但是规模变小，因此是原给定实例的子实例。通过迭代执行分解操作，即可使得问题规模呈指数形式下降。在使用递归调用将小的数组排好序之后，我们只需依据这两个已排好序的小的数组，“归并”(Merge)出整个数组。这里的归并包括两层意思:合并、以及排序。归并过程：循环不变量，是指关于程序行为的一个断言;这个断言在循环起始时成立，并且每执行一轮循环时都保持成立，因此可以推论出当循环结束时，断言必定成立。 时间复杂度分析$$T(n) =\\begin{cases}1 &amp; n = 1 \\2T(\\frac{n}{2}) + O(n) &amp; otherwise\\end{cases} = O(n \\log n)$$ 分而治之算法时间复杂度分析及Master定理在分而治之算法中，一种常见的情况是将一个规模为 $n$ 的实例归约成 $a$ 个子实例，每个子实例规模都相同(设为 $\\frac{n}{b}$ )。假如“组合”子实例解的时间开销是 $O(n^d)$，则我们可将时间复杂度 $T(n)$ 的递归关系及基始情形表示如下:$$T(n) =\\begin{cases}1 &amp; n = 1 \\aT(\\frac{n}{b}) + O(n^d) &amp; otherwise\\end{cases}$$对于子问题比较规整的情况，即每个子问题的规模都相同，T(n) 上界的显式表达式已被总结成 Master 定理:$$\\begin{aligned}T(n)&amp;= aT(\\frac{n}{b}) + O(n^d) \\&amp;\\leq aT(\\frac{n}{b}) + cn^d \\&amp;\\leq a(aT(\\frac{n}{b^2}) + c(\\frac{n}{b})^d) + cn^d \\&amp;\\leq \\cdots \\&amp;\\leq cn^d(1 + \\frac{a}{b^d} + (\\frac{a}{b^d})^2 + \\cdots + (\\frac{a}{b^d})^{\\log_b n}) + a^{\\log_b n} \\&amp;=\\begin{cases}O(n^{\\log_b a}) &amp; d &lt; \\log_b a \\O(n^{\\log_b a}\\log n) &amp; d = \\log_b a \\O(n^{d}) &amp; d &gt; \\log_b a \\\\end{cases}\\end{aligned}$$依据元素的值拆分数组-快速排序算法分析依据元素的数值将大数组拆分成小数组，即选定一个元素作“中心元”(Pivot)，比中心元数值小的元素组成一个小数组，比中心元数值大的那些元素组成另一个小数组。时间复杂度称排序后的数组 $A$ 为 $\\tilde{A}$，因此数组 &amp;A$ 的最小元是 $\\tilde{A}[0]$， 最大元是 $\\tilde{A}[n − 1]$，中位数是 $\\tilde{A}[⌈ \\frac{n}{2} ⌉]$。我们在选择中心元时可能面临如下两种情况:(1) $\\tilde{A}[n − 1]$/ $\\tilde{A}[0]$: 这样只会生成一个子实例，规模减少了 1，呈线性降低。如果在每一次迭代都是如此选择的话， 运行过程就与 InsertionSort 算法相同，时间复杂度为:$$T(n) = T(n - 1) + O(n) = O(n^2)$$(2) $\\tilde{A}[⌈ \\frac{n}{2} ⌉]$: 这样会生成两个子实例，每个子实例的规模都是原来的一半，呈指数下降。如果在每一次迭代都是如此选择的话，运行过程就与 MergeSort 算法相同，时间复杂度为:$$T(n) = 2T(\\frac{n}{2}) + O(n) = O(n\\log n)$$证明运行时间的期望值依然是 $O(n \\log n)$Modified-QuickSort 算法只做了一点修改:随机选择一个元素做中心元之后，先检验一下这个中心元是否足够好;如果足够好，则继续执行后续的比较和排序，否则重新选择一个元素做中心元。所谓的中心元足够好，是指它位于A ̃的中间区域，即$\\tilde{A}[⌈ \\frac{n}{4} ⌉] \\cdots \\tilde{A}[⌈ \\frac{3n}{4} ⌉]$，修改后的算法时间复杂度为：$$\\begin{aligned}T(n)&amp;\\leq T(\\frac{n}{4}) + T(\\frac{3n}{4}) + 2n \\&amp;\\leq (T(\\frac{n}{16}) + T(\\frac{3n}{16}) + 2\\frac{n}{4}) + (T(\\frac{3n}{16}) + T(\\frac{9n}{16}) + 2\\frac{3n}{4}) + 2n \\&amp;= (T(\\frac{n}{16}) + T(\\frac{3n}{16}) ) + (T(\\frac{3n}{16}) + T(\\frac{9n}{16})) + 2n + 2n \\&amp;\\leq \\cdots \\&amp;= O(n \\log_{\\frac{4}{3}} n)\\end{aligned}$$接下来我们分析 QuickSort 算法的时间复杂度。在做具体的分析之前，我们先陈述关于运行时间的 3 点事实:(1) 运行时间由比较次数界定:我们的目标就是计算期望运行时间 $\\mathbb{E}[X]$。(2) 任意两个元素 $\\tilde{A}[i]$ 和 $\\tilde{A}[j]$ 最多只会比较一次(3) 两个元素$\\tilde{A}[i]$ 和 $\\tilde{A}[j]$发生比较的概率是$\\frac{2}{j - i + 1}$$$\\begin{aligned}Pr[\\tilde{A}[i]与\\tilde{A}[j]进行比较]&amp;= \\frac{1}{n} + \\frac{1}{n} + \\frac{n - (j - i + 1)}{n} \\times \\frac{2}{j - i + 1} \\&amp;= (\\frac{j - i + 1}{n} + \\frac{n - (j - i + 1)}{n}) \\times \\frac{2}{j - i + 1} \\&amp;= \\frac{2}{j - i + 1}\\end{aligned}$$由此计算时间复杂度为：$$\\begin{aligned}\\mathbb{E}[X]&amp;= \\mathbb{E}[\\sum_{i=0}^{n-1}\\sum_{j = i + 1}^{n - 1}X_{ij}] \\&amp;= \\sum_{i=0}^{n-1}\\sum_{j = i + 1}^{n - 1}\\mathbb{E}[X_{ij}] \\&amp;= \\sum_{i=0}^{n-1}\\sum_{j = i + 1}^{n - 1}\\frac{2}{j - i + 1} \\&amp;= \\sum_{i=0}^{n-1}\\sum_{k = 1}^{n - i - 1}\\frac{2}{k + 1} \\&amp;\\leq \\sum_{i=0}^{n-1}\\sum_{k = 1}^{n - 1}\\frac{2}{k + 1} \\= O(n \\log n)\\end{aligned}$$空间复杂度需要创建两个辅助数组 $S_−$ 和 $S_+$，这样一来，除了数组本身之外还要额外占用 $n$ 个内存单元，导致当 $n$ 比较大时，内存需求有时难以满足。所以有了原位排序算法，为避免开辟辅助数组 $S_−$ 和 $S_+$，Lomuto 算法直接将数组 A 的左半部分当做 $S_−$，存放比中心元小的元素;把数组 A 的右半部分当做 $S_+$，存放比中心元大的元素","link":"/blog/2022/09/12/%E5%8D%9C%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-02-%E5%88%86%E8%80%8C%E6%B2%BB%E4%B9%8B%E7%AE%97%E6%B3%9501/"},{"title":"统计学习方法学习笔记-02-感知机","text":"首先介绍感知机模型，然后叙述感知机的学习策略，特别是损失函数，最后介绍感知机学习算法，包括原始模式和对偶模式，并证明算法的收敛性 感知机模型输入空间(特征空间)：$\\mathcal{X} \\subseteq R^n$，假设$x \\in \\mathcal{X}$输出空间：$\\mathcal{Y} = {+1,-1}$，假设$y \\in \\mathcal{Y}$由输入空间到输出空间的如下函数称为感知机：$$f(x) = sign(\\omega \\cdot x + b)$$其中$\\omega \\in R^n$叫做权值，$b \\in R$叫做偏置，$\\omega \\cdot x$表示内积，sign表示符号函数：$$sign(x) =\\begin{cases}+1 &amp; x\\geq 0 \\-1 &amp; x \\lt 0\\end{cases}$$ 感知机是一种线性分类模型，属于判别模型，对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面。 感知机模型的假设空间时定义在特征空间中的所有线性分类函数或线性分类器，即函数集合$f|f(x) = \\omega \\cdot x + b$ 感知机学习策略目标：确定模型参数$\\omega,b$损失函数：误分类点到超平面$S$的总距离$$\\frac{1}{||\\omega||}|\\omega \\cdot x_i + b|$$$||\\omega||$是指$\\omega$的$L_2$范数又因为误分类点有如下定义：$$-y_i(\\omega \\cdot x_i + b) \\gt 0$$所以误分类点到超平面的距离为：$$-\\frac{1}{||\\omega||}y_i(\\omega \\cdot x_i + b)$$假设误分类点集合$M$，那么所有误分类点到超平面的总距离为：$$-\\frac{1}{||\\omega||}\\sum_{x_i \\in M}y_i(\\omega \\cdot x_i + b)$$得到损失函数：$$L(\\omega,b) = -\\sum_{x_i \\in M}y_i(\\omega \\cdot x_i + b)$$ 感知机学习算法感知机学习问题转化为求解损失函数式的最优化问题，最优化的方法是随机梯度下降法 感知机学习算法的原始形式输入：训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in \\mathcal{X} = R^n,y_i \\in \\mathcal{Y} = {-1,+1},i=1,2,\\cdots,N$；学习率$\\eta(0 \\lt \\eta \\leq 1)$输出：$\\omega,b$；感知机模型$f(x) = sign(\\omega \\cdot x + b)$ 选取初值$\\omega_0,b_0$； 在训练集中选取数据$(x_i,y_i)$； 如果$y_i(\\omega \\cdot x_i + b) \\leq 0$:$$\\omega \\leftarrow \\omega + \\eta y_ix_i \\b \\leftarrow b + \\eta y_i$$ 转至第二步，直到没有误分类点 算法的收敛性收敛性证明：误分类的次数$k$是有上界的，经过有限次搜索可以找到将训练数据完全正确分开的超平面假设$\\hat{\\omega} = (\\omega^T,b)^T,\\hat{x} = (x^T,1)^T,\\hat{\\omega} \\in R^{n + 1},\\hat{x} \\in R^{n + 1}$，则$\\hat{\\omega} \\cdot \\hat{x} = \\omega \\cdot x + b$定理： 存在满足条件$||\\hat{\\omega}{opt}|| = 1$的超平面$\\hat{\\omega} {opt}\\cdot \\hat{x} = \\omega{opt} \\cdot x + b{opt} = 0$将训练数据集完全正确分开；且存在$\\gamma \\gt 0$对所有$i = 1,2,\\cdots,N$$$y_i(\\hat{\\omega} {opt}\\cdot \\hat{x_i}) =y_i(\\omega{opt} \\cdot x_i + b_{opt}) \\geq \\gamma$$ 令$R = \\mathop{max}\\limits_{1 \\leq i \\leq N}||\\hat{x}_i||$，则感知机算法在训练数据集上的误分类次数$k$满足不等式：$$k \\leq \\left( \\frac{R}{\\gamma} \\right)^2$$ 证明： 由于训练数据集是线性可分的，所以存在超平面可将训练数据集完全正确分开，取此超平面为$\\hat{\\omega} {opt}\\cdot \\hat{x} = \\omega{opt} \\cdot x + b_{opt} = 0$，使$||\\hat{\\omega}{opt}|| = 1$，由于对有限的$i = 1,2,\\cdots,N$，均有$$y_i(\\hat{\\omega} {opt}\\cdot \\hat{x_i}) =y_i(\\omega{opt} \\cdot x_i + b{opt}) \\gt 0$$所以存在$$\\gamma = \\mathop{min}\\limits_{i}{y_i(\\omega_{opt} \\cdot x_i + b_{opt})}$$使$$y_i(\\hat{\\omega} {opt}\\cdot \\hat{x_i}) =y_i(\\omega{opt} \\cdot x_i + b_{opt}) \\geq \\gamma$$ 感知机算法从$\\hat{\\omega}0 = 0$开始，如果实例被误分类，则更新权重，令$\\hat{\\omega}{k-1}$是第$k$个误分类实例之前的扩充权重向量，即：$$\\hat{\\omega}{k-1} = (\\omega{k-1}^T,b_{k-1})^T$$则第$k$个误分类实例的条件是：$$y_i(\\hat{\\omega}{k-1} \\cdot \\hat{x}i) = y_i(\\omega{k-1} \\cdot x_i + b{k - 1}) \\leq 0$$若$(x_i,y_i)$是被$\\hat{\\omega}{k-1} = (\\omega{k-1}^T,b_{k-1})^T$误分类的数据，则$\\omega$和$b$的更新是：$$\\omega_k \\leftarrow \\omega_{k-1} + \\eta y_ix_i \\b_k \\leftarrow b_{k-1} + \\eta y_i$$即：$$\\hat{\\omega}k = \\hat{\\omega}{k-1} + \\eta y_i \\hat{x}i$$递推1：$$\\begin{aligned}\\hat{\\omega}k \\cdot \\hat{\\omega}{opt} &amp;= \\hat{\\omega}{k-1} \\cdot \\hat{\\omega}{opt} + \\eta y_i \\hat{\\omega}{opt} \\cdot \\hat{x}i \\&amp;\\geq \\hat{\\omega}{k-1} \\cdot \\hat{\\omega}{opt} + \\eta\\gamma \\&amp;\\geq \\hat{\\omega}{k-2} \\cdot \\hat{\\omega}{opt} + 2\\eta\\gamma \\&amp;\\geq \\cdots \\&amp;\\geq k\\eta\\gamma\\end{aligned}$$递推2：$$\\begin{aligned}||\\hat{\\omega}k||^2 &amp;= ||\\hat{\\omega}{k-1}||^2 + 2\\eta y_i\\hat{\\omega}{k-1} \\cdot \\hat{x}i + \\eta^2||\\hat{x}i||^2 \\&amp;\\leq ||\\hat{\\omega}{k-1}||^2 + \\eta^2||\\hat{x}i||^2 \\&amp;\\leq ||\\hat{\\omega}{k-1}||^2 + \\eta^2R^2 \\&amp;\\leq ||\\hat{\\omega}{k-2}||^2 + 2\\eta^2R^2 \\&amp;\\leq \\cdots \\&amp;\\leq k\\eta^2 R^2\\end{aligned}$$结合两个递推式：$$k\\eta\\gamma \\leq \\hat{\\omega}k \\cdot \\hat{\\omega}{opt} \\leq ||\\hat{\\omega}k||\\ ||\\hat{\\omega}{opt}|| \\leq \\sqrt{k}\\eta R \\k^2\\gamma^2 \\leq kR^2 \\k \\leq \\left( \\frac{R}{\\gamma} \\right)^2$$感知机算法的对偶形式不失一般性，假设$\\omega_0,b_0$均为0，已知下式$$\\omega \\leftarrow \\omega + \\eta y_ix_i \\b \\leftarrow b + \\eta y_i$$逐步修改$\\omega,b$，设修改$n$次，则$\\omega,b$关于$(x_i,y_i)$的增量分别是$\\alpha_iy_ix_i,\\alpha_iy_i$，这里$\\alpha_i = n_i\\eta$，可以得到：$$\\omega = \\sum_{i = 1}^N\\alpha_iy_ix_i \\b = \\sum_{i = 1}^N\\alpha_iy_i$$输入：训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in \\mathcal{X} = R^n,y_i \\in \\mathcal{Y} = {-1,+1},i=1,2,\\cdots,N$；学习率$\\eta(0 \\lt \\eta \\leq 1)$输出：$\\alpha,b$感知机模型：$$f(x) = sign\\left(\\sum_{j = 1}^N\\alpha_jy_jx_j \\cdot x + b \\right),\\alpha = (\\alpha_1,\\alpha_2,\\cdots,\\alpha_N)^T$$ $\\alpha \\leftarrow 0,b \\leftarrow 0$; 在训练集中选取数据$(x_i,y_i)$ 如果$y_i\\left(\\sum_{j = 1}^N\\alpha_jy_jx_j \\cdot x_i + b \\right) \\leq 0$，则：$$\\alpha_i \\leftarrow \\alpha_i + \\eta \\b \\leftarrow b + \\eta y_i$$ 转至第二步直到没有误分类数据 注：在计算的过程中训练实例以内积的形式出现，可以预先计算储存下来，这个矩阵叫做Gram矩阵，$G = [x_i \\cdot x_j]_{N \\times N}$","link":"/blog/2022/09/14/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-02-%E6%84%9F%E7%9F%A5%E6%9C%BA/"},{"title":"统计学习方法学习笔记-03-k近邻法","text":"首先叙述$k$近邻算法，然后讨论$k$近邻模型及三个基本要素，最后讲述$k$近邻法的一个实现方法，$kd$树，介绍构造和搜索$kd$树的算法。 k近邻算法输入：训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中，$x_i \\in \\mathcal{X} \\subseteq R^n$为实例的特征向量，$y_i \\in \\mathcal{Y} = {c_1,c_2,\\cdots,c_K}为实例的类别$，$i = 1,2,\\cdots,N$；实例特征向量$x$输出：实例$x$所属的类$y$ 根据给出的距离度量，在训练集中找到和$x$最近的$k$个点，涵盖这$k$个点的$x$的邻域记作$N_k(x)$ 在$N_k(x)$中根据分类决策规则(如多数表决)决定$x$的类别$y$:$$y = arg\\ \\mathop{max}\\limits_{c_j}\\sum_{x_i \\in N_k(x)}I(y_i = c_j),i = 1,2,\\cdots,N;j = 1,2,\\cdots,K$$$I$为指示函数，当$y_i = c_j$时$I$为1，否则为0 k近邻模型的三要素$k$近邻法使用的模型实际上对应着对特征空间的划分，模型三要素为距离度量、$k$值的选择和分类决策规则 距离度量特征空间中两个实例点的距离是两个实例点相似程度的反映。设特征空间$\\mathcal{X}$是$n$维实数向量空间$R^n$，$x_i,x_j \\in \\mathcal{X},x_i = (x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(n)})^T,x_j = (x_j^{(1)},x_j^{(2)},\\cdots,x_j^{(n)})^T$ $L_p$距离：$$L_p(x_i,x_j) = \\left(\\sum_{l=1}^n|x_i^{(l)} - x_j^{(l)}|^p \\right)^{\\frac{1}{p}}$$ 欧式距离Euclidean distance：$p = 2$$$L_2(x_i,x_j) = \\left(\\sum_{l=1}^n|x_i^{(l)} - x_j^{(l)}|^2 \\right)^{\\frac{1}{2}}$$ 曼哈顿距离Manhattan distance：$p = 1$$$L_1(x_i,x_j) = \\sum_{l=1}^n|x_i^{(l)} - x_j^{(l)}|$$ 各个坐标距离的最大值：$p = \\infty$$$L_{\\infty}(x_i,x_j) = \\mathop{max}\\limits_l|x_i^{(l)} - x_j^{(l)}|$$ k值的选择 较小的$k$值：学习的近似误差会减小，估计误差会增大，预测结果会对邻近的实例点非常敏感，如果该点恰好是噪声，预测就会出错，也就是说$k$值的减小会使模型变得复杂，容易发生过拟合。 较大的$k$值：学习的近似误差会增大，估计误差会减小，也就是说$k$值的增大会使模型变得简单 一般使用交叉验证法来确定该值 分类决策规则 多数表决majority voting rule：如果分类的损失函数为0-1损失函数，分类函数为：$$f:R^n \\rightarrow {c_1,c_2,\\cdots,c_k}$$那么误分类的概率是$$P(Y \\neq f(X)) = 1 - P(Y = f(X))$$对于给定的实例$x \\in \\mathcal{X}$，其最邻近的$k$个训练实例点构成集合$N_k(x)$，如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是：$$\\frac{1}{k}\\sum_{x_i \\in N_k(x)}I(y_i \\neq c_j) = 1 - \\frac{1}{k}\\sum_{x_i \\in N_k(x)}I(y_i = c_j)$$要使误分类率最小即经验风险最小，就要使$\\sum_{x_i \\in N_k(x)}I(y_i = c_j)$最大，所以多数表决规则等价于经验风险最小化。 k近邻法的实现：kd树目的：对训练数据进行快速$k$近邻搜索 构造$kd$树输入：$k$维空间数据集$T = {x_1,x_2,\\cdots,x_N}$，其中$x_i = (x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(k)})^T,i = 1,2,\\cdots,N$输出：平衡$kd$树 构造根节点，使根节点对应于$k$维空间中包含所有实例点的超矩形区域； 对于深度为$j$的树结点，选择$x^{(l)}$为切分的坐标轴，$l = j(mod\\ k) + 1$，以该结点的区域中的所有实例点的$x^{(l)}$坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域，对应两个子结点，左子结点对应坐标$x^{(l)}$小于切分点的子区域，右子结点对应坐标$x^{(l)}$大于切分点的子区域，将落在切分超平面上的实例点保存在该结点； 重复第二步，直到两个子区域内没有实例点时终止； 搜索$kd$树输入：已构造的$kd$树，目标点$x$；输出：$x$的最近邻；更适用于训练实例数远大于空间维数的情况，平均计算复杂度为$O(\\log N)$ 在$kd$树中找到包含目标点$x$的叶结点：从根节点出发，递归的向下访问$kd$树。若目标点$x$的当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点，直到子结点为叶结点为止 以此叶结点为当前最近点 递归的向上回退，在每个结点进行以下操作：如果该结点保存的实例点比当前最近点距离目标更近，则以该实例点为当前最近点；当前的最近点一定存在于该结点一个子结点对应的区域，检查该子结点的父节点的另一个子结点对应的区域是否有更近的点，具体的，检查另一个子结点对应的区域是否与以目标点为球心，以目标点与当前最近点间的距离为半径的超球体相交，如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点，接着递归的进行最近邻搜索，如果不相交，向上回退， 当回退到根结点时，搜索结束，当前最近点即为$x$的最近邻点","link":"/blog/2022/09/14/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-03-k%E8%BF%91%E9%82%BB%E6%B3%95/"},{"title":"统计学习方法学习笔记-04-朴素贝叶斯法","text":"朴素贝叶斯的学习与分类，朴素贝叶斯的参数估计算法。 朴素贝叶斯法的学习与分类设输入空间$\\mathcal{X} \\subseteq R^n$为$n$维向量的集合，输出空间为类标记集合$\\mathcal{Y} = {c_1,c_2,\\cdots,c_K}$，输入为特征向量$x \\in \\mathcal{X}$，输出为类标记$y \\in \\mathcal{Y}$,$X$是定义在输入空间$\\mathcal{X}$上的随机向量，$Y$是定义在输出空间$\\mathcal{Y}$上的随机变量，$P(X,Y)$是$X$和$Y$的联合概率分布，训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$由$P(X,Y)$独立同分布产生。 先验概率分布：$$P(Y = c_k),k = 1,2,\\cdots,K$$ 条件概率分布：$$P(X = x|Y = c_k) = P(X^{(1)} = x^{(1)},\\cdots,X^{(n)} = x^{(n)}|Y = c_k),k = 1,2,\\cdots,K$$ 条件独立性假设下的概率分布：$$\\begin{aligned}P(X = x|Y = c_k) &amp;= P(X^{(1)} = x^{(1)},\\cdots,X^{(n)} = x^{(n)}|Y = c_k) \\&amp;= \\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)\\end{aligned}$$ 后验概率分布：$$\\begin{aligned}P(Y = c_k|X = x)&amp;= \\frac{P(X = x|Y = c_k)P(Y = c_k)}{\\sum_kP(X = x|Y = c_k)P(Y = c_k)} \\&amp;= \\frac{P(Y = c_k)\\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)}{\\sum_kP(Y = c_k)\\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)}\\end{aligned}$$ 朴素贝叶斯分类器：将实例分到后验概率最大的类中，这等价于期望风险最小化$$y = f(x) = arg \\mathop{max}\\limits_{c_k}\\frac{P(Y = c_k)\\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)}{\\sum_kP(Y = c_k)\\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)}$$分母与类别无关所以：$$y = f(x) = arg \\mathop{max}\\limits_{c_k}P(Y = c_k)\\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)$$ 朴素贝叶斯法的参数估计极大似然估计在朴素贝叶斯法中，学习意味着估计先验概率$P(Y = c_y)$和条件概率分布$P(X^{(j)} = x^{(j)}|Y = c_k)$ 先验概率的学习：$$P(Y = c_k) = \\frac{\\sum_{i = 1}^NI(y_i = c_k)}{N},k = 1,2,\\cdots,K$$ 条件概率的学习：$$P(X^{(j)} = a_{jl}|Y = c_k) = \\frac{\\sum_{i = 1}^NI(x_i^{(j)} = a_{jl},y_i = c_k)}{\\sum_{i = 1}^NI(y_i = c_k)} \\j = 1,2,\\cdots,n;\\ l = 1,2,\\cdots,S_j;\\ k = 1,2,\\cdots,K$$第$j$个特征$x^{(j)}$的可能取值的集合为${a_{j1},a_{j2},\\cdots,a_{jS_j}}$，$x_i^{(j)}$是第$i$个样本的第$j$个特征，$a_{jl}$是第$j$个特征可能取的第$l$个值，$I$是指示函数。 学习与分类算法 计算先验概率和条件概率 对于给定的实例$x = (x^{(1)},x^{(2)},\\cdots,x^{(n)})^T$，计算：$$P(Y = c_k)\\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)$$ 确定实例的类别$$y = arg \\mathop{max}\\limits_{c_k}P(Y = c_k)\\prod_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c_k)$$ 贝叶斯估计目的：用极大似然估计可能会出现所要估计的概率值为0的情况，解决的办法是采用贝叶斯估计 条件概率的贝叶斯估计是：$$P_\\lambda(X^{(j)} = a_{jl}|Y = c_k) = \\frac{\\sum_{i = 1}^NI(x_i^{(j)} = a_{jl},y_i = c_k) + \\lambda}{\\sum_{i = 1}^NI(y_i = c_k) + S_j\\lambda} \\$$式中$\\lambda \\geq 0$，当$\\lambda = 0$时就是极大似然估计，常取$\\lambda = 1$，这时称为拉普拉斯平滑 先验概率的贝叶斯估计：$$P_\\lambda(Y = c_k) = \\frac{\\sum_{i = 1}^NI(y_i = c_k) + \\lambda}{N + K\\lambda}$$分母中$\\lambda$前面的系数是用来保证概率和为1","link":"/blog/2022/09/14/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-04-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"},{"title":"统计学习方法学习笔记-05-决策树","text":"首先介绍决策树的基本概念，然后通过$ID3$和$C4.5$介绍特征的选择、决策树的生成以及决策树的修剪，最后介绍$CART$算法 决策树模型与学习 分类决策树模型的树结构有两种结点，内部结点表示一个特征或属性，叶结点表示一个类； 决策树所有的从根节点到叶结点的路径构成if-else规则集，这些规则是互斥且完备的； 决策树学习算法包含特征选择、决策树的生成和决策树的修剪 从可能的决策树中直接选取最优决策树是$NP$完全问题，现实中采用启发式方法学习次优的决策树 特征选择如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的，特征选择在于选取对训练数据具有分类能力的特征，特征选择的准则一般是信息增益或信息增益比 信息增益 设$X$是一个取有限个值的离散随机变量，其概率分布为$P(X = x_i) = p_i,i = 1,2,\\cdots,n$，则随机变量$X$的熵为：$$H(X) = -\\sum_{i = 1}^np_i \\log p_i$$熵越大，随机变量的不确定性越大，$0 \\leq H(p) \\leq \\log n$ 条件熵：$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望$$H(Y|X) = \\sum_{i = 1}^np_iH(Y|X = x_i)$$这里$p_i = P(X = x_i),i = 1,2,\\cdots,n$ 信息增益表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度，特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差：$$g(D,A) = H(D) - H(D|A)$$信息增益的算法输入：训练数据集$D$和特征$A$输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$设训练数据集为$D$，$|D|$表示其样本容量，即样本个数，设有$K$个类$C_k,k = 1,2,\\cdots,K,|C_k|$为属于类$C_k$的样本个数，$\\sum_{k = 1}^K|C_k| = |D|$，设特征$A$有$n$个不同的取值${a_1,a_2,\\cdots,a_n}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,\\cdots,D_n$，$|D_i|$为$D_i$的样本个数，$\\sum_{i = 1}^n|D_i| = |D|$，记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$即$D_{ik} = D_i \\bigcap C_k,|D_{ik}|$为$D_{ik}$的样本个数 计算数据集$D$的经验熵$H(D)$$$H(D) = -\\sum_{k = 1}^K\\frac{|C_k|}{|D|}\\log_2 \\frac{|C_k|}{|D|}$$ 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$$$H(D|A) = \\sum_{i = 1}^n\\frac{|D_i|}{|D|}H(D_i) = -\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}^K\\frac{|D_{ik}|}{|D_i|} \\log_2\\frac{|D_{ik}|}{|D_i|}$$ 计算信息增益$$g(D,A) = H(D) - H(D|A)$$ 信息增益比目的：以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比来校正 特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比$$\\begin{aligned}g_R(D,A) &amp;= \\frac{g(D,A)}{H_A(D)} \\&amp;= \\frac{g(D,A)}{-\\sum_{i = 1}^n\\frac{|D_i|}{D} \\log_2 \\frac{|D_i|}{D}}\\end{aligned}$$ 决策树的生成ID3算法输入：训练数据集$D$，特征集$A$，阈值$\\varepsilon$输出：决策树$T$ 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将类$C_k$作为该结点的类标记，返回$T$; 若$A = \\emptyset$，则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$; 否则计算各特征对$D$的信息增益，选择信息增益最大的特征$A_g$ 如果$A_g$的信息增益小于阈值$\\varepsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$; 否则，对$A_g$的每一个可能取值$a_i$，依$A_g = a_i$将$D$分割为非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$； 对第$i$个子结点，以$D_i$为训练集，以$A-{A_g}$为特征集，递归的调用第一步到第五步，得到子树$T_i$，返回$T_i$ C4.5生成算法输入：训练数据集$D$，特征集$A$，阈值$\\varepsilon$输出：决策树$T$ 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将类$C_k$作为该结点的类标记，返回$T$; 若$A = \\emptyset$，则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$; 否则计算各特征对$D$的信息增益比，选择信息增益比最大的特征$A_g$ 如果$A_g$的信息增益比小于阈值$\\varepsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$; 否则，对$A_g$的每一个可能取值$a_i$，依$A_g = a_i$将$D$分割为非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$； 对第$i$个子结点，以$D_i$为训练集，以$A-{A_g}$为特征集，递归的调用第一步到第五步，得到子树$T_i$，返回$T_i$ 决策树的剪枝目的：解决过拟合问题 剪枝时的损失函数设树$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该叶结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k = 1,2,\\cdots,K$，$H_t(T)$为叶结点$t$上的经验熵，$\\alpha \\geq 0$为参数，则决策树学习的损失函数可以定义为：$$C_\\alpha(T) = \\sum_{t = 1}^{|T|}N_tH_t(T) + \\alpha|T|$$其中经验熵为：$$H_t(T) = -\\sum_k\\frac{N_{tk}}{N_t}\\log \\frac{N_{tk}}{N_t}$$将式子左边记为：$$C(T) = \\sum_{t = 1}^{|T|}N_tH_t(T) = -\\sum_{t = 1}^{|T|}\\sum_{k = 1}^KN_{tk} \\log \\frac{N_{tk}}{N_t}$$有：$$C_\\alpha(T) = C(T) + \\alpha|T|$$$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$表示模型复杂度，参数$\\alpha$控制两者之间的影响，较大促使选择较简单的模型，较小促使选择较复杂的模型 树的剪枝算法当$\\alpha$确定时，选择损失函数最小的模型输入：生成算法产生的整个树$T$，参数$\\alpha$；输出：修剪后的子树$T_\\alpha$ 计算每个结点的经验熵 递归的从树的叶结点向上回缩，设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$与$T_A$，其对应的损失函数分别是$C_\\alpha(T_B)$与$C_\\alpha(T_A)$，如果$C_\\alpha(T_B) \\leq C_\\alpha(T_A)$，则进行剪枝，即将父结点变为新的叶结点。 返回第二步，直到不能继续为止，得到损失函数最小的子树$T_{\\alpha}$ CART算法classification and regression tree分类与回归树模型 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大； 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准； 回归树的生成假设输入$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集$D = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$输入：训练数据集$D$;输出：回归树$f(x)$，对应着将特征空间划分为$M$个单元$R_1,R_2,\\cdots,R_M$，在每个单元上有一个固定的输出值$c_m$; 选择最优切分变量$j$与切分点$s$，求解$$\\mathop{min}\\limits_{j,s}\\left[\\mathop{min}\\limits_{c_1}\\sum_{x_i \\in R_1(j,s)}(y_i - c_1)^2 +\\mathop{min}\\limits_{c_2}\\sum_{x_i \\in R_2(j,s)}(y_i - c_2)^2\\right]$$选择第$j$个变量$x^{(j)}$和它取的值$s$作为切分变量和切分点，定义两个区域$R_1(j,s) = {x|x^{(j)} \\leq s},R_2(j,s) = {x|x^{(j)} \\gt s}$,遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值的对$(j,s)$ 用选定的对$(j,s)$划分区域并决定相应的输出值：$$R_1(j,s) = {x|x^{(j)} \\leq s},R_2(j,s) = {x|x^{(j)} \\gt s} \\\\hat{c}m = \\frac{1}{N_m}\\sum{x_i \\in R_m(j,s)}y_i,x \\in R_m,m=1,2$$ 继续对两个子区域调用步骤1,2，直到满足停止条件； 将输入空间划分为$M$个区域$R_1,R_2,\\cdots,R_M$，生成决策树：$$f(x) = \\sum_{m = 1}^M\\hat{c}_mI(x \\in R_m)$$ 分类树的生成分类树使用基尼指数选择最优特征，同时决定该特征的最优二值切分点 基尼指数分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义：$$Gini(p) = \\sum_{k = 1}^Kp_k(1-p_k) = 1 - \\sum_{k = 1}^Kp_k^2$$对于给定的样本集合$D$，其基尼指数为：$$Gini(D) = 1 - \\sum_{k = 1}^K\\left(\\frac{|C_k|}{|D|}\\right)^2$$$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数。如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，即：$$D_1 = {(x,y) \\in D|A(x) = a},D_2 = D - D_1$$则在特征$A$的条件下，集合$D$的基尼指数定义为：$$Gini(D,A) = \\frac{|D_1|}{|D|}Gini(D_1) + \\frac{|D_2|}{|D|}Gini(D_2)$$基尼指数$Gini(D)$表示集合$D$的不确定性，基尼指数$Gini(D,A)$表示经$A = a$分割后集合$D$的不确定性，基尼指数值越大，样本集合的不确定性越大。 CART生成算法输入：训练数据集$D$，停止计算的条件；输出：CART决策树； 设节点的训练数据集为$D$，计算现有特征对该数据集的基尼指数，此时，对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A = a$的测试为“是”或“否”将$D$分割成$D_1$和$D_2$两部分，计算$A = a$的基尼指数 在所有可能额特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现有结点生成两个子结点，将训练数据集依特征分配到两个子结点中去； 对两个子结点递归的调用1,2步，直至满足停止条件 生成CART决策树算法停止计算的条件是节点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值(样本基本属于同一类)，或者没有更多特征。 CART剪枝首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列${T_0,T_1,\\cdots,T_n}$，之后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树 剪枝，形成一个子树序列 子树的损失函数：$$C_\\alpha(T) = C(T) + \\alpha|T|$$ 对整体树$T_0$开始剪枝，对$T_0$的任意内部结点$t$，以$t$为单结点树的损失函数是：$$C_\\alpha(t) = C(t) + \\alpha$$ 以$t$为根结点的子树$T_t$的损失函数是：$$C_\\alpha(T_t) = C(T_t) + \\alpha|T_t|$$ 当$\\alpha = 0$时，有不等式：$$C_\\alpha(T_t) = C_\\alpha(t)$$ 当$\\alpha$不断增大时，不等式反向 在$\\alpha = \\frac{C(t) - C(T_t)}{|T_t| - 1}$时，$T_t$和$t$有相同的损失函数值，也就是单个节点的损失函数值和一颗子树的损失函数值相同，所以这课子树可以被剪掉 对$T_0$中的每一个内部结点进行计算：$$g(t) = \\frac{C(t) - C(T_t)}{|T_t| - 1}$$在$T_0$中减去$g(t)$最小的子树$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\\alpha_1$，$T_1$为区间$[\\alpha_1,\\alpha_2)$的最优子树，如此剪枝下去，不断增加$\\alpha$的值，产生了新的区间。 在剪枝得到的子树序列中通过交叉验证选取最优子树 使用独立的验证数据集，测试子树序列$T_0,T_1,\\cdots,T_n$中各棵子树的平方误差或基尼指数 平方误差或基尼指数最小的决策树被称为最优的决策树$T_\\alpha$ 子树和$\\alpha$是一一对应的 CART剪枝算法输入：CART算法生成的决策树$T_0$输出：最优决策树$T_\\alpha$ 设$k = 0,T = T_0$； 设$\\alpha = +\\infty$； 自下而上的对各内部节点$t$计算:$$g(t) = \\frac{C(t) - C(T_t)}{|T_t| - 1} \\\\alpha = min(\\alpha,g(t))$$ 对$g(t) = \\alpha$的内部结点$t$进行剪枝，并对叶结点$t$以多数表决法决定其类，得到树$T$； 设$k = k + 1, \\alpha_k = \\alpha,T_k = T$； 如果$T_k$不是由根结点及两个叶结点构成的树，则回到第二步，否则$T_k = T_n$； 采用交叉验证法在子树序列$T_0,T_1,\\cdots,T_n$中选取最优子树$T_\\alpha$；","link":"/blog/2022/09/15/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-05-%E5%86%B3%E7%AD%96%E6%A0%91/"},{"title":"统计学习方法学习笔记-06-逻辑斯谛回归与最大熵模型01","text":"首先介绍逻辑斯谛模型，然后介绍最大熵模型，最后讲述逻辑斯谛回归与最大熵模型的学习算法，包括改进的迭代尺度算法和拟牛顿法 逻辑斯谛回归模型逻辑斯谛分布设$X$是连续随机变量，具有下列分布函数和密度函数：$\\mu$是位置参数，$\\gamma \\gt 0$是形状参数，越小，分布函数在中心增长得越快$$F(x) = P(X \\leq x) = \\frac{1}{1+e^{-(x - \\mu) / \\gamma}} \\f(x) = F’(x) = \\frac{e^{-(x - \\mu) / \\gamma}}{\\gamma(1 + e^{-(x - \\mu) / \\gamma})^2}$$曲线如下：分布函数$F(x)$是一条$S$形曲线，该曲线以点$(\\mu,\\frac{1}{2})$为中心对称：$$F(-x + \\mu) - \\frac{1}{2} = -F(x + \\mu) + \\frac{1}{2}$$ 二项逻辑斯谛回归模型 二项逻辑斯谛回归模型是一种分类模型，$x \\in R^n$是输入，$Y \\in {0,1}$，$\\omega \\in R^n,b\\in R$是参数，$\\omega$是权值向量，$b$是偏置，$\\omega \\cdot x$是$\\omega,x$的内积模型是如下的条件概率分布：$$P(Y = 1|x) = \\frac{exp(\\omega \\cdot x + b)}{1 + exp(\\omega \\cdot x + b)} \\P(Y = 0|x) = \\frac{1}{1 + exp(\\omega \\cdot x + b)}$$ 为了方便将权值向量和输入向量进行扩充：$\\omega = (\\omega^{(1)},\\omega^{(2)},\\cdots,\\omega^{(n)},b)^T,x = (x^{(1)},x^{(2)},\\cdots,x^{(n)},1)^T$，模型如下：$$P(Y = 1|x) = \\frac{exp(\\omega \\cdot x)}{1 + exp(\\omega \\cdot x)} \\P(Y = 0|x) = \\frac{1}{1 + exp(\\omega \\cdot x)}$$ 一个事件的几率：该事件发生与不发生的概率的比值$\\frac{p}{1-p}$ 该事件的对数几率是：$logit(p) = \\log \\frac{p}{1-p}$ 逻辑斯谛回归模型输出$Y = 1$的对数几率是$\\log \\frac{P(Y = 1|x)}{1 - P(Y = 1|x)} = \\omega \\cdot x$，是输入$x$的线性函数，通过逻辑斯谛回归模型可以将线性函数$\\omega \\cdot x$转换为概率 模型参数估计目的：逻辑斯谛回归模型学习$\\omega$的估计值，可以使用极大似然估计给定训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in R^n,y \\in {0,1}$设：$$P(Y = 1|x) = \\pi(x),P(Y = 0|x) = 1 - \\pi(x)$$似然函数为：$$\\prod_{i = 1}^N[\\pi(x_i)]^{y_i}[1 - \\pi(x_i)]^{1 - y_i}$$对数似然函数为：$$\\begin{aligned}L(\\omega)&amp;= \\sum_{i = 1}^N[y_i \\log \\pi(x_i) + (1 - y_i) \\log (1 - \\pi(x_i))] \\&amp;= \\sum_{i = 1}^N\\left[y_i\\log \\frac{\\pi(x_i)}{1 - \\pi(x_i)} + \\log(1 - \\pi(x_i))\\right] \\&amp;= \\sum_{i = 1}^N[y_i(\\omega \\cdot x_i) - \\log(1 + exp(\\omega \\cdot x_i))]\\end{aligned}$$对$L(\\omega)$求极大值，得到$\\omega$的估计值问题转化为以对数似然函数为目标函数的最优化问题，可以采用梯度下降法及拟牛顿法 多项逻辑斯谛回归此时离散型随机变量$Y$的取值集合是${1,2,\\cdots,K}$，模型为：$$P(Y = k|x) = \\frac{exp(\\omega_k \\cdot x)}{1 + \\sum_{k = 1}^{K - 1}exp(\\omega_k \\cdot x)},k = 1,2,\\cdots,K-1 \\P(Y = K|x) = \\frac{1}{1 + \\sum_{k = 1}^{K - 1}exp(\\omega_k \\cdot x)}$$$x \\in R^{n + 1},\\omega_k \\in R^{n + 1}$ 最大熵模型最大熵模型由最大熵原理推导完成，首先叙述一般的最大熵原理，然后讲解最大熵模型的推导，最后给出最大熵模型学习的形式 最大熵原理学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型，假设离散随机变量$X$的概率分布是$P(X)$，其熵是：$$H(P) = -\\sum_xP(x)\\log P(x)$$熵满足$0 \\leq H(P) \\leq \\log |X|$，$|X|$是$X$的取值个数，当$X$的分布是均匀分布时右边的等号成立，也就是说$X$服从均匀分布时熵最大；直观的，最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件，在没有更多信息的情况下，那些不确定的部分必须是等可能的，最大熵原理通过熵的最大化来表示等可能性 最大熵模型的定义假设分类模型是一个条件概率分布$P(Y|X),X \\in \\mathcal{X} \\subseteq R^n$表示输入，$Y \\in \\mathcal{Y}$表示输出，$\\mathcal{X},\\mathcal{Y}$分别是输入和输出的集合，这个模型表示的是对于给定的输入$X$，以条件概率$P(Y|X)$输出$Y$，训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，学习的目标是用最大熵原理选择最好的分类模型。$$\\tilde{P}(X = x,Y = y) = \\frac{\\nu(X = x,Y = y)}{N} \\\\tilde{P}(X = x) = \\frac{\\nu(X = x)}{N}$$$\\nu(X = x,Y = y)$表示的是训练样本中$(x,y)$出现的频数，$\\nu(X = x)$表示训练数据中输入$x$出现的频数，$N$表示样本容量。特征函数$f(x,y)$描述输入$x$和输出$y$之间的某一个事实：$$f(x,y) =\\begin{cases}1 &amp; x和y满足某一事实\\0 &amp; otherwise\\end{cases}$$特征函数$f(x,y)$关于经验分布$\\tilde{P}(X,Y)$的期望值：$$E_{\\tilde{P}}(f) = \\sum_{x,y}\\tilde{P}(x,y)f(x,y)$$特征函数$f(x,y)$关于模型$P(Y|X)$与经验分布$\\tilde{P}(X)$的期望值：$$E_(f) = \\sum_{x,y}\\tilde{P}(x)P(y|x)f(x,y)$$如果模型能获取训练数据中的信息，那么就可以假设这两个期望值相等$$E_{\\tilde{P}}(f) = E_(f) \\\\sum_{x,y}\\tilde{P}(x,y)f(x,y) = \\sum_{x,y}\\tilde{P}(x)P(y|x)f(x,y)$$我们将上式作为模型学习的约束条件，假如有$n$个特征函数$f_i(x,y),i = 1,2,\\cdots,n$，那么就有$n$个约束条件假设满足所有约束条件的模型集合为：$$\\mathcal{C} \\equiv {P \\in \\mathcal{P}|E_P(f_i) = E_{\\tilde{P}}(f_i),i = 1,2,\\cdots,n}$$定义在条件概率分布上的条件熵为：$$H(P) = -\\sum_{x,y}\\tilde{P}(x)P(y|x) \\log P(y |x)$$集合模型$\\mathcal{C}$中条件熵$H(P)$最大的模型称为最大熵模型 最大熵模型的学习(P100例题值得一看)最大熵模型的学习过程就是求解最大熵模型的过程，最大熵模型的学习等价于约束最优化问题：$$\\mathop{max}\\limits_{P \\in \\mathcal{C}}H(P) = -\\sum_{x,y}\\tilde{P}(x)P(y|x) \\log P(y |x) \\\\begin{aligned}s.t. \\ \\ \\ &amp; E_P(f_i) = E_{\\tilde{P}}(f_i),i = 1,2,\\cdots,n \\&amp; \\sum_{y}P(y|x) = 1\\end{aligned}$$转化为等价的求最小值问题：$$\\mathop{min}\\limits_{P \\in \\mathcal{C}}-H(P) = \\sum_{x,y}\\tilde{P}(x)P(y|x) \\log P(y |x) \\\\begin{aligned}s.t. \\ \\ \\ &amp; E_P(f_i) - E_{\\tilde{P}}(f_i) = 0,i = 1,2,\\cdots,n \\&amp; \\sum_{y}P(y|x) = 1\\end{aligned}$$将约束最优化的原始问题转换为无约束最优化的对偶问题，首先引进拉格朗日乘子$\\omega_0,\\omega_1,\\omega_2,\\cdots,\\omega_n$，定义拉格朗日函数$L(P,\\omega)$:$$\\begin{aligned}L(P,\\omega) &amp;\\equiv -H(P) + \\omega_0\\left(1 - \\sum_yP(y|x)\\right) + \\sum_{i = 1}^n\\omega_i(E_{\\tilde{P}}(f_i) - E_P(f_i)) \\&amp;= \\sum_{x,y}\\tilde{P}(x)P(y|x) \\log P(y |x) + \\omega_0\\left(1 - \\sum_yP(y|x)\\right) + \\sum_{i = 1}^n\\omega_i\\left(\\sum_{x,y}\\tilde{P}(x,y)f_i(x,y) - \\sum_{x,y}\\tilde{P}(x)P(y|x)f_i(x,y)\\right)\\end{aligned}$$最优化的原始问题是：$$\\mathop{min}\\limits_{P \\in \\mathcal{C}} \\mathop{max}\\limits_{\\omega} L(P,\\omega)$$对偶问题是：$$\\mathop{max}\\limits_{\\omega} \\mathop{min}\\limits_{P \\in \\mathcal{C}} L(P,\\omega)$$由于拉格朗日函数是$P$的凸函数，原始问题的解与对偶问题的解是等价的首先求解对偶问题内部的极小化问题$\\mathop{min}\\limits_{P \\in \\mathcal{C}} L(P,\\omega)$，$\\mathop{min}\\limits_{P \\in \\mathcal{C}} L(P,\\omega)$是$\\omega$的函数，将其记作$$\\Psi(\\omega) = \\mathop{min}\\limits_{P \\in \\mathcal{C}} L(P,\\omega) = L(P_{\\omega},\\omega)$$将其解记作：$$P_\\omega = arg \\mathop{min}\\limits_{P \\in \\mathcal{C}} L(P,\\omega) = P_\\omega(y|x)$$具体地，求$L(P,\\omega)$对$P(y|x)$的偏导数$$\\begin{aligned}\\frac{\\partial L(P,\\omega)}{\\partial P(y|x)}&amp;= \\left(\\sum_{x,y}\\tilde{P}(x)P(y|x) \\log P(y |x) + \\omega_0\\left(1 - \\sum_yP(y|x)\\right) + \\sum_{i = 1}^n\\omega_i\\left(\\sum_{x,y}\\tilde{P}(x,y)f_i(x,y) - \\sum_{x,y}\\tilde{P}(x)P(y|x)f_i(x,y)\\right)\\right)’{P(y|x)} \\&amp;= \\sum{x,y}\\tilde{P}(x)(1 + \\log P(y |x)) + \\sum_y\\omega_0 + \\sum_{x,y}\\left(\\tilde{P}(x)\\sum_{i = 1}^n\\omega_if_i(x,y)\\right) \\&amp;= \\sum_{x,y}\\tilde P(x) \\left( \\log P(y|x) + 1 - \\omega_0 - \\sum_{i = 1}^n \\omega_if_i(x,y)\\right)\\end{aligned}$$令偏导数等于0，在$\\tilde P(x) \\gt 0$的情况下解得$$P(y|x) = exp \\left( \\sum_{i = 1}^n \\omega_if_i(x,y) + \\omega_0 - 1 \\right) = \\frac{exp \\left( \\sum_{i = 1}^n \\omega_if_i(x,y) \\right)}{exp(1 - \\omega_0)}$$由于$\\sum_y P(y|x) = 1$:$$P_\\omega(y|x) = \\frac{1}{Z_\\omega(x)}exp \\left( \\sum_{i = 1}^n \\omega_if_i(x,y) \\right)$$其中：$Z_\\omega(x)$被称为规范化因子$$Z_\\omega(x) = \\sum_yexp \\left( \\sum_{i = 1}^n \\omega_if_i(x,y) \\right)$$$P_\\omega = P_\\omega(y|x)$就是最大熵模型之后将求解得到的最大熵模型带到拉格朗日函数中得到包含$\\omega$的函数，求关于$\\omega$的极大化问题，分别对$\\omega_1,\\omega_2,\\cdots,\\omega_n$求导，令偏导数为0求出$\\omega$的值，将得出的$\\omega$值带到最大熵模型中得到最大熵模型的结果。","link":"/blog/2022/09/15/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-06-%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B01/"},{"title":"学术写作规范-论文结构","text":"介绍论文写作中需要注意的一些地方，从学术论文写作课程上面总结而来 题目 当介绍文章时一句话可以概括文章内容的，需要足够吸睛， 不能太泛了，需要体现文章的特点，需要是具体的 最好不要题目太长，可以使用副标题，冒号后面的就是副标题， 首字母大写，冠词、介词、连词除外 可用动名词，不要动词 可以使用疑问句 摘要 目的是提供内容梗概，本身就是一篇高度浓缩的论文，需要充分反映研究的亮点，拥有与论文等量的主要信息，即不阅读论文就可以获得必要的信息； 总体上适用imrad结构 关键词 并非关键的词 又有一个英文翻译indexing word 目的是编制索引，标识同行，分配评阅人 所以需要反映学科的方向，标识同行，不要使用太宽泛的词 引言 目的：让人明白你解决了一个重要问题，定位是大同行能懂 核心任务：定义问题（清晰、必要） – 领域背景：语境，代入– 领域现状：提共性需求– 问题凝练– 挑战 思路 贡献（结果、结论、技术创新） 文章结构提问题的常见思路 破：质疑现有的工作的基本假设，釜底抽薪 立：研究别人忽视的方面，提新问题 补：揭示现有工作的gap，逐步改良 示例 xx技术已被广泛使用，对它的研究非常活跃，但是有一个很重要的方面被长期忽视了。因此，我们聚焦于这个方面，致力于回答xxx问题。基于xxx方法，我们发现xxx。具体贡献如下：xxx。 方法method文章是怎么做的，主要表现形式是系统、算法和证明 结果result发现了什么，实验性结果，细分为两个部分 报告实验结果，客观、详细、完整 点评实验数据 讨论discussion/conclusion意味着什么，是对整个文章的总结。 目标完成情况，首尾呼应 对结果做出解释，并进行综合、推理和归纳，反映事物内在联系 与其他研究结果的比较，提出导致新结果的可能原因 分析本次研究的不足，提出open问题和未来方向 讨论应用价值和影响 只有这里才可以出现有主观性的话","link":"/blog/2022/09/17/%E5%AD%A6%E6%9C%AF%E5%86%99%E4%BD%9C%E8%A7%84%E8%8C%83-%E8%AE%BA%E6%96%87%E7%BB%93%E6%9E%84/"},{"title":"自然语言处理学习笔记-lecture3-隐马尔科夫模型","text":"首先讲解了马尔科夫模型，之后讲述了马尔科夫模型的学习过程，之后深入到隐马尔可夫模型，涉及到观测序列概率的计算和在给定观测序列的情况下最可能的隐状态序列，最后给出了隐马尔可夫模型的学习 马尔科夫模型状态集合：$\\mathcal{S} = {s_1,\\cdots,s_N}$观测状态序列：$x = x_1,\\cdots,x_t,\\cdots,x_T$,其中$x_t \\in \\mathcal{S}$状态初始化概率：$\\pi_i = p(x_1 = s_i),1 \\leq i \\leq N$状态转移概率：$a_{ij} = p(x_t = s_j|x_{t - 1} = s_i),1 \\leq i,j \\leq N$计算观测状态序列的概率(假设当前的状态$x_t$的生成只依赖于前一个状态$x_{t-1}$，通常称为二阶马尔科夫模型):$$\\begin{aligned}P(x;\\theta)&amp;= \\prod_{t = 1}^Tp(x_t|x_1,\\cdots,x_{t - 1}) \\&amp;\\approx p(x_1) \\times \\prod_{t = 2}^Tp(x_t|x_{t - 1})\\end{aligned}$$其中$\\theta = {p(x)|x \\in \\mathcal{S}} \\bigcup {p(x’|x)|x,x’ \\in \\mathcal{S}}$ 模型的学习目的：学习得到模型的参数$\\theta$，也即状态初始化概率和状态转移概率的学习，通过极大似然估计完成参数学习假设训练集包含$D$个样本$\\mathscr{D} = {x^{(d)}}^D_{d = 1}$，使用极大似然估计来从训练数据中自动获取最优模型参数：$$\\hat{\\theta} = arg \\mathop{max}\\limits_{\\theta}{L(\\theta)}$$似然函数的对数形式：$$\\begin{aligned}L(\\theta)&amp;= \\sum_{d = 1}^D \\log P(x^{(d)};\\theta) \\&amp;= \\sum_{d = 1}^D\\left(\\log p(x_1^{(d)}) + \\sum_{t = 2}^{T^{(d)}} \\log p(x_t^{(d)}|x_{t - 1}^{(d)})\\right)\\end{aligned}$$其中$T^{(d)}$表示第$d$个训练数据的序列长度，模型参数还需要满足一下两个约束条件：$$\\sum_{x \\in \\mathcal{S}} p(x) = 1 \\\\forall x:\\sum_{x’ \\in \\mathcal{S}}p(x’|x) = 1$$引入拉格朗日乘子法来实现约束条件下的极值求解：$$J(\\theta,\\lambda,\\gamma) = L(\\theta) - \\lambda\\left(\\sum_{x \\in \\mathcal{S}} p(x) - 1 \\right) - \\sum_{x \\in \\mathcal{S}} \\gamma_x \\left( \\sum_{x’ \\in \\mathcal{S}} p(x’|x) - 1\\right)$$其中$\\lambda$是与状态初始概率约束相关的拉格朗日乘子，$\\gamma = {\\gamma_x|x \\in \\mathcal{S}}$是与状态转移概率约束相关的拉格朗日乘子的集合 首先计算状态初始概率$p(x)$的偏导$$\\begin{aligned}\\frac{\\partial J(\\theta,\\lambda,\\gamma)}{\\partial p(x)}&amp;= \\left(\\sum_{d = 1}^D\\left(\\log p(x_1^{(d)}) + \\sum_{t = 2}^{T^{(d)}} \\log p(x_t^{(d)}|x_{t - 1}^{(d)})\\right) - \\lambda\\left(\\sum_{x \\in \\mathcal{S}} p(x) - 1 \\right) - \\sum_{x \\in \\mathcal{S}} \\gamma_x \\left( \\sum_{x’ \\in \\mathcal{S}} p(x’|x) - 1\\right)\\right)’{p(x)} \\&amp;= \\frac{\\partial}{\\partial p(x)} \\sum{d = 1}^D \\log p(x_1^{(d)}) - \\lambda \\&amp;= \\frac{1}{p(x)} \\sum_{d = 1}^D \\delta(x_1^{(d)},x) - \\lambda\\end{aligned}$$其中$\\delta(a,b)$的取值当$a = b$时为1，否则为0用$c(x,\\mathscr{D})$表示训练数据中第一个状态是$x$的次数：$$c(x,\\mathscr{D}) = \\sum_{d = 1}^D \\delta(x_1^{(d)},x)$$之后计算拉格朗日乘子$\\lambda$的偏导$$\\frac{\\partial J(\\theta,\\lambda,\\gamma)}{\\partial \\lambda} = \\sum_{x \\in \\mathcal{S}}p(x) - 1$$根据上式可以推导：$$p(x) = \\frac{c(x,\\mathscr{D})}{\\lambda}$$有$\\lambda = \\sum_{x \\in \\mathcal{S}}c(x,\\mathscr{D})$所以状态初始化概率的估计公式：$$p(x) = \\frac{c(x,\\mathscr{D})}{\\sum_{x’ \\in \\mathcal{S}}c(x’,\\mathscr{D})}$$ 计算状态转移概率的偏导$$\\begin{aligned}\\frac{\\partial J(\\theta,\\lambda,\\gamma)}{\\partial p(x’|x)}&amp;= \\left(\\sum_{d = 1}^D\\left(\\log p(x_1^{(d)}) + \\sum_{t = 2}^{T^{(d)}} \\log p(x_t^{(d)}|x_{t - 1}^{(d)})\\right) - \\lambda\\left(\\sum_{x \\in \\mathcal{S}} p(x) - 1 \\right) - \\sum_{x \\in \\mathcal{S}} \\gamma_x \\left( \\sum_{x’ \\in \\mathcal{S}} p(x’|x) - 1\\right)\\right)’{p(x’|x)} \\&amp;= \\frac{\\partial}{\\partial p(x’|x)} \\sum{d = 1}^D \\sum_{t = 2}^{T^{(d)}} \\log p\\left(x_t^{(d)}|x_{t - 1}^{(d)}\\right) - \\gamma_x \\&amp;= \\frac{1}{p(x’|x)} \\sum_{d = 1}^D \\sum_{t = 2}^{T^{(d)}} \\delta(x_{t - 1}^{(d)},x) \\delta(x_t^{(d)},x’) - \\gamma_x\\end{aligned}$$将训练数据中$x’$紧跟着出现在$x$后面的次数表示为：$$c(x,x’,\\mathscr{D}) = \\sum_{d = 1}^D \\sum_{t = 2}^{T^{(d)}} \\delta(x_{t - 1}^{(d)},x) \\delta(x_t^{(d)},x’)$$之后计算拉格朗日乘子$\\gamma_x$的偏导：$$\\frac{\\partial J(\\theta,\\lambda,\\gamma)}{\\partial \\gamma_x} = \\sum_{x’ \\in \\mathcal{S}}p(x’|x) - 1$$从上式可以推出：$$p(x’|x) = \\frac{c(x,x’,\\mathscr{D})}{\\gamma_x}$$又可以得出$\\gamma_x = \\sum_{x’’ \\in \\mathcal{S}}c(x,x’’,\\mathscr{D})$，故：$$p(x’|x) = \\frac{c(x,x’,\\mathscr{D})}{\\sum_{x’’ \\in \\mathcal{S}}c(x,x’’,\\mathscr{D})}$$ 计算观测概率目的：计算观测序列的概率一个例子：在暗室中不知道外面的天气，但是可以通过触摸地面的潮湿程度来推测外部的天气情况，此时，地面的潮湿程度是观测状态，外面的天气是隐状态观测状态集合：$\\mathscr{O} = {o_1,\\cdots,o_m}$隐状态集合：$\\mathcal{S} = {s_1,\\cdots,s_N}$观测状态序列：$x = x_1,\\cdots,x_t,\\cdots,x_T$隐状态序列：$z = z_1,\\cdots,z_t\\cdots,z_T$隐状态初始化概率：$\\pi_i = p(z_1 = s_i),1 \\leq i \\leq N$隐状态转移概率：$a_{ij} = p(z_t = s_j|z_{t - 1} = s_i),1 \\leq i,j \\leq N$观测状态生成概率：$b_j(k) = p(x_t = o_k|z_t = s_j),1 \\leq j \\leq N \\bigwedge 1 \\leq k \\leq M$ 隐马尔可夫模型：$$\\begin{aligned}P(x;\\theta)&amp;= \\sum_zP(x,z;\\theta) \\&amp;= \\sum_zp(z_1) \\times p(x_1|z_1) \\times \\prod_{t = 2}^Tp(z_t|z_{t - 1}) \\times p(x_t|z_t)\\end{aligned}$$模型参数$\\theta = {p(z)|z \\in \\mathcal{S}} \\bigcup {p(z’|z)|z,z’ \\in \\mathcal{S}} \\bigcup {p(x|z)|x \\in \\mathscr{O} \\bigwedge z \\in \\mathcal{S}}$ 前向概率部分观测状态序列$x_1,\\cdots,x_t$与第$t$个隐状态为$s_i$的联合概率称为前向概率：$$\\alpha_t(i) = P(x_1,\\cdots,x_t,z_t = s_i;\\theta)$$使用动态规划算法递归计算： 初始化：$t = 1$$$\\alpha_1(i) = \\pi_ib_i(x_1),1 \\leq i \\leq N$$ 递归：$t = 2,\\cdots,T$$$\\alpha_t(j) = \\left( \\sum_{i = 1}^N\\alpha_{t - 1}(i)a_{ij}\\right)b_j(x_t),1 \\leq j \\leq N$$ 终止：$$P(x;\\theta) = \\sum_{i = 1}^N\\alpha_T(i)$$ 后向概率第$t$个隐状态为$s_j$生成部分观测状态序列$x_{t + 1},\\cdots,x_T$的条件概率称为后向概率，定义为：$$\\beta_t(i) = P(x_{t + 1},\\cdots,x_T|z_t = s_i;\\theta)$$使用动态规划算法递归计算如下： 初始化：$t = T$$$\\beta_T(i) = 1,1 \\leq i \\leq N$$ 递归：$t = T - 1,\\cdots,1$$$\\beta_t(i) = \\sum_{j = 1}^Na_{ij}b_j(x_{t + 1})\\beta_{t + 1}(j),1 \\leq i \\leq N$$ 终止：$$P(x;\\theta) = \\sum_{i = 1}^N\\pi_ib_i(x_1)\\beta_1(i)$$ 计算最优隐状态序列-Viterbi算法目的：在给定一个观测状态序列$x = x_1,\\cdots,x_t,\\cdots,x_T$和模型参数$\\theta$的条件下求出最优的隐状态序列$$\\begin{aligned}\\hat{z}&amp;= arg \\mathop{max}\\limits_z\\left{P(z|x;\\theta)\\right} \\&amp;= arg \\mathop{max}\\limits_z\\left{\\frac{P(x,z;\\theta)}{P(x;\\theta)}\\right} \\&amp;= arg \\mathop{max}\\limits_z\\left{P(x,z;\\theta)\\right} \\&amp;= arg \\mathop{max}\\limits_z\\left{\\sum_zp(z_1) \\times p(x_1|z_1) \\times \\prod_{t = 2}^Tp(z_t|z_{t - 1}) \\times p(x_t|z_t)\\right} \\\\end{aligned}$$假设$\\delta_i = \\mathop{max}\\limits_{j \\in heads(i)}{\\omega_{ji}\\delta_j}$是从结点1到结点$i$的最大路径取值，$\\psi_i = arg \\mathop{max}\\limits_{j \\in heads(i)}{\\omega_{ji}\\delta_j}$ Viterbi算法 初始化：$$\\delta_1(i) = \\pi_ib_1(x_1),\\psi_1(i) = 0$$ 递归：$t = 2,\\cdots,T$$$\\delta_t(j) = \\mathop{max}\\limits_{1 \\leq i \\leq N}{\\delta_{t - 1}(i)a_{ij}}b_j(x_t) \\\\psi_t(j) = arg \\mathop{max}\\limits_{1 \\leq i \\leq N}{\\delta_{t - 1}(i)a_{ij}}b_j(x_t)$$ 结束：$$\\hat{P} = \\mathop{max}\\limits_{1 \\leq i \\leq N}{\\delta_{T}(i)} \\\\hat{z}T = arg \\mathop{max}\\limits{1 \\leq i \\leq N}{\\delta_{T}(i)}$$ 回溯：$t = T - 1,\\cdots,1$$$\\hat{z}t = \\psi{t + 1}(\\hat{z}_{t + 1})$$ 模型的学习-前向后向算法目的：估计模型参数，我们知道的是观测序列，隐状态序列是不确定的，所以参数的主要挑战是需要对指数级的隐状态序列进行求和给定训练集$\\mathscr{D} = {x^{(d)}}^D_{d = 1}$，使用极大似然估计来获得模型的最优参数：$$\\hat{\\theta} = arg \\mathop{max}\\limits_{\\theta}{L(\\theta)}$$Expectation-Maximization(简称EM)算法被广泛用于估计隐状态模型的参数。令$\\mathbf{X}$表示一组观测数据，$\\mathbf{Z}$表示未观测数据，也就是隐状态序列：EM算法在以下两个步骤中迭代运行： E步：确定对数似然的期望值$$\\mathbf{Q}(\\theta|\\theta^{old}) = \\mathbb{E}_{\\mathbf{Z}|\\mathbf{X};\\theta^{old}}\\left[\\log P(\\mathbf{X},\\mathbf{Z};\\theta)\\right]$$ M步：计算最大化该期望值的参数$$\\theta^{new} = arg \\mathop{max}\\limits_{\\theta}\\left{\\mathbf{Q}(\\theta|\\theta^{old})\\right}$$那么使用EM算法来训练隐马尔可夫模型，在E步实际使用的目标函数定义如下：$$\\begin{aligned}J(\\theta,\\lambda,\\gamma,\\phi)&amp;= \\sum_{d = 1}^D\\mathbb{E}{\\mathbf{Z}|\\mathbf{X}^{(d)};\\theta^{old}}\\left[\\log P(\\mathbf{x}^{(d)},\\mathbf{Z};\\theta)\\right] \\&amp;- \\lambda\\left(\\sum{z \\in \\mathcal{S}}p(z) - 1\\right) \\&amp;- \\sum_{z \\in \\mathcal{S}}\\gamma_z\\left(\\sum_{z’ \\in \\mathcal{S}}p(z’|z) - 1\\right) \\&amp;- \\sum_{z \\in \\mathcal{S}}\\phi_z\\left(\\sum_{x \\in \\mathscr{O}}p(x|z) - 1\\right)\\end{aligned}$$通过计算偏导，可以得到公式：$$p(z) = \\frac{c(z,\\mathscr{D})}{\\sum_{z’ \\in \\mathcal{S}}c(z’,\\mathscr{D})} \\p(z’|z) = \\frac{c(z,z’,\\mathscr{D})}{\\sum_{z’’ \\in \\mathcal{S}}c(z,z’’,\\mathscr{D})} \\p(x|z) = \\frac{c(z,x,\\mathscr{D})}{\\sum_{x’ \\in \\mathscr{O}}c(z,x’,\\mathscr{D})}$$其中$c(\\cdot)$表示计数函数，$c(z,\\mathscr{D})$是在训练集$\\mathscr{D}$上第一个隐状态是$z$的次数的期望值，$c(z,z’,\\mathscr{D})$是在训练集上隐状态$z’$出现在隐状态$z$的次数的期望值，$c(z,x,\\mathscr{D})$是训练集上隐状态$z$生成观测状态$x$的次数的期望值。上述的期望值定义如下：$$\\begin{aligned}c(z,\\mathscr{D}) &amp;\\equiv \\sum_{d = 1}^D\\mathbb{E}{\\mathbf{Z}|\\mathbf{X};\\theta^{old}}\\left[\\delta(z_1,z)\\right] \\c(z,z’,\\mathscr{D}) &amp;\\equiv \\sum{d = 1}^D\\mathbb{E}{\\mathbf{Z}|\\mathbf{X};\\theta^{old}}\\left[\\sum{t = 2}^{T^{(d)}}\\delta(z_{t - 1},z)\\delta(z_t,z’)\\right] \\c(z,x,\\mathscr{D}) &amp;\\equiv \\sum_{d = 1}^D\\mathbb{E}{\\mathbf{Z}|\\mathbf{X};\\theta^{old}}\\left[\\sum{t = 1}^{T{(d)}}\\delta(z_t,z)\\delta(x_t^{(d)},x)\\right]\\end{aligned}$$期望基于隐状态的后验概率$P(\\mathbf{z}|\\mathbf{x}^{(d)};\\theta^{old})$，计算期望的过程涉及指数级数量的计算，下面以隐状态转换次数的期望为例：$$\\begin{aligned}\\mathbb{E}{\\mathbf{Z}|\\mathbf{X};\\theta^{old}}\\left[\\delta(z{t - 1},z)\\delta(z_t,z’)\\right]&amp;= \\sum_{\\mathbf{z}}P(\\mathbf{z}|\\mathbf{x}^{(d)};\\theta^{old})\\delta(z_{t - 1},z)\\delta(z_t,z’) \\&amp;= \\sum_{\\mathbf{z}}\\frac{P(\\mathbf{x}^{(d)},\\mathbf{z};\\theta^{old})}{P(\\mathbf{x}^{(d)};\\theta^{old})}\\delta(z_{t - 1},z)\\delta(z_t,z’) \\&amp;= \\frac{1}{P(\\mathbf{x}^{(d)};\\theta^{old})}\\sum_{\\mathbf{z}}P(\\mathbf{x}^{(d)},\\mathbf{z};\\theta^{old})\\delta(z_{t - 1},z)\\delta(z_t,z’) \\&amp;= \\frac{P(\\mathbf{x}^{(d)},z_{t - 1} = z,z_t = z’;\\theta^{old})}{P(\\mathbf{x}^{(d)};\\theta^{old})}\\end{aligned}$$上式的分母可以使用前向概率来计算，下面是分母的计算：$$\\begin{aligned}P(\\mathbf{x},z_{t - 1} = s_i,z_t = s_j;\\theta)= &amp;P(x_1,\\cdots,x_{t - 1},z_{t - 1} = s_i;\\theta) \\times \\&amp;P(z_t = s_j|z_{t - 1} = s_i;\\theta) \\times \\&amp;P(x_t|z_t = s_j;\\theta) \\times \\&amp;P(x_{t + 1},\\cdots,x_T|z_t = s_j;\\theta) \\= &amp;\\alpha_{t - 1}(i)a_{ij}b_j(x_t)\\beta_t(j)\\end{aligned}$$ 估计隐状态初始化概率$$\\begin{aligned}p(z) &amp;= \\frac{c(z,\\mathscr{D})}{\\sum_{z’ \\in \\mathcal{S}}c(z’,\\mathscr{D})} \\&amp;= \\frac{\\sum_{d = 1}^DP(\\mathbf{x}^{(d)},z_1 = z;\\theta^{old})}{\\sum_{d = 1}^DP(\\mathbf{x}^{(d)};\\theta^{old})} \\\\overline{\\pi}i &amp;= \\frac{\\sum{d = 1}^D\\alpha_1(i)\\beta_1(i)}{\\sum_{d = 1}^D\\sum_{i = 1}^N\\alpha_{T^{(d)}}(i)}\\end{aligned}$$ 估计隐状态转换概率$$\\begin{aligned}p(z’|z) &amp;= \\frac{c(z,z’,\\mathscr{D})}{\\sum_{z’’ \\in \\mathcal{S}}c(z,z’’,\\mathscr{D})} \\&amp;= \\frac{\\sum_{d = 1}^D\\sum_{t = 2}^{T^{(d)}}P(\\mathbf{x},z_{t - 1} = z,z_t = z’;\\theta^{old}) }{\\sum_{d = 1}^D\\sum_{t = 2}^{T^{(d)}}P(\\mathbf{x},z_{t - 1} = z;\\theta^{old}) }\\\\overline{a}{ij} &amp;= \\frac{\\sum{d = 1}^D\\sum_{t = 2}^{T^{(d)}}\\alpha_{t - 1}(i)a_{ij}b_j(x^{(d)}t)\\beta_t(j)}{\\sum{d = 1}^D\\sum_{t = 2}^{T^{(d)}}\\alpha_{t - 1}(i)\\beta_t(j)}\\end{aligned}$$ 估计观测状态生成概率$$\\begin{aligned}p(x|z) &amp;= \\frac{c(z,x,\\mathscr{D})}{\\sum_{x’ \\in \\mathscr{O}}c(z,x’,\\mathscr{D})} \\&amp;= \\frac{\\sum_{d = 1}^D\\sum_{t = 1}^{T^{(d)}}\\delta(x_t^{(d)},x)P(\\mathbf{x}^{(d)},z_t = z;\\theta^{old})}{\\sum_{d = 1}^D\\sum_{t = 1}^{T^{(d)}}P(\\mathbf{x}^{(d)},z_t = z;\\theta^{old})} \\\\overline{b}i(k) &amp;= \\frac{\\sum{d = 1}^D\\sum_{t = 1}^{T^{(d)}}\\delta(x^{(d)},o_k)\\alpha_t(i)\\beta_t(i)}{\\sum_{d = 1}^D\\sum_{t = 1}^{T^{(d)}}\\alpha_t(i)\\beta_t(i)}\\end{aligned}$$","link":"/blog/2022/09/17/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-lecture3-%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/"},{"title":"统计学习方法学习笔记-附录-拉格朗日对偶性","text":"包含原始问题和对偶问题的定义，并说明了原始问题和对偶问题的关系，和最优解相同的条件 原始问题假设$f(x),c_i(x),h_j(x)$是定义在$R^n$上的连续可微函数，考虑约束最优化问题$$\\begin{aligned}\\mathop{min}\\limits_{x \\in R^n}\\ &amp;f(x) \\s.t.\\ &amp;c_i(x) \\leq 0,i = 1,2,\\cdots,k \\&amp;h_j(x) = 0,j = 1,2,\\cdots,l\\end{aligned}$$称此约束最优化问题为原始最优化问题或原始问题，首先引入广义拉格朗日函数：$$L(x,\\alpha,\\beta) = f(x) + \\sum_{i = 1}^k\\alpha_ic_i(x) + \\sum_{j = 1}^l\\beta_jh_j(x)$$这里，$x = (x^{(1)},x^{(2)},\\cdots,x^{(n)})^T \\in R^n,\\alpha_i,\\beta_j$是拉格朗日乘子，$\\alpha_i \\geq 0$，考虑$x$的函数：$$\\theta_P(x) = \\mathop{max}\\limits_{\\alpha,\\beta,\\alpha_i \\geq 0}L(x,\\alpha,\\beta)$$这里的下标$P$表示原始问题，因为$c_i(x) \\leq 0,\\alpha_i \\geq 0$，所以$\\sum_{i = 1}^k\\alpha_ic_i(x) \\leq 0$，类似的$\\sum_{j = 1}^l\\beta_jh_j(x) = 0$，故可以得到以下结论：$$\\theta_P(x) =\\begin{cases}f(x) &amp; x满足原始约束条件 \\+\\infty &amp; otherwise\\end{cases}$$考虑极小化问题：$$\\mathop{min}\\limits_x\\theta_P(x) = \\mathop{min}\\limits_{x} \\mathop{max}\\limits_{\\alpha,\\beta,\\alpha_i \\geq 0} L(x,\\alpha,\\beta)$$上式和原始的最优化问题是等价的，也就是和原问题有相同的解，$\\mathop{min}\\limits_{x} \\mathop{max}\\limits_{\\alpha,\\beta,\\alpha_i \\geq 0} L(x,\\alpha,\\beta)$称为广义拉格朗日函数的极小极大问题，为了方便，定义原始问题的最优值$p^* = \\mathop{min}\\limits_{x}\\theta_P(x)$称为原始问题的值 对偶问题定义$$\\theta_D(\\alpha,\\beta) = \\mathop{min}\\limits_xL(x,\\alpha,\\beta)$$再考虑极大化$\\theta_D(\\alpha,\\beta) = \\mathop{min}\\limits_xL(x,\\alpha,\\beta)$，即：$$\\mathop{max}\\limits_{\\alpha,\\beta;\\alpha_i \\geq 0}\\theta_D(\\alpha,\\beta) = \\mathop{max}\\limits_{\\alpha,\\beta;\\alpha_i \\geq 0} \\mathop{min}\\limits_xL(x,\\alpha,\\beta)$$问题$\\mathop{max}\\limits_{\\alpha,\\beta;\\alpha_i \\geq 0} \\mathop{min}\\limits_xL(x,\\alpha,\\beta)$称为广义拉格朗日函数的极大极小问题，可以将极大极小问题表示为约束最优化问题：$$\\begin{aligned}\\mathop{max}\\limits_{\\alpha,\\beta}\\ &amp;\\theta_D(\\alpha,\\beta) = \\mathop{max}\\limits_{\\alpha,\\beta} \\mathop{min}\\limits_xL(x,\\alpha,\\beta) \\s.t.\\ &amp;\\alpha_i \\geq 0,i = 1,2,\\cdots,k\\end{aligned}$$对偶问题的最优值$d^* = \\mathop{max}\\limits_{\\alpha,\\beta;\\alpha_i \\geq 0}\\ \\theta_D(\\alpha,\\beta)$ 原始问题和对偶问题的关系定理1若原始问题和对偶问题都有最优值，则：$$d^* = \\mathop{max}\\limits_{\\alpha,\\beta;\\alpha_i \\geq 0} \\mathop{min}\\limits_xL(x,\\alpha,\\beta) \\leq \\mathop{min}\\limits_{x} \\mathop{max}\\limits_{\\alpha,\\beta,\\alpha_i \\geq 0} L(x,\\alpha,\\beta) = p^*$$证明如下：对任意的$\\alpha,\\beta,x$有$$\\theta_D(\\alpha,\\beta) = \\mathop{min}\\limits_xL(x,\\alpha,\\beta) \\leq L(x,\\alpha,\\beta) \\leq \\mathop{max}\\limits_{\\alpha,\\beta,\\alpha_i \\geq 0} L(x,\\alpha,\\beta) = \\theta_P(x)$$所以：$$\\theta_D(\\alpha,\\beta) \\leq \\theta_P(x)$$由于原始问题和对偶问题均有最优值，所以：$$ \\mathop{max}\\limits_{\\alpha,\\beta;\\alpha_i \\geq 0} \\theta_D(\\alpha,\\beta) \\leq \\mathop{min}\\limits_x \\theta_P(x)$$得证。 定理2考虑原始问题和对偶问题，假设函数$f(x)$和$c_i(x)$是凸函数，$h_j(x)$是仿射函数，假设不等式约束$c_i(x)$是严格可行的，及存在$x$对所有的$i$有$c_i(x) \\lt 0$，则存在$x^*,\\alpha^*,\\beta^*$，使得$x^*$是原问题的解，$\\alpha^*,\\beta^*$是对偶问题的解，并且有：$$p^* = d^* = L(x^*,\\alpha^*,\\beta^*)$$ 定理3假设函数$f(x)$和$c_i(x)$是凸函数，$h_j(x)$是仿射函数，不等式约束$c_i(x)$是严格可行的，那么有$x^*$是原问题的解，$\\alpha^*,\\beta^*$是对偶问题的解的充分必要条件是$x^*,\\alpha^*,\\beta^*$满足下面的KKT条件(Karush-Kuhn-Tucker):$$\\nabla_xL(x^*,\\alpha^*,\\beta^*) = 0 \\\\alpha^*_ic_i(x^*) = 0,i = 1,2,\\cdots,k \\c_i(x^*) \\leq 0,i = 1,2,\\cdots,k \\\\alpha_i^* \\geq 0,i = 1,2,\\cdots,k \\h_j(x^*) = 0,j = 1,2,\\cdots,l$$$\\alpha^*_ic_i(x^*) = 0,i = 1,2,\\cdots,k$称为KKT的对偶互补条件，由此条件可知：若$\\alpha_i^* \\gt 0$，则$c_i(x^*) = 0$.","link":"/blog/2022/09/18/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%99%84%E5%BD%95-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/"},{"title":"统计学习方法学习笔记-07-支持向量机01","text":"包含对三种支持向量机的介绍，包括线性可分支持向量机，线性支持向量机和非线性支持向量机，包含核函数和一种快速学习算法-序列最小最优化算法SMO。 线性可分支持向量机与硬间隔最大化线性可分支持向量机假设一个特征空间上线性可分的训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in \\mathcal{X} = R^n,y_i \\in \\mathcal{Y} = {+1,-1},i = 1,2,\\cdots,N$线性可分支持向量机利用间隔最大化求最优分离超平面，解是唯一的分离超平面：$$\\omega^* \\cdot x + b^* = 0$$相应的分类决策函数：$$f(x) = sign(\\omega^* \\cdot x + b^*)$$ 函数间隔和几何间隔 超平面$(\\omega,b)$关于训练数据集$T$中样本点$(x_i,y_i)$的函数间隔functional margin：$$\\hat{\\gamma}_i = y_i(\\omega \\cdot x_i + b)$$ 超平面关于训练数据集的函数间隔：所有关于样本点的函数间隔的最小值：$$\\hat{\\gamma} = \\mathop{min}\\limits_{i = 1,\\cdots,N}\\hat{\\gamma}_i$$ 对超平面的法向量$\\omega$规范化使得$||\\omega|| = 1$，$||\\omega||$是$\\omega$的$L_2$范数，这时函数间隔变为几何间隔geometric margin，关于样本点的几何间隔为：$$\\gamma_i = y_i\\left(\\frac{\\omega}{||\\omega||} \\cdot x_i + \\frac{b}{||\\omega||}\\right)$$ 超平面关于训练集的几何间隔：$$\\gamma = \\mathop{min}\\limits_{i = 1,\\cdots,N}\\gamma_i$$ 如果$||\\omega|| = 1$，那么函数间隔和几何间隔相等 间隔最大化求几何间隔最大的分离超平面，这个问题表示为下面的约束最优化问题，表示每个样本点的几何间隔最小是$\\gamma$：$$\\begin{aligned}\\mathop{max}\\limits_{\\omega,b}\\ &amp;\\gamma \\s.t.\\ &amp;y_i\\left(\\frac{\\omega}{||\\omega||} \\cdot x_i + \\frac{b}{||\\omega||}\\right) \\geq \\gamma,i = 1,2,\\cdots,N\\end{aligned}$$将几何间隔转化为函数间隔：$$\\begin{aligned}\\mathop{max}\\limits_{\\omega,b}\\ &amp;\\frac{\\hat{\\gamma}}{||\\omega||} \\s.t.\\ &amp;y_i(\\omega \\cdot x_i + b) \\geq \\hat{\\gamma},i = 1,2,\\cdots,N\\end{aligned}$$而已知$\\hat{\\gamma}$的取值不影响最优化问题的解，取其值为1，于是得到线性可分支持向量机学习的最优化问题：$$\\begin{aligned}\\mathop{min}\\limits_{\\omega,b}\\ &amp;\\frac{1}{2}||\\omega||^2 \\s.t.\\ &amp;y_i(\\omega \\cdot x_i + b) - 1 \\geq 0,i = 1,2,\\cdots,N\\end{aligned}$$凸优化问题是指约束最优化问题：$$\\begin{aligned}\\mathop{min}\\limits_\\omega\\ &amp;f(\\omega) \\s.t.\\ &amp;g_i(\\omega) \\leq 0,i = 1,2,\\cdots,k \\&amp;h_i(\\omega) = 0,i = 1,2,\\cdots,l\\end{aligned}$$其中目标函数$f(\\omega)$和约束函数$g_i(\\omega)$都是$R^n$上连续可微的凸函数，约束函数$h_i(\\omega)$是$R^n$上的仿射函数($f(x)$称为仿射函数，如果满足$f(x) = a \\cdot x + b,a \\in R^n,b \\in R, x \\in R^n$）当目标函数$f(\\omega)$是二次函数且约束函数$g_i(\\omega)$是仿射函数时，上述凸优化问题成为凸二次规划问题 线性可分支持向量机学习算法-最大间隔法输入：线性可分的训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in \\mathcal{X} = R^n,y_i \\in \\mathcal{Y} = {+1,-1},i = 1,2,\\cdots,N$输出：最大间隔分离超平面和分类决策函数 构造并求解约束最优化问题，得到最优解$\\omega^*,b^*$：$$\\begin{aligned}\\mathop{min}\\limits_{\\omega,b}\\ &amp;\\frac{1}{2}||\\omega||^2 \\s.t.\\ &amp;y_i(\\omega \\cdot x_i + b) - 1 \\geq 0,i = 1,2,\\cdots,N\\end{aligned}$$ 得到分离超平面$$\\omega^* \\cdot x + b^* = 0$$分类决策函数：$$f(x) = sign(\\omega^* \\cdot x + b^*)$$ 若训练数据集$T$线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一p117页证明(回忆不起来可看)支持向量：是指约束条件不等式等号成立的样本点$y_i(\\omega \\cdot x_i + b) - 1 = 0$间隔：依赖于分离超平面的法向量，等于$\\frac{2}{||\\omega||}$ 学习的对偶算法首先引入拉格朗日乘子$\\alpha_i \\geq 0,i = 1,2,\\cdots,N$定义拉格朗日函数，$\\alpha = (\\alpha_1,\\alpha_2,\\cdots,\\alpha_N)^T$为拉格朗日乘子向量：$$L(\\omega,b,\\alpha) = \\frac{1}{2}||\\omega||^2 - \\sum_{i = 1}^N\\alpha_iy_i(\\omega \\cdot x_i + b) + \\sum_{i = 1}^N\\alpha_i$$对偶问题是极大极小问题，先求极小再求极大：$$\\mathop{max}\\limits_\\alpha \\mathop{min}\\limits_{\\omega,b}L(\\omega,b,\\alpha)$$ 求$\\mathop{min}\\limits_{\\omega,b}L(\\omega,b,\\alpha)$，对$\\omega,b$求偏导$$\\nabla_\\omega L(\\omega,b,\\alpha) = \\omega - \\sum_{i = 1}^N\\alpha_iy_ix_i = 0 \\\\nabla_b L(\\omega,b,\\alpha) = -\\sum_{i = 1}^N\\alpha_iy_i = 0$$得到：$$\\omega = \\sum_{i = 1}^N\\alpha_iy_ix_i \\\\sum_{i = 1}^N\\alpha_iy_i = 0$$将上式带到拉格朗日函数：$$\\begin{aligned}L(\\omega,b,\\alpha)&amp;= \\frac{1}{2}\\sum_{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) - \\sum_{i = 1}^N\\alpha_iy_i\\left(\\left(\\sum_{j = 1}^N\\alpha_jy_jx_j\\right) \\cdot x_i + b\\right) + \\sum_{i = 1}^N\\alpha_i \\&amp;= -\\frac{1}{2}\\sum_{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) + \\sum_{i = 1}^N\\alpha_i\\end{aligned}$$所以：$$\\mathop{min}\\limits_{\\omega,b}L(\\omega,b,\\alpha) = -\\frac{1}{2}\\sum_{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) + \\sum_{i = 1}^N\\alpha_i$$ 求$\\mathop{min}\\limits_{\\omega,b}L(\\omega,b,\\alpha)$对$\\alpha$的极大：$$\\begin{aligned}\\mathop{max}\\limits_{\\alpha}\\ &amp;-\\frac{1}{2}\\sum_{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) + \\sum_{i = 1}^N\\alpha_i \\s.t.\\ &amp;\\sum_{i = 1}^N\\alpha_iy_i = 0 \\&amp; \\alpha_i \\geq 0,i = 1,2,\\cdots,N\\end{aligned}$$将上式的目标函数由求极大转换为求极小，得到下面和上式等价的对偶最优化问题$$\\begin{aligned}\\mathop{min}\\limits_{\\alpha}\\ &amp;\\frac{1}{2}\\sum_{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) - \\sum_{i = 1}^N\\alpha_i \\s.t.\\ &amp;\\sum_{i = 1}^N\\alpha_iy_i = 0 \\&amp; \\alpha_i \\geq 0,i = 1,2,\\cdots,N\\end{aligned}$$因为原始问题满足原始问题和对偶问题最优值相同所需的条件，所以存在$\\omega^*,\\alpha^*,\\beta^*$分别是原始问题的解和对偶问题的解，求解原始问题可以转化为求解对偶问题 假设对偶问题解为$\\alpha^* = (\\alpha^*1,\\alpha^*2,\\cdots,\\alpha_N^*)^T$，存在下标$j$，使得$\\alpha_j^* \\gt 0$，可以由$\\alpha^*$求得原始问题的解$\\omega^*,b^*$$$\\omega^* = \\sum{i = 1}^N\\alpha_i^y_ix_i \\b^ = y_j - \\sum{i = 1}^N\\alpha_i^y_i(x_i \\cdot x_j)$$证明如下：由KKT条件：$$\\nabla_\\omega L(\\omega^,b^*,\\alpha^*) = \\omega^* - \\sum_{i = 1}^N\\alpha^*iy_ix_i = 0$$得到：$$\\omega^* = \\sum{i = 1}^N\\alpha_i^y_ix_i$$至少存在一个$a^j \\gt 0$，这是因为假设所有的都为0，那么$\\omega^* = 0$，这不是原始最优化问题的解，所以至少存在一个，对这个$j$结合KKT条件$\\alpha^*i(y_i(\\omega^* \\cdot x_i + b^*) - 1) = 0$，得到$y_j(\\omega^* \\cdot x_j + b^*) - 1 = 0$，又有$y^2_j = 1$，得到：$$y_j = \\omega^* \\cdot x_j + b^* \\b^* = y_j - \\omega^* \\cdot x_j \\b^* = y_j - \\sum{i = 1}^N\\alpha_i^y_i(x_i \\cdot x_j)$$至此$\\omega^,b^*$求出分离超平面为：$$\\sum{i = 1}^N\\alpha_i^y_i(x \\cdot x_i) + b^ = 0$$分类决策函数为：$$f(x) = sign\\left(\\sum_{i = 1}^N\\alpha_i^y_i(x \\cdot x_i) + b^\\right)$$ 线性可分支持向量机学习算法-对偶输入：线性可分的训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in \\mathcal{X} = R^n,y_i \\in \\mathcal{Y} = {+1,-1},i = 1,2,\\cdots,N$输出：最大间隔分离超平面和分类决策函数 构造并求解约束最优化问题求得最优解$\\alpha^* = (\\alpha^*1,\\alpha^*2,\\cdots,\\alpha_N^*)^T$$$\\begin{aligned}\\mathop{min}\\limits{\\alpha}\\ &amp;\\frac{1}{2}\\sum{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) - \\sum_{i = 1}^N\\alpha_i \\s.t.\\ &amp;\\sum_{i = 1}^N\\alpha_iy_i = 0 \\&amp; \\alpha_i \\geq 0,i = 1,2,\\cdots,N\\end{aligned}$$ 计算$$\\omega^* = \\sum_{i = 1}^N\\alpha_i^y_ix_i$$选择$\\alpha^$的一个分量$\\alpha_j^* \\gt 0$，计算：$$b^* = y_j - \\sum_{i = 1}^N\\alpha_i^*y_i(x_i \\cdot x_j)$$ 求得分离超平面和决策函数：$$\\sum_{i = 1}^N\\alpha_i^y_i(x \\cdot x_i) + b^ = 0 \\f(x) = sign\\left(\\sum_{i = 1}^N\\alpha_i^y_i(x \\cdot x_i) + b^\\right)$$ 训练数据中对应$\\alpha^*_i \\gt 0$的样本点$(x_i,y_i)$的$x_i$称为支持向量，根据KKT条件得到：$y_i(\\omega^* \\cdot x_i + b^*) - 1 = 0$，支持向量$x_i$一定在间隔边界上","link":"/blog/2022/09/18/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-07-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA01/"},{"title":"统计学习方法学习笔记-07-支持向量机02","text":"包含对三种支持向量机的介绍，包括线性可分支持向量机，线性支持向量机和非线性支持向量机，包含核函数和一种快速学习算法-序列最小最优化算法SMO。 线性支持向量机与软间隔最大化假设训练集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)},x_i \\in \\mathcal{X} = R^n,y_i \\in \\mathcal{Y} = {+1,-1},i = 1,2,\\cdots,N$，这个训练数据集不是线性可分的，有一些特异点，将这些特异点去除之后，剩下的集合是线性可分的，所以我们对每一个样本点$(x_i,y_i)$引入一个松弛变量$\\xi_i \\geq 0$，之后约束条件变为：$$y_i(\\omega \\cdot x_i + b) \\geq 1 - \\xi_i$$同时对于每一个松弛变量需要支付一个代价，所以目标函数变为：$$\\frac{1}{2}||\\omega||^2 + C\\sum_{i = 1}^N\\xi_i$$$C \\gt 0$称为惩罚参数，其值越大对误分类的惩罚越大线性支持向量机的学习问题的原始问题如下：$$\\begin{aligned}\\mathop{min}\\limits_{\\omega,b,\\xi}\\ &amp;\\frac{1}{2}||\\omega||^2 + C\\sum_{i = 1}^N\\xi_i \\s.t.\\ &amp;y_i(\\omega \\cdot x_i + b) \\geq 1 - \\xi_i,i = 1,2,\\cdots,N \\&amp;\\xi_i \\geq 0,i = 1,2,\\cdots,N\\end{aligned}$$引入拉格朗日乘子，原始最优化问题的拉格朗日函数为：$$L(\\omega,b,\\xi,\\alpha,\\mu) \\equiv \\frac{1}{2}||\\omega||^2 + C\\sum_{i = 1}^N\\xi_i - \\sum_{i = 1}^N\\alpha_i(y_i(\\omega \\cdot x_i + b) - 1 + \\xi_i) - \\sum_{i = 1}^N\\mu_i\\xi_i$$其中$\\alpha_i \\geq 0,\\mu_i \\geq 0$首先求$L(\\omega,b,\\xi,\\alpha,\\mu)$对$\\omega,b,\\xi$的极小：$$\\nabla_\\omega L(\\omega,b,\\xi,\\alpha,\\mu) = \\omega - \\sum_{i = 1}^N\\alpha_iy_ix_i = 0 \\\\nabla_b L(\\omega,b,\\xi,\\alpha,\\mu) = -\\sum_{i = 1}^N\\alpha_iy_i = 0 \\\\nabla_{\\xi_i} L(\\omega,b,\\xi,\\alpha,\\mu) = C - \\alpha_i -\\mu_i = 0$$可以得到：$$\\omega = \\sum_{i = 1}^N\\alpha_iy_ix_i \\\\sum_{i = 1}^N\\alpha_iy_i = 0 \\C - \\alpha_i -\\mu_i = 0$$将上式带到拉格朗日函数当中可以得到：$$\\mathop{min}\\limits_{\\omega,b,\\xi} L(\\omega,b,\\xi,\\alpha,\\mu) = -\\frac{1}{2}\\sum_{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) + \\sum_{i = 1}^N\\alpha_i$$再对$\\mathop{min}\\limits_{\\omega,b,\\xi} L(\\omega,b,\\xi,\\alpha,\\mu)$求$\\alpha$的最大即得对偶问题：$$\\begin{aligned}\\mathop{max}\\limits_\\alpha\\ &amp; -\\frac{1}{2}\\sum_{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) + \\sum_{i = 1}^N\\alpha_i \\s.t.\\ &amp;\\sum_{i = 1}^N\\alpha_iy_i = 0 \\&amp;C - \\alpha_i - \\mu_i = 0 \\&amp;\\alpha_i \\geq 0 \\&amp;\\mu_i \\geq 0, i = 1,2,\\cdots,N\\end{aligned}$$利用等式约束消去$\\mu_i$，只剩下变量$\\alpha_i$，于是上式变为：$$\\begin{aligned}\\mathop{max}\\limits_\\alpha\\ &amp; -\\frac{1}{2}\\sum_{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) + \\sum_{i = 1}^N\\alpha_i \\s.t.\\ &amp;\\sum_{i = 1}^N\\alpha_iy_i = 0 \\&amp;0 \\leq \\alpha_i \\leq C \\\\end{aligned}$$将上式的求极大变为求极小：$$\\begin{aligned}\\mathop{min}\\limits_\\alpha\\ &amp; \\frac{1}{2}\\sum_{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) - \\sum_{i = 1}^N\\alpha_i \\s.t.\\ &amp;\\sum_{i = 1}^N\\alpha_iy_i = 0 \\&amp;0 \\leq \\alpha_i \\leq C \\\\end{aligned}$$通过求解对偶问题而得到原始问题的解，设$\\alpha^* = (\\alpha^*1,\\alpha^*2,\\cdots,\\alpha_N^*)^T$是对偶问题的解，存在下标$j$，使得$0 \\leq \\alpha_j^* \\leq C$，可以由$\\alpha^*$求得原始问题的解$\\omega^*,b^*$$$\\omega^* = \\sum{i = 1}^N\\alpha_i^y_ix_i \\b^ = y_j - \\sum{i = 1}^N\\alpha_i^*y_i(x_i \\cdot x_j)$$其证明过程和线性可分支持向量机的类似 线性支持向量机学习算法：输入：训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in \\mathcal{X} = R^n,y_i \\in \\mathcal{Y} = {+1,-1},i = 1,2,\\cdots,N$输出：分离超平面和分类决策函数 选择惩罚参数$C \\gt 0$，构造并求解约束最优化问题求得最优解$\\alpha^* = (\\alpha^*1,\\alpha^*2,\\cdots,\\alpha_N^*)^T$$$\\begin{aligned}\\mathop{min}\\limits{\\alpha}\\ &amp;\\frac{1}{2}\\sum{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) - \\sum_{i = 1}^N\\alpha_i \\s.t.\\ &amp;\\sum_{i = 1}^N\\alpha_iy_i = 0 \\&amp; 0 \\leq \\alpha_i \\leq C,i = 1,2,\\cdots,N\\end{aligned}$$ 计算$$\\omega^* = \\sum_{i = 1}^N\\alpha_i^y_ix_i$$选择$\\alpha^$的一个分量$0 \\lt \\alpha_j^* \\lt 0$，计算：$$b^* = y_j - \\sum_{i = 1}^N\\alpha_i^*y_i(x_i \\cdot x_j)$$ 求得分离超平面和决策函数：$$\\sum_{i = 1}^N\\alpha_i^y_i(x \\cdot x_i) + b^ = 0 \\f(x) = sign\\left(\\sum_{i = 1}^N\\alpha_i^y_i(x \\cdot x_i) + b^\\right)$$ 训练数据中对应$\\alpha^*_i \\gt 0$的样本点$(x_i,y_i)$的$x_i$称为支持向量: $\\alpha_i^* \\lt C$，有$\\xi_i = 0$支持向量落在间隔边界上 $\\alpha_i^* = C,0 \\lt \\xi_i \\lt 1$支持向量分类正确，落在间隔边界与分离超平面之间 $\\alpha_i^* = C, \\xi_i = 1$支持向量落在分离超平面上 $\\alpha_i^* = C, \\xi_i \\gt 1$支持向量落在分离超平面误分一侧 实例$x_i$到间隔边界的距离为$\\frac{\\xi_i}{||\\omega||}$ 两间隔边界之间的距离为$\\frac{2}{||\\omega||}$","link":"/blog/2022/09/18/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-07-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA02/"},{"title":"统计学习方法学习笔记-07-支持向量机03","text":"包含对三种支持向量机的介绍，包括线性可分支持向量机，线性支持向量机和非线性支持向量机，包含核函数和一种快速学习算法-序列最小最优化算法SMO。 非线性支持向量机与核函数核技巧非线性分类问题给定一个训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i$属于输入空间，$x_i \\in \\mathcal{X} = R^n,y_i \\in \\mathcal{Y} = {-1,+1},i = 1,2,\\cdots,N$，如果可以用$R^n$中的一个超曲面将正负例正确分开，则称这个问题为非线性可分问题用线性分类方法求解非线性问题分为两步，首先使用一个变换将原空间的数据映射到新空间，然后在新空间里用线性分类方法从训练数据中学习分类模型，基本想法就是通过一个非线性变换将输入空间对应于一个特征空间 核函数的定义设$\\mathcal{X}$是输入空间(欧式空间$R^n$的子集或离散集合)，又设$\\mathcal{H}$为特征空间(希尔伯特空间)，如果存在一个从$\\mathcal{X}$到$\\mathcal{H}$的映射：$$\\phi(x):\\mathcal{X} \\rightarrow \\mathcal{H}$$使得对所有$x,z \\in \\mathcal{X}$，函数$K(x,z)$满足条件:$$K(x,z) = \\phi(x) \\cdot \\phi(z)$$则称$K(x,z)$为核函数，$\\phi(x)$为隐射函数，式中$\\phi(x) \\cdot \\phi(z)$是两者的内积，核技巧的想法是，在学习与预测中只定义核函数$K(x,z)$，而不显示地定义隐射函数$\\phi$，可以看到，对于给定的核$K(x,z)$，特征空间$\\mathcal{H}$和隐射函数$\\phi$的取法并不唯一，可以取不同的特征空间，即便是在同一特征空间里也可以取不同的映射 核技巧在支持向量机中的应用在线性支持向量机的对偶问题中，无论是目标函数还是决策函数都只涉及输入实例与实例之间的内积$$\\frac{1}{2}\\sum_{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_j(x_i \\cdot x_j) - \\sum_{i = 1}^N\\alpha_i$$在对偶问题的目标函数中的内积$x_i \\cdot x_j$可以用核函数$K(x,z) = \\phi(x) \\cdot \\phi(z)$来代替，此时对偶问题的目标函数为：$$W(\\alpha) = \\frac{1}{2}\\sum_{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j) - \\sum_{i = 1}^N\\alpha_i$$同样，分类决策中的内积也可以用核函数代替：$$f(x) = sign\\left(\\sum_{i = 1}^{N_s}\\alpha_i^y_iK(x_i , x) + b^\\right)$$这等价于经过映射函数$\\phi$将原来的输入空间变换到一个新的特征空间，将输入空间的内积$x_i \\cdot x_j$变换为特征空间中的内积$\\phi(x_i) \\cdot \\phi(x_j)$ 正定核目的：函数$K(x,z)$成为核函数的条件 正定核的充要条件设$K:\\mathcal{X} \\times \\mathcal{X} \\rightarrow R$是对称函数，则$K(x,z)$为正定核的充要条件是对任意$x_i \\in \\mathcal{X},i = 1,2,\\cdots,m,K(x,z)$对应的Gram矩阵：$$K = [K(x_i,x_j)]_{m \\times m}$$是半正定矩阵 正定核的等价定义设$\\mathcal{X} \\subset R^n,K(x,z)$是定义在$\\mathcal{X} \\times \\mathcal{X}$上的对称函数，如果对任意的$x_1,x_2,\\cdots,x_m \\in \\mathcal{X},K(x,z)$关于$x_1,x_2,\\cdots,x_m$的Gram矩阵$K = [K(x_i,x_j)]_{m \\times m}$是半正定矩阵，则称$K(x,z)$是正定核。 常用核函数多项式核函数polynomial kernel function此时对应的支持向量机是一个$p$次多项式分类器$$K(x,z) = (x \\cdot z + 1)^p$$ 高斯核函数gaussain kernel function此时对应的支持向量机是高斯径向基函数分类器$$K(x,z) = exp\\left(-\\frac{||x - z||^2}{2\\sigma^2}\\right)$$ 非线性支持向量分类机非线性支持向量机学习算法输入：训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in \\mathcal{X} = R^n,y_i \\in \\mathcal{Y} = {+1,-1},i = 1,2,\\cdots,N$输出：分类决策函数 选择适当的核函数$K(x,z)$和惩罚参数$C \\gt 0$，构造并求解约束最优化问题求得最优解$\\alpha^* = (\\alpha^*1,\\alpha^*2,\\cdots,\\alpha_N^*)^T$$$\\begin{aligned}\\mathop{min}\\limits{\\alpha}\\ &amp;\\frac{1}{2}\\sum{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_jK(x_i, x_j) - \\sum_{i = 1}^N\\alpha_i \\s.t.\\ &amp;\\sum_{i = 1}^N\\alpha_iy_i = 0 \\&amp; 0 \\leq \\alpha_i \\leq C,i = 1,2,\\cdots,N\\end{aligned}$$ 选择$\\alpha^*$的一个分量$0 \\lt \\alpha_j^* \\lt C$，计算：$$b^* = y_j - \\sum_{i = 1}^N\\alpha_i^*y_iK(x_i, x_j)$$ 求得决策函数：$$f(x) = sign\\left(\\sum_{i = 1}^N\\alpha_i^y_iK(x, x_i) + b^\\right)$$ 序列最小最优化算法(SMO算法)sequential minimal optimization目的：当训练样本容量很大的时候，快速实现支持向量机学习SMO算法要解的问题如下：$$\\begin{aligned}\\mathop{min}\\limits_{\\alpha}\\ &amp;\\frac{1}{2}\\sum_{i = 1}^N\\sum_{j = 1}^N\\alpha_i\\alpha_jy_iy_jK(x_i, x_j) - \\sum_{i = 1}^N\\alpha_i \\s.t.\\ &amp;\\sum_{i = 1}^N\\alpha_iy_i = 0 \\&amp; 0 \\leq \\alpha_i \\leq C,i = 1,2,\\cdots,N\\end{aligned}$$变量是$\\alpha_i$，算法是一种启发式算法，基本思路如下，如果所有变量的解都满足此最优化问题的KKT条件，那么此时最优化问题的解就得到了，因为KKT条件是最优化问题的充分必要条件，否则选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题，这个二次规划问题关于这两个变量的解应该更加接近原始二次规划问题的解，因为这会使原始二次规划问题的目标函数值变小，重要的是，这时子问题可以通过解析方法求解，这样就可以大大提高整个算法的计算速度，子问题有两个变量，一个是违反KKT条件最严重的那一个，另一个由约束条件自动确定，假设$\\alpha_1,\\alpha_2$为两个变量，$\\alpha_3,\\alpha_4,\\cdots,\\alpha_N$固定，那么由约束不等式$\\sum_{i = 1}^N\\alpha_iy_i = 0$和$y_1^2 = 1$可知：$$\\alpha_1 = -y_1\\sum_{i = 2}^N\\alpha_iy_i$$整个SMO算法包含两个部分：求解两个变量二次规划的解析方法和选择变量的启发式方法 两个变量二次规划的求解方法不失一般性，假设$\\alpha_1,\\alpha_2$为两个变量，$\\alpha_3,\\alpha_4,\\cdots,\\alpha_N$固定，省略不含$\\alpha_1,\\alpha_2$的常数项，于是最优化问题可以写为：$$\\begin{aligned}\\mathop{min}\\limits_{\\alpha_1,\\alpha_2}\\ &amp; W(\\alpha_1,\\alpha_2) = \\frac{1}{2}K_{11}\\alpha_1^2 + \\frac{1}{2}K_{22}\\alpha_2^2 + y_1y_2K_{12}\\alpha_1\\alpha_2 - (\\alpha_1 + \\alpha_2) + y_1\\alpha_1\\sum_{i = 3}^Ny_i\\alpha_iK_{i1} + y_2\\alpha_2\\sum_{i = 3}^Ny_i\\alpha_iK_{i2} \\s.t.\\ &amp; \\alpha_1y_1 + \\alpha_2y_2 = -\\sum_{i = 3}^Ny_i\\alpha_i = \\zeta \\&amp; 0 \\leq \\alpha_i \\leq C,i = 1,2\\end{aligned}$$约束可以用二维空间中的图形表示：不等式约束使得$(\\alpha_1,\\alpha_2)$在盒子$[0,C] \\times [0,C]$中，等式约束使得$(\\alpha_1,\\alpha_2)$在平行于盒子对角线的直线上，因此要求的是目标函数在一条平行于对角线的线段上的最优值，这使得两个变量的最优化问题成为实质上的单变量的最优化问题，此时考虑$\\alpha_2$的最优化问题最优值$\\alpha_2^{new}$的取值范围需要满足条件：$$L \\leq \\alpha_2^{new} \\leq H$$其中$L,H$是$\\alpha_2^{new}$所在的对角线段端点的界： $y_1 \\neq y_2$$$L = max(0,\\alpha_2^{old} - \\alpha_1^{old}),H = min(C,C + \\alpha_2^{old} - \\alpha_1^{old})$$ $y_1 = y_2$$$L = max(0, \\alpha_2^{old} + \\alpha_1^{old} - C),H = min(C,\\alpha_2^{old} + \\alpha_1^{old})$$ 假设问题的初始可行解为$\\alpha_1^{old},\\alpha_2^{old}$，最优解为$\\alpha_1^{new},\\alpha_2^{new}$，并且假设沿着约束方向未经剪辑时$\\alpha_2$的最优解为$\\alpha_2^{new,unc}$先记:$$g(x) = \\sum_{i = 1}^N\\alpha_iy_iK(x_i,x) + b \\E_i = g(x_i) - y_i = \\left(\\sum_{i = 1}^N\\alpha_iy_iK(x_i,x) + b\\right) - y_i$$最优化问题沿着约束方向未经剪辑时的解为：$$\\alpha_2^{new,unc} = \\alpha_2^{old} + \\frac{y_2(E_1 - E_2)}{\\eta}$$其中：$$\\eta = K_{11} + K_{22} - 2K_{12} = ||\\Phi(x_1) - \\Phi(x_2)||^2$$经剪辑后$\\alpha_2$的解是：$$\\alpha_2^{new} =\\begin{cases}H, &amp; \\alpha_2^{new,unc} \\gt H \\\\alpha_2^{new,unc}, &amp; L \\leq \\alpha_2^{new,unc} \\leq H \\L, &amp; \\alpha_2^{new,unc} \\lt L\\end{cases}$$由$\\alpha_2^{new}$求得$\\alpha_1^{new}$是：$$\\alpha_1^{new} = \\alpha_1^{old} + y_1y_2(\\alpha_2^{old} - \\alpha_2^{new})$$ 变量的选择方法SMO算法在每个子问题中选择两个变量优化，其中至少一个变量是违反KKT条件。 第1个变量的选择选择第一个变量的过程为外层循环，在训练样本中选取违反KKT条件最严重的点，检查样本点$(x_i,y_i)$满足KKT条件的方法：$$\\begin{aligned}\\alpha_i = 0 &amp; \\Leftrightarrow y_ig(x_i) \\geq 1 \\0 \\lt \\alpha_i \\lt C &amp; \\Leftrightarrow y_ig(x_i) = 1 \\\\alpha_i = C &amp; \\Leftrightarrow y_ig(x_i) \\leq 1\\end{aligned}$$其中$g(x_i) = \\sum_{j = 1}^N\\alpha_jy_jK(x_i,x_j) + b$，在检验过程中，首先遍历所有满足条件$0 \\lt \\alpha_i \\lt C$的样本点，即在间隔边界上的支持向量点，检查是否满足KKT条件，然后检查整个训练数据集 第2个变量的选择在找到第一个变量$\\alpha_1$的基础上寻找$\\alpha_2$，已知$\\alpha_2^{new}$依赖于$|E_1 - E_2|$，而且$\\alpha_1$确定，所以$E_1$确定，如果$E_1$是正的，选择最小的$E_i$作为$E_2$，否则选择最大的$E_i$作为$E_2$，如果选择的$\\alpha_2$不能使目标函数有足够的下降，那么采用启发式规则继续选择$\\alpha_2$试用，如果还是找不到合适的$\\alpha_2$，就重新找另外的$\\alpha_1$ 计算阈值$b$和差值$E_i$ SMO算法输入：训练数据集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in \\mathcal{X} = R^n,y_i \\in \\mathcal{Y} = {+1,-1},i = 1,2,\\cdots,N$，精度$\\varepsilon$输出：近似解$\\hat{\\alpha}$ 取初值$\\alpha^{(0)} = 0,k = 0$; 选取优化变量$\\alpha_1^{(k)},\\alpha_2^{(k)}$，解析求解两个变量的最优化问题，过程见上上小结，求解得到最优解$\\alpha_1^{(k + 1)},\\alpha_2^{(k + 1)}$，更新$\\alpha$为$\\alpha^{(k + 1)}$； 若在精度$\\varepsilon$范围内满足停机条件$$\\sum_{i = 1}^N\\alpha_iy_i = 0,0 \\leq \\alpha_i \\leq C,i = 1,2,\\cdots,N \\y_i \\cdot g(x_i)\\begin{cases}\\geq 1, &amp; {x_i|\\alpha_i = 0} \\= 1, &amp; {x_i|0 \\lt \\alpha_i \\lt C} \\\\leq 1, &amp; {x_i|\\alpha_i = C}\\end{cases}$$其中$g(x_i) = \\sum_{j = 1}^N\\alpha_jy_jK(x_i,x_j) + b$则转第四步，否则令$k = k + 1$，转到第二步 取$\\hat{\\alpha} = \\alpha^{(k + 1)}$","link":"/blog/2022/09/18/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-07-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA03/"},{"title":"卜算法学习笔记-02-分而治之算法02","text":"包含逆序数和寻找数组中第k大的数的算法 数组中的逆序对计数算法分析所谓逆序对，是指数组中的两个元素 $A[i]$ 和 $A[j]$，其下标 $i &lt; j$，但是考察元素的值，却有 $A[i] &gt; A[j]$。输入：一个包含 $n$ 个元素的数组 $A[0..n − 1]$;输出：数组中的逆序对的数目。从最简单的实例入手:如果数组 A 只有两个元素 A[0] 和 A[1]，我们只需比较这两个元素，即可计算出逆序数。接下来我们考虑如何求解规模更大的实例。对于一个包含 $n$ 个元素的数组 A，我们可以很容易地依据下标将 A 分解成两个小的数组，即左一半 $A[0..⌈ \\frac{n}{2} ⌉ − 1]$(简记为 $L$)和右一半 $A[⌈ \\frac{n}{2} ⌉..n − 1]$(简记为 $R$)。在将大的实例分解成子实例之后，我们可以假定子实例已经求解，即使用递归调用分别求出两个元素都在 $L$ 中的逆序对数目、 以及两个元素都在 $R$ 中的逆序对数目;因此只剩下最后一个困难:如何将子实例的 解“组合”成原始给定实例的解。逆序对计数算法中计算一个元素在右半边，一个元素在右半边的逆序对数目：如果 $L$ 和 $R$ 都已经排好序的话，只需执行 $O(n)$ 次比较即可完成逆序对的计数。 时间复杂度$$T(n) = 2T(\\frac{n}{2}) + O(n) = O(n \\log n)$$ 选择问题：对数组的归约如何从数组中找出第 k 小的数输入：一个包含 $n$ 个元素的数组 $A[0..n − 1]$，以及一个整数 $k，(0 ≤ k ≤ n − 1)$;输出：数组 $A$ 中第 $k$ 小的元素。采用依据元素值拆分数组的方案: 选择分组中位数的中位数作为中心元算法分析方法：将数组$A$分组，然后以组中位数作为$A$的近似，进而以组中位数的中位数作为中心元可以看到，组中位数的中位数在该数前面和后面均有一定量的值大于和小于他们(可以计算，如分组数为$m$，每组有$n$个数，为了方便计算，假设$m、n$都是奇数，那么比该数小的数至少有$\\lceil \\frac{n}{2} \\rceil \\times \\lceil \\frac{m}{2} \\rceil - 1$，比组中位数的中位数大的数也有$\\lceil \\frac{n}{2} \\rceil \\times \\lceil \\frac{m}{2} \\rceil - 1$个) 时间复杂度分析算法总共需要比较的次数不超过$24n$次$$T() \\leq T(\\frac{n}{5}) + T(\\frac{7n}{10}) + O(n) = O(n)$$ 依据随机样本的统计量确定中间元算法分析构造数组的近似，一个直观的想法是对数组进行随机采样，以随机采样来近似数组，进而利用样本的统计量来确定中心元，比如采用样本中位数作为中心元，还进行一个扩展，不是只使用样本的中位数，而是使用$\\frac{1}{4}分位数和$$\\frac{3}{4}$分位数，将点估计扩展成区间估计。假设随机采样的数量为$r$，采样$r$的元素得到的样本为$S$，之后计算出$S$的分位数$u$和$v$，接着对每个元素都和$u$和$v$比较，依据比较结果分别放入集合$L,M,H$，最后将$M$排序，返回其中位数作为数组的中位数估计： 时间复杂度分析对于一个包含$n$个元素的数组$A$，当设置$r = n^{\\frac{3}{4}},\\delta = n^{-\\frac{1}{4}}$，算法的期望时间复杂度是$O(n)$，此时位于$u,v$区间的$S$中的元素共有$n^{-\\frac{1}{4}} \\times r$个，所以可以期望位于$u,v$区间的$A$中的元素共有$n^{-\\frac{1}{4}} \\times n = n^{\\frac{3}{4}}$个，所以$||M||$不会太大也不会太小 采用随机选择的一个元素作中心元算法分析我们有$\\frac{1}{2}$的概率选到中间区域的元素，所以连续迭代两轮，可以以很高的概率使得子问题的规模呈指数级降低 时间复杂度分析可以看到将每两次算法执行作为一期$$T(n) = T(\\frac{3n}{4}) + 2n = O(n)$$","link":"/blog/2022/09/19/%E5%8D%9C%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-02-%E5%88%86%E8%80%8C%E6%B2%BB%E4%B9%8B%E7%AE%97%E6%B3%9502/"},{"title":"图像处理学习笔记-01-绪论","text":"图像处理的目的： 为解释图像而改善图像信息； 为存储、传输和提取图像信息等任务处理图像数据； 本章的主要目的如下:定义图像处理的范围;通过了解图像处理技术的主要应用领域简介图像处理技术的现状;简要讨论数字图像处理所用的主要方法;什么是数字图像处理 数字图像：一幅图像可以定义为—个二维函数$f(x,y)$，其中$x$和$y$是空间(平面)坐标，任意一对空间坐标$(x,y)$处的幅值$f$称为图像在该点的强度或灰度,当$x,y$和灰度值$f$都是有限的离散量时，我们称该图像为数字图像； 数字图像处理：借助于数字计算机来处理数字图像，输入输出都是图像，包含从图像中提取特征和目标的识别； 人类的感知仅限于可见光波段，但是成像机器可以几乎覆盖从伽马射线到无线电波的整个电磁波谱范围； 计算机处理分为初级处理、中级处理和高级处理，初级处理如降低噪声，对比度增强等任务输入输出都是图像，中级处理如分割等输入是图像，输出是从这些图形中提取的特征； 数字图像处理技术应用领域实例 伽马射线成像 核医学中：将放射性同位素注入人体，同位素衰变会发射伽马射线； 天文观测中 X射线成像 医学诊断：CT计算机断层成像，血管造影；用于医学和工业成像的X射线由X射线管产生,X射线管是带有阴极和阳极的真空管。阴极加热后，释放的自由电子高速流向阳极，当电子撞击一个原子核时, 会以X射线辐射的形式释放能量； 工业：检测电路板中的制造缺陷等； 天文学； 紫外波段成像 荧光显微方法：紫外光本身并不可见，但当紫外线辐射的光子与荧光物质内原子中的电子碰撞时，会使电子跃迁到较高的能级，随后回到较低的能级，并以可见光范围内的低能光子形式发光； 天文学； 可见光和红外波段成像 光学显微镜 遥感：通常包括可见光和红外波谱范围内的一些波段，每个波谱波段显示一幅图像； 工业检测 微波波段成像 雷达：在任何区域和任何时间内，不管天气周围光照条件如何都有收集数据的能力，某些雷达可穿透云层，在一定条件下还可穿透植被、冰层和沙漠，在许多情况下，雷达是探测地表不可接近地区的唯一办法 无线电波段成像 医学：磁共振，将病人放在强磁场中，让无线电波以短脉冲形式通过病人的身体，每个脉冲都会使病人的组织发射相应的无限电波脉冲； 天文学； 其他成像方式 声波：矿产和石油勘探，获取海洋图像，胎儿图像 电子显微镜 数字图像处理的基本步骤","link":"/blog/2022/09/21/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-01-%E7%BB%AA%E8%AE%BA/"},{"title":"图像处理学习笔记-02-数字图像基础","text":"第一节简述人类视觉系统的一些重要方面，包括人眼中图像的生成及人眼适应和辨认灰度的一些能力，第二节讨论光、电磁波谱的其他分量及他们的成像特点，第三节讨论成像传感器及如何使用成像传感器来生成数字图像，第四节介绍均匀图像取样和灰度量化的概念，第五节介绍像素间的各种基本关系 视觉感知要素目的：图像形成并被人类感知的基本原理 光感受器：锥状体(对颜色高度敏感，明视觉或亮视觉)，杆状体(对低光照度敏感，表现为白天色彩鲜艳的物体在月光下却没有颜色，因为此时只有杆状体受到刺激，暗视觉、微光视觉) 人眼通过调节晶状体的形状来调节焦距，物体在视网膜上成像，光感受器的相对激励作用产生感知，把辐射能量转换为最终由大脑解码的电脉冲 感知亮度不是实际灰度的简单函数，视觉系统往往会在不同灰度区域的边界处出现下冲和上冲的现象，马赫带效应；同时对比，在背景不同的时候，人眼会对实际灰度相同的物体感到不同的亮度；光觉错视表示一些视觉上面的错觉； 光和电磁波谱 电磁波谱可用波长、频率或能量表示，波长$\\lambda$和频率$v$的关系为：$$\\lambda = \\frac{c}{v}$$$c$是光速$2.998 \\times 10^8m/s$电磁波谱各分量的能量为：$$E = hv$$$h$是普朗克常数，能量和频率成正比，和波长成反比 可见光的范围为$0.43\\mu m$紫色~$0.79\\mu m$红色 感知的物体颜色由物体反射的光的性质来决定 没有颜色的光称为单色光或无色光，单色光的唯一属性是亮度，单色光的感知亮度是从黑色到灰色最后到白色变化，单色光从黑到白的数值范围通常称为灰度级，单色图像称为灰度图像 辐射：从光源流出的总能量，用瓦特来度量 光通量：观察者从光源感知的能量，用流明$lm$来度量，例如不可见光的流明几乎为0 图像感知与获取 由“照射”源和形成图像的“场景”元素对光能的反射或吸收产生的，传感器对正被检测能量类型的响应，将入射能量转换为电压，输出电压波形是传感器的响应，将传感器响应数字化，得到一个数字量 光二极管：输出是与光强成正比的电压 一个简单的成像模型：以$f(x,y)$的二维函数来表示图像，$i(x,y)$表示入射分量，表示入射到被观察场景的光源照射量；$r(x,y)$表示反射分量，限制在0(全吸收)和1(全反射)之间：$$f(x,y) = i(x,y)r(x,y) \\0 \\leq f(x,y) \\lt \\infty \\0 \\leq i(x,y) \\lt \\infty \\0 \\leq r(x,y) \\leq 1$$ 图像取值和量化目的：将连续观测的数据(例如电压)转换为数字形式，这种转换包括取样和量化 对坐标值进行数字化称为取样或采样，对幅值进行数字化称为量化，也就是对一幅连续的图像划分为一个个小格子以对坐标值量化，后对每一个格子取值以对灰度值量化； $(x,y)$是笛卡尔积$Z^2$中的整数，且$f$是把灰度值(即实数集$R$中的一个实数)赋给每个特定坐标对$(x,y)$的一个函数，那么$f(x,y)$就是一幅数字图像； 灰度级$L$出于对存储和量化硬件的考虑通常取$2$的整数次幂； 灰度跨越的值域称为动态范围，定义为最大可度量灰度与最小可度量灰度之比； 图像对比度定义为一幅图像中最高和最低灰度级间的灰度差 一个像素的位置有坐标索引或下表索引$(x,y)$，和线性索引，线性索引分为行扫描和列扫描，以列扫描为例，从最左边一列开始，从上到下从0开始标号，之后到第二列，接着第一列最后一个号标号，后面的列以此类推； 空间分辨率：单位距离的线对数和单位距离的点数(像素数)单位(dpi，点数/英寸) 灰度分辨率：量化灰度时的比特数，例如灰度被量化为256级的图像，其灰度分辨率为8比特 图像内插：通常在图像放大、缩小、旋转和几何校正等任务中使用，主要是用已知数据估计未知位置的值的过程，包含最近邻内插、双线性内插，双三次内插 像素间的一些基本关系 坐标$(x,y)$处的像素$p$有两个水平的相邻像素和2个垂直的相邻像素，坐标分别是$(x + 1,y),(x - 1,y)(x,y+1),(x,y-1)$，这组像素称为$p$的4邻域，用$N_4(p)$； $p$的4个对角邻域的坐标是$(x+1,y+1),(x+1,y-1),(x-1,y+1),(x-1,y-1)$，用$N_D(p)$表示； $p$的4个对角邻域和4邻域合起来合称$p$的$8$邻域，用$N_8(p)$表示； 如果一个邻域包含$p$，称为闭邻域，否则称为开邻域； 对于坐标分别为$(x,y),(s,t),(v,w)$的像素$p,q,z$，如果:$$\\begin{aligned}D(p,q) &amp;\\geq 0(D(p,q) = 0 当且仅当p=q) \\D(p,q) &amp;= D(q,p) \\D(p,z) &amp;\\leq D(p,q) + D(q,z)\\end{aligned}$$则称$D$是距离函数或度量 欧氏距离：$D_e(p,q) = [(x - s)^2 + (y - t)^2]^{\\frac{1}{2}}$ $D_4$城市街区距离：$D_4(p,q) = |x - s| + |y - t|$ $D_8$棋盘距离：$D_8(p,q) = max(|x - s|,|y - t|)$ 涉及数学工具 图像的阵列操作：逐像素操作 考虑一般的算子$H$，该算子对于给定的输入图像$f(x,y)$，产生一幅输出图像$g(x,y)$：$$H[f(x,y)] = g(x,y)$$如果：$$H[a_if_i(x,y) + a_jf_j(x,y)] = a_iH[f_i(x,y)] + a_jH[f_j(x,y)] = a_ig_i(x,y) + a_jg_j(x,y)$$则称$H$是一个线性算子，符合加性和同质性或齐次性 图像相加：去噪，现有照片是原图像加上加性噪声形成的，噪声的平均值为0，所以多张带噪声的图片相加平均可以起到去噪的效果 图像相减：增强图像之间的差，可以看清血管，用使用造影剂前后的图像相减即可得到血管 图像相乘：阴影校正，假设现有图像是由原图像乘上一个阴影函数，那么我们利用阴影函数的反函数就可以得到原图片，修正光线的影响； 单像素操作：通过变换函数改变某像素点的灰度值 领域操作：变换后的图像里面的某一点的像素值由以该点为中心的邻域内所有点计算而来； 几何变换：包含坐标的空间变换和灰度内插两个过程 坐标转换：仿射变换，假设$(x,y)$是变换后的坐标，$(v,w)$是变换前的坐标，下面的公式是前向映射，也就是将变换前的坐标转换到变换后的坐标$$[\\begin{matrix}x &amp; y &amp; 1\\end{matrix}] = [\\begin{matrix}v &amp; w &amp; 1\\end{matrix}]T = [\\begin{matrix}v &amp; w &amp; 1\\end{matrix}]\\left[\\begin{matrix}t_{11} &amp; t_{12} &amp; 0 \\t_{21} &amp; t_{22} &amp; 0 \\t_{31} &amp; t_{32} &amp; 1\\end{matrix}\\right]$$还可以做反向映射，也就是由转换后的图像的坐标计算出变换前所在的原图中的坐标，这种方法更加有效：$$[\\begin{matrix}v &amp; w &amp; 1\\end{matrix}] = T^{-1}[\\begin{matrix}x &amp; y &amp; 1\\end{matrix}]$$灰度内插：最近邻、双线性、双三次内插 图像变换：之前介绍的任务是直接在空间域工作，在一些情况下，可以变换图像，然后在变换域执行指定的任务，之后再反变化到空间域，二维线性变换$T(v,u)$:$$T(u,v) = \\sum_{x = 0}^{M - 1}\\sum_{y = 0}^{N - 1}f(x,y)r(x,y,u,v)$$其中，$f(x,y)$是输入图像，$r(x,y,u,v)$是正变换核，上式对$u = 0,1,2,\\cdots,M-1,v = 0,1,2,\\cdots,N-1$计算，$x,y$是空间变量，$M,N$是$f$的行和列，$u,v$是变换变量，$T(u,v)$称为$f(x,y)$的正变换，还可以用$T(u,v)$反变换$f(x,y)$:$$f(x,y) = \\sum_{u = 0}^{M - 1}\\sum_{v = 0}^{N - 1}T(u,v)s(x,y,u,v)$$$s(x,y,u,v)$称为反变换核","link":"/blog/2022/09/21/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-02-%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%9F%BA%E7%A1%80/"},{"title":"图像处理学习笔记-03-灰度变换与空间滤波","text":"术语空间域指图像平面本身，这类图像处理方法直接以图像中的像素操作为基础；空间域处理主要分为灰度变换和空间滤波两类，灰度变换在图像的单个像素上操作，主要以对比度和阈值处理为目的；空间滤波涉及改善性能的操作。 背景知识本章讨论的空间域处理可由下式表示：$$g(x,y) = T[f(x,y)]$$其中$f(x,y)$是输入图像，$g(x,y)$是处理后的图像，$T$是在点$(x,y)$的邻域上定义的关于$f$的一种算子 一些基本灰度变换函数假设$r,s$分别代表处理前后的像素值，$s = T(r)$，其中$T$是把像素值$r$映射到像素值$s$的一种变换； 图像反转：灰度级范围为$[0,L - 1]$$$s = L - 1 - r$$增强一幅图像的暗区域中的白色或灰色细节 对数变换：$c$是一个常数$$s = c \\log (1 + r)$$该变换将输入中范围较窄的低灰度值映射为输出中较宽范围的灰度值，使用这种变换来扩展图像中的暗像素的值，同时压缩更高灰度级的值，也就是将暗区域的细节显现出来 幂律(伽马)变换：$c,\\gamma$为正常数$$s = cr^\\gamma$$当$\\gamma = 1$为恒等变换，$\\gamma \\gt 1$增加了亮区域的细节，使原图片变暗，$\\gamma \\lt 1$增加了暗区域的细节，使原图片变亮，效果和对数变换类似，用于校正幂律响应现象的处理称为伽马校正 分段线性变换函数对比度拉伸灰度级分层比特平面分层直方图处理灰度级范围为$[0,L - 1]$的数字图像的直方图是离散函数$h(r_k) = n_k$，其中$r_k$是第$k$级灰度值，$n_k$是图像中灰度为$r_k$的像素个数，假设$M,N$分别是图像行和列的维数，那么归一化后的直方图为$p(r_k) = n_k / MN，k= 0,1,\\cdots,L-1$，简单的说$p(r_k)$是灰度级$r_k$在图像中出现的概率的一个估计直方图均衡用$r$表示待处理图像的灰度，通常我们假设$r$的取值区间为$[0,L - 1]$，$r = 0$表示黑色，$r = L - 1$表示白色，对于输入图像中每个具有$r$值的像素值产生一个灰度值$s$：$$s = T(r),0 \\leq r \\leq L - 1$$转换函数需要满足的条件： $T(r)$在区间$0 \\leq r \\leq L - 1$上为单调递增函数； 当$0 \\leq r \\leq L - 1$时，$0 \\leq T(r) \\leq L - 1$ 我们知道高对比度的图片的直方图占据整个灰度级而且分布均匀那么直方图均衡的转换函数为：$$s_k = T(r_k) = (L - 1)\\sum_{j = 0}^kp_r(r_j) = \\frac{L - 1}{MN}\\sum_{j = 0}^kn_j,k = 0,1,\\cdots,L-1$$上式中的变换称为直方图均衡或直方图线性变换证明：首先变换后的变量$s$的$PDF$概率密度函数可以由下式得到：$$p_s(s) = p_r(r)\\left|\\frac{dr}{ds}\\right|$$当变换函数如下时可以证明$s$的概率密度函数是均匀分布函数，该变换公式满足开始的两个条件$$s = T(r) = (L - 1)\\int_0^rp_r(w)dw$$又有：$$\\frac{ds}{dr} = \\frac{dT(r)}{dr} = (L - 1)\\frac{d}{dr}\\left[\\int_0^rp_r(w)dw\\right] = (L - 1)p_r(r)$$可以推出：$$p_s(s) = p_r(r)\\left|\\frac{dr}{ds}\\right| = p_r(r)\\left|\\frac{1}{(L - 1)p_r(r)}\\right| = \\frac{1}{L - 1},0 \\leq s \\leq L - 1$$是一个均匀概率密度函数，也就是经过变换之后的图像的灰度值的分布是均匀的，灰度值是离散取值的：$$p_r(r_k) = \\frac{n_k}{MN}，k= 0,1,\\cdots,L-1$$所以变换函数为：$$s_k = T(r_k) = (L - 1)\\sum_{j = 0}^kp_r(r_j) = \\frac{L - 1}{MN}\\sum_{j = 0}^kn_j,k = 0,1,\\cdots,L-1$$ 直方图匹配目的：有的时候采用均匀直方图的基本增强不是最好的方法，希望处理后的图像具有规定的直方图形状可能更有用直方图匹配或直方图规定化：产生处理后有特殊直方图的方法 方法：首先将原图均匀化：$$s = T(r) = (L - 1)\\int_0^rp_r(w)dw$$之后将我们需要的直方图形式所对应的输出的灰度值$z$均匀化，这个$z$也就是我们需要求的：$$G(z) = (L - 1)\\int_0^zp_z(t)dt = s$$之后通过反函数求得$z$$$z = G^{-1}[T(r)] = G^{-1}(s)$$于是就得到了我们需要的输出的灰度值 一个例子：假设一幅图像的灰度$PDF$为$p_r(r) = 2r/(L - 1)^2,0\\leq r\\leq L - 1$，对于其他$r$值有$p_r(r) = 0$，寻找一个变换函数使得产生的灰度$PDF$是$p_z(z) = 3z^2/(L - 1)^3,0 \\leq z \\leq L - 1$，对于其他的$z$值有$p_z(z) = 0$首先对原图像进行直方图均衡变换，得到灰度值为$s$的均匀$PDF$图像：$$s = T(r) = (L - 1)\\int_0^rp_r(w)dw = \\frac{2}{L - 1}\\int_0^rwdw = \\frac{r^2}{L - 1}$$对希望输出的图像进行直方图均衡变换，也得到灰度值为$s$的均匀$PDF$图像：$$G(z) = (L - 1)\\int_0^zp_z(w)dw = \\frac{3}{(L - 1)^2}\\int_0^zw^2dw = \\frac{z^3}{(L - 1)^2}$$之后求得反函数，得到$s$到$z$的映射：$$z = [(L - 1)^2s]^\\frac{1}{3}$$有$s = \\frac{r^2}{L - 1}$，带到上式可得$r$到$z$的映射：$$z = [(L - 1)^2s]^\\frac{1}{3} = \\left[(L - 1)^2\\frac{r^2}{L - 1}\\right]^{\\frac{1}{3}} = [(L - 1)r^2]^\\frac{1}{3}$$而因为灰度值是离散的，所以可以不需要计算$G$的反变换，我们可以首先计算$G(z_q) = (L - 1)\\sum_{i = 0}^qp_z(z_i)$得到$z$和$s$的一个对应形成一张表，之后对于输入的图像的每一个灰度值$r$计算均衡化之后的$s$值，在表中查找对应的$z$值 局部直方图处理上面两个小节的图像处理都是基于整个图像而言的，还有一种情况是，每一个像素基于邻域来进行局部增强，在每个位置，计算邻域中的点的直方图，例如直方图均衡化或者规定化变换函数，在这个处理的过程中不断移动邻域中心 在图像增强中使用直方图统计$r$的均值为$m$：$$m = \\sum_{i = 0}^{L - 1}r_ip(r_i)$$$r$关于其均值的$n$阶矩定义为：$$\\mu_n(r) = \\sum_{i = 0}^{L - 1}(r_i - m)^np(r_i)$$其中二阶矩称为灰度方差，通常使用$\\sigma^2$表示，是图像对比度的度量，还可以直接用下面的公式计算：$$m = \\frac{1}{MN}\\sum_{x = 0}^{M - 1}\\sum_{y = 0}^{N - 1}f(x,y) \\\\sigma^2 = \\frac{1}{MN}\\sum_{x = 0}^{M - 1}\\sum_{y = 0}^{N - 1}[f(x,y) - m]^2$$局部均值和方差是根据图像中每一像素的邻域内的图像特征进行改变的基础，$S_{xy}$表示规定大小的以$(x,y)$为中心的邻域，也就是子图像，$p_{S_{xy}}$是区域$S_{xy}$的直方图，该领域中像素的均值为：$$m_{S_{xy}} = \\sum_{i = 0}^{L - 1}r_ip_{s_{xy}}(r_i)$$邻域中像素的方差由下式给出：$$\\sigma_{S_{xy}}^2 = \\sum_{i = 0}^{L - 1}(r_i - m_{S_{xy}})^2p_{S_{xy}}(r_i)$$下面是一个局部图像增强的例子 空间滤波基础 空间滤波器由一个邻域，对该邻域包围的图像像素执行的预定义操作组成，滤波产生一个新像素，新像素的坐标等于邻域中心的坐标，像素的值是滤波操作的结果； 一般来说，假设$m = 2a + 1,n = 2b + 1$，使用大小为$m \\times n$的滤波器对大小为$M \\times N$的图像进行线性空间滤波，可由下式表示：$$g(x,y) = \\sum_{s = -a}^a\\sum_{t = -b}^bw(s,t)f(x + s,y + t)$$ 相关和卷积：卷积需要将滤波器旋转180°，相关不需要旋转，如果滤波器是对称的，那么相关和卷积的结果相同相关：$$g(x,y) = \\sum_{s = -a}^a\\sum_{t = -b}^bw(s,t)f(x + s,y + t)$$卷积：一个函数与单位冲激的卷积，相当于在单位冲激的位置复制该函数$$g(x,y) = \\sum_{s = -a}^a\\sum_{t = -b}^bw(s,t)f(x - s,y - t)$$ 线性滤波的向量表示：$w$是一个大小为$m \\times n$的滤波器的系数，$z$为滤波器覆盖的相关图像的灰度值$$R = w_1z_1 + w_2z_2 + \\cdots + w_{mn}z_{mn} = \\sum_{k = 1}^{mn}w_kz_k = w^Tz$$ 平滑空间滤波器目的：用于模糊处理和降低噪声 平滑线性滤波器平滑线性空间滤波器的输出是包含在滤波器模板邻域内的像素的简单平均值，有时也称为均值滤波器，可以归入低通滤波器,由于典型的随机噪声由灰度级的急剧变化组成，因此，常见的平滑处理应用就是降低噪声，但是由于图像边缘也是由图像灰度尖锐变化带来的特性，所以均值滤波处理还是有着边缘模糊的负面效应，所有系数都相等的空间均值滤波器称为盒状滤波器$$R = \\frac{1}{9}\\sum_{i = 1}^9z_i$$减少边缘模糊的一种方法：加权均值滤波器，也就是权重值随着离中心点的距离增大而减小，一幅$M \\times N$的图像经过一个大小为$m \\times n$的加权均值滤波器的过程可由下式给出：$$g(x,y) = \\frac{\\sum_{-a}^a\\sum_{-b}^bw(s,t)f(x+s,y+t)}{\\sum_{-a}^a\\sum_{-b}^bw(s,t)}$$ 统计排序(非线性)滤波器这种滤波器的响应以滤波器包围的图像区域中所包含的像素的排序为基础，例如中值滤波器，中值滤波器对处理椒盐噪声非常有效，这种噪声是以黑白点的形式叠加在图像上面的 锐化空间滤波器锐化处理的主要目的就是突出灰度的过渡部分，增强了边缘和其他突变(例如噪声)，削弱了灰度变化缓慢的区域，本节将讨论由数字微分来定义和实现锐化算子的各种方法 一阶微分的基本定义：$$\\frac{\\partial f}{\\partial x} = f(x + 1) - f(x)$$ 二阶微分的基本定义：$$\\frac{\\partial^2 f}{\\partial x^2} = f(x + 1) + f(x - 1) - 2f(x)$$ 使用二阶微分进行图像锐化-拉普拉斯算子 各向同性滤波器：这种滤波器和滤波器作用的图像的突变方向无关，也就是说各向同性滤波器是旋转不变的，即将原图像旋转后进行滤波处理得到的结果和先对图像滤波然后再旋转的结果相同 最简单的各向同性微分算子是拉普拉斯算子，一个二维图像函数$f(x,y)$的拉普拉斯算子定义为$$\\nabla^2f = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2}$$以离散形式描述，在$x$方向上：$$\\frac{\\partial^2 f}{\\partial x^2} = f(x+1,y) + f(x-1,y) - 2f(x,y)$$在$y$方向上：$$\\frac{\\partial^2 f}{\\partial y^2} = f(x,y+1) + f(x,y-1) - 2f(x,y)$$由上式可得，拉普拉斯算子是：$$\\nabla^2f(x,y) = f(x+1,y) + f(x-1,y) + f(x,y+1) + f(x,y-1) - 4f(x,y)$$上式可以用滤波模板来实现：第一幅是以90°为增量进行旋转的一个各向同性结果，第二幅是以45°为增量的使用拉普拉斯对图像进行增强的基本方法如下：如果使用$a,b$中的滤波器，$c = -1$，另外两幅的话$c = 1$$$g(x,y) = f(x,y) + c[\\nabla^2f(x,y)]$$ 非锐化掩蔽和高提升滤波非锐化掩蔽的处理过程： 模糊原图像 从原图像中减去模糊图像(产生的差值图像称为模板) 将模板加到原图像上 令$\\overline{f}(x,y)$表示模糊图像，非锐化掩蔽以公式形式描述如下，首先，我们得到模板：$$g_{mask}(x,y) = f(x,y) - \\overline{f}(x,y)$$然后，在原图像上加上该模板的一个权重部分：$$g(x,y) = f(x,y) + k\\times g_{mask}(x,y)$$当$k = 1$时，得到上面定义的非锐化掩蔽，$k \\gt 1$时，该处理称为高提升滤波，$k \\lt 1$则不强调非锐化模板的贡献 使用一阶微分对(非线性)图像锐化-梯度图像处理中的一阶微分是用梯度幅值来实现的，对于函数$f(x,y)$，$f$在坐标$(x,y)$处的梯度定义为二维向量$$\\nabla_f \\equiv grad(f) = \\left[\\begin{matrix}g_x \\g_y\\end{matrix}\\right] = \\left[\\begin{matrix}\\frac{\\partial f}{\\partial x} \\\\frac{\\partial f}{\\partial y}\\end{matrix}\\right]$$该向量具有重要的几何特性，即它指出了在位置$(x,y)$处$f$的最大变化率的方向，向量$\\nabla_f$的幅度值表示为$M(x,y)$:$$M(x,y) = mag(\\nabla_f) = \\sqrt{g_x^2 + g_y^2}$$$M(x,y)$是与原图像大小相同的图像，该图像称为梯度图像，梯度向量的分量是微分，所以他们是线性算子，然而该向量的幅度不是线性算子，偏微分不是旋转不变的，梯度向量的幅度是旋转不变的，在某些实现中，用绝对值的近似平方和平方根操作更适合计算，近似式在旋转90°的倍数的时候是各向同性的，近似式也是非线性的：$$M(x,y) \\approx |g_x| + |g_y|$$假设使用交叉差分：$$g_x = (z_9 - z_5) \\g_y = (z_8 - z_6)$$结合上式得到梯度图像：$$M(x,y) = \\left[(z_9 - z_5)^2 + (z_8 - z_6)^2 \\right]^\\frac{1}{2}$$如使用近似平方根：$$M(x,y) = |z_9 - z_5| + |z_8 - z_6|$$能实现上述所需偏微分项的滤波器称为罗伯特交叉梯度算子，见图$b,c$，但是偶数尺寸的模板难以实现，使用$3 \\times 3$的模板：$$g_x = \\frac{\\partial f}{\\partial x} = (z_7 + 2z_8 + z_9) - (z_1 + 2z_2 + z_3) \\g_y = \\frac{\\partial f}{\\partial y} = (z_3 + 2z_6 + z_9) - (z_1 + 2z_4 + z_7)$$计算梯度幅值：$$M(x,y) \\approx |(z_7 + 2z_8 + z_9) - (z_1 + 2z_2 + z_3)| + |(z_3 + 2z_6 + z_9) - (z_1 + 2z_4 + z_7)|$$对应的模板为图$d,e$，称为Soble算子","link":"/blog/2022/09/22/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-03-%E7%81%B0%E5%BA%A6%E5%8F%98%E6%8D%A2%E4%B8%8E%E7%A9%BA%E9%97%B4%E6%BB%A4%E6%B3%A2/"}],"tags":[{"name":"hexo","slug":"hexo","link":"/blog/tags/hexo/"},{"name":"icarus","slug":"icarus","link":"/blog/tags/icarus/"},{"name":"makefile","slug":"makefile","link":"/blog/tags/makefile/"},{"name":"基础","slug":"基础","link":"/blog/tags/%E5%9F%BA%E7%A1%80/"},{"name":"机器学习","slug":"机器学习","link":"/blog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"docker","slug":"docker","link":"/blog/tags/docker/"},{"name":"算法","slug":"算法","link":"/blog/tags/%E7%AE%97%E6%B3%95/"},{"name":"模式识别","slug":"模式识别","link":"/blog/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"},{"name":"自然语言处理","slug":"自然语言处理","link":"/blog/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"离散数学","slug":"离散数学","link":"/blog/tags/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/"},{"name":"高级人工智能","slug":"高级人工智能","link":"/blog/tags/%E9%AB%98%E7%BA%A7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"统计学习方法","slug":"统计学习方法","link":"/blog/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"论文","slug":"论文","link":"/blog/tags/%E8%AE%BA%E6%96%87/"},{"name":"图像处理","slug":"图像处理","link":"/blog/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}],"categories":[{"name":"网站创建","slug":"网站创建","link":"/blog/categories/%E7%BD%91%E7%AB%99%E5%88%9B%E5%BB%BA/"},{"name":"个人小记","slug":"个人小记","link":"/blog/categories/%E4%B8%AA%E4%BA%BA%E5%B0%8F%E8%AE%B0/"},{"name":"linux","slug":"linux","link":"/blog/categories/linux/"},{"name":"刷题笔记","slug":"刷题笔记","link":"/blog/categories/%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"},{"name":"机器学习-李宏毅","slug":"机器学习-李宏毅","link":"/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9D%8E%E5%AE%8F%E6%AF%85/"},{"name":"卜算法","slug":"卜算法","link":"/blog/categories/%E5%8D%9C%E7%AE%97%E6%B3%95/"},{"name":"模式识别与机器学习","slug":"模式识别与机器学习","link":"/blog/categories/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"自然语言处理","slug":"自然语言处理","link":"/blog/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"离散数学","slug":"离散数学","link":"/blog/categories/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/"},{"name":"高级人工智能","slug":"高级人工智能","link":"/blog/categories/%E9%AB%98%E7%BA%A7%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"统计学习方法","slug":"统计学习方法","link":"/blog/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"论文","slug":"论文","link":"/blog/categories/%E8%AE%BA%E6%96%87/"},{"name":"图像处理","slug":"图像处理","link":"/blog/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]}